[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Auswertung quantitativer Daten",
    "section": "",
    "text": "Herzlich willkommen!\nDieses Dokument wird Sie durch die Veranstaltung “Auswertung quantitativer Daten” begleiten. Es ist zum und im Wintersemeter 2024/25 an der Universität Erfurt entstanden und wird seitdem fortlaufend aktualisiert.",
    "crumbs": [
      "Herzlich willkommen!"
    ]
  },
  {
    "objectID": "Einleitung.html",
    "href": "Einleitung.html",
    "title": "1  Willkommen zum Kurs",
    "section": "",
    "text": "1.1 Lernziele\nKommunikationswissenschaft ist nur mit einem bodenständigen Verständnis von Daten und Statistik möglich. Das gilt einerseits für die Forschung, die selbst quantitativ sein kann, mindestens aber voraussetzt, quantitative Studien lesen und verstehen zu können. Es gilt andererseits aber auch für die Berufspraxis: Viele Berufe erfordern heute ein gutes Verständnis von Daten oder sogar einen sicheren Umgang damit.\nVor diesem Hintergrund verfolgt diese Veranstaltung zwei zentrale Ziele:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zum Kurs</span>"
    ]
  },
  {
    "objectID": "Einleitung.html#lernziele",
    "href": "Einleitung.html#lernziele",
    "title": "1  Willkommen zum Kurs",
    "section": "",
    "text": "Wir wollen Ihnen die statistischen Grundlagen vermitteln, die Sie im weiteren Verlauf Ihres Studiums benötigen werden, zum Beispiel in der Veranstaltung „Erhebung quantitativer Daten“, möglicherweise aber auch in der PSP oder einem anschließenden Masterstudium.\nWir wollen Ihnen die Werkzeuge an die Hand geben, die Sie benötigen, um vielfältige Arbeiten mit Daten auszuüben. Konkret bedeutet das, dass Sie lernen, Daten computergestützt aufzubereiten (also in ein verwertbares Format zu bringen), zu beschreiben und zu visualisieren. Dazu werden wir R benutzen.\n\n\n\n\n\n\n\nWas ist R?\n\n\n\nR ist ein Open-Source-Programm, das heißt, Sie können es kostenfrei nutzen. R zeichnet sich durch eine sehr aktive Community aus, die zahlreiche, ebenfalls kostenfreie, Erweiterungen entwickelt hat. In R nennen wir diese Erweiterungen Pakete. Einige davon werden wir im Lauf der Veranstaltung kennenlernen. R ist aber auch eine Programmiersprache mit einem Schwerpunkt auf statistisches Programmieren. Aber keine Sorge: Sie benötigen für diesen Kurs weder Vorkenntnisse in Statistik noch Informatik. Die wichtigsten Grundlagen bringen wir Ihnen bei. Weniger bedrohlich könnte man auch sagen, dass R einfach ein sehr guter und umfangreicher Taschenrechner ist.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zum Kurs</span>"
    ]
  },
  {
    "objectID": "Einleitung.html#ablauf-und-aufbau-des-kurses",
    "href": "Einleitung.html#ablauf-und-aufbau-des-kurses",
    "title": "1  Willkommen zum Kurs",
    "section": "1.2 Ablauf und Aufbau des Kurses",
    "text": "1.2 Ablauf und Aufbau des Kurses\nDer Kurs ist so konzipiert, dass Sie die Inhalte zwischen den Präsenzsitzungen selbstständig erarbeiten. Die Sitzungen werden wir dann nutzen, um das Erlernte zu üben. Sie sollten also gut vorbereitet kommen! Als Grundlage dazu dient dieses Dokument. Jedes Kapitel erläutert die Inhalte einer Woche.\nIn den ersten Sitzungen werden wir uns Zeit für die Grundlagen nehmen, die Sie im Lauf des Semesters immer wieder benötigen werden. Dazu zählen auch einige Grundlagen der Statistik. In den späteren Sitzungen werden wir uns dann mit konkreten statistischen Verfahren beschäftigen. Auch hier werden wir uns sowohl mit der Statistik an sich beschäftigen, als auch mit der Umsetzung in R.\nNachfolgendend finden Sie einen tabellarischen Ablauf der Sitzungen.\n\n\n\n\n\n\n\n\nDatum\nInhalt\nVorbereitung\n\n\n\n\n08.04.\n\nLernziele\nAblauf und Aufbau des Kurses\nR und RStudio installieren\nProjekt anlegen\nSkript anlegen und speichern\n\n-\n\n\n15.04.\n\nObjekte (Objekte deklarieren, Objekte benennen, Objekttypen)\nDatensätze (Daten erstellen, Objekte in Datensätzen ansprechen, Daten einlesen)\nFunktionen (Argumente und Rückgabe von Funktionen, Funktionen ausführen, Funktionen verschachteln)\n\nKapitel 2\n\n\n22.04.\n\nPakete installieren und Pakete laden\nDatentransformation\n\nKapitel 3\n\n\n29.04.\n\nSkalenniveaus und zenrtale Lagemaße\ndeskriptive Datenanylse\n\nKapitel 4\n\n\n06.05.\n\nHypothesen (Alternativ- und Nullhypothese, Arten von Hypothesen)\nTesttheorie (p-Werte, statistische Signifikanz, statistische Power)\n\nKapitel 5\n\n\n13.05.\n\nZusammenhänge zwischen zwei nominalen Variablen testen (Kreuztabellen)\n\nKapitel 6\n\n\n20.05.\n\nZusammenhänge zwischen metrischen oder ordinalen Daten testen (Korrelationen)\n\nKapitel 7\n\n\n27.05.\n\nRegressionen I\n\nKapitel 8\n\n\n03.06.\n\nRegressionen II\n\nKapitel 9\n\n\n10.06.\n\nt-Tests\n\nKapitel 10\n\n\n17.06.\n\nVarianzanalysen\nkeine Präsenz; Übungsaufgaben + Online-Sprechstunde nach Bedarf\n\nKapitel 11\n\n\n24.06.\n\nReliabilität\n\nKapitel 12\n\n\n01.07.\n\nRecap\nÜbungen\n\nKapitel 13\n\n\n08.07.\n\nAbschlusssitzung",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zum Kurs</span>"
    ]
  },
  {
    "objectID": "Einleitung.html#r-herunterladen",
    "href": "Einleitung.html#r-herunterladen",
    "title": "1  Willkommen zum Kurs",
    "section": "1.3 R herunterladen",
    "text": "1.3 R herunterladen\nBevor es losgehen kann, müssen Sie R auf Ihrem Laptop installieren. Die beste Anlaufstelle dafür ist CRAN (The Comprehensive R Archive Network): https://cran.r-project.org/\nOben mittig auf der Seite können Sie Ihr Betriebssystem auswählen.\n\nWenn Sie Windows nutzen, klicken Sie auf “base” und starten dann auf der nächsten Seite den Download.\nWenn Sie macOS nutzen, müssen Sie darauf achten, die für Ihr System korrekte R-Version herunterzuladen.\n\nNachdem Sie R heruntergeladen und installiert haben, könnten Sie im Prinzip loslegen. Allerdings ist die Nutzeroberfläche von R nicht gerade leicht verständlich. Darum arbeiten wir in diesem Kurs mit RStudio. Hierbei handelt es sich um eine sogenannte integrierte Entwicklungsumgebung, die das Arbeiten mit R deutlich leichter macht. Sie können es auf https://posit.co/download/rstudio-desktop/ herunterladen. Wählen Sie auch hier die für Ihr Betriebssystem vorgegebene Version herunter und installieren Sie diese.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zum Kurs</span>"
    ]
  },
  {
    "objectID": "Einleitung.html#in-rstudio-arbeiten",
    "href": "Einleitung.html#in-rstudio-arbeiten",
    "title": "1  Willkommen zum Kurs",
    "section": "1.4 In RStudio arbeiten",
    "text": "1.4 In RStudio arbeiten\n\n1.4.1 Die Nutzeroberfläche von RStudio\nWenn Sie auch RStudio installiert haben, kann es endlich losgehen. Wenn Sie das Programm das erste Mal öffnen, werden Sie gefragt, welche R-Version genutzt werden soll. Hier können Sie angeben, dass die System-Standardversion genutzt werden soll. Das bedeutet auch: Immer wenn Sie RStudio öffnen, wird R im Hintergrund ebenfalls gestartet. Sie müssen also nicht beide Programme öffnen!\nAnschließend sollte Ihr Programm in etwa so aussehen wie auf dem Screenshot unten.\n\n\n\nScreenshot RStudio\n\n\nLinks sehen Sie die Konsole. Hier zeigt R Ihnen, welche Befehle ausgeführt wurden und was das Ergebnis ist. Sie können auch direkt Befehle eingeben, aber dazu unten mehr. Oben rechts sehen Sie das sogenannte Environment. Hier finden Sie Objekte, die Sie angelegt haben. Ein Objekt kann erst mal alles Mögliche sein, z. B. ein Datensatz, eine Variable oder eine Funktion. Im Lauf des Kurses werden wir darauf noch genauer eingehen. Das Feld unten rechts erfüllt mehrere Funktionen. Die zwei wichtigsten verbergen sich hinter den Reitern Plots und Help. Die Namen sind relativ selbsterklärend: Unter Plots werden uns Grafiken gezeigt, die wir in R erstellen, und unter Help finden wir Hilfe. Beides ist im Moment noch nicht relevant für uns, aber wir werden später darauf zurückkommen.\n\n\n1.4.2 Die Konsole\nZunächst schauen wir uns die Konsole etwas genauer an. Hier können Sie direkt mit R interagieren. Durch das Größer-als-Zeichen (&gt;) am Anfang der untersten Zeile signalisiert R Ihnen, dass Sie Befehle ausführen können. Wenn Sie z. B. einfache Rechnungen in der Konsole eingeben und mit Enter bestätigen, wird R Ihnen das Ergebnis ausgeben. Die [1] können Sie zunächst ignorieren. Dazu kommen wir später noch. Daneben sollte nun das Ergebnis der Rechnung stehen, so wie in den folgenden Zeilen:\n\n3+2 \n\n[1] 5\n\n\nEs kann vorkommen, dass Sie in der untersten Zeile der Konsole nicht das Größer-als-Zeichen sehen, sondern ein Plus. Das passiert immer dann, wenn Sie einen unvollständigen Befehl ausführen wollen. Geben Sie z. B. nur 17- ein und versuchen, den Befehl auszuführen, wird R Ihnen ein “+” anzeigen, da es nicht weiß, was von 17 abgezogen werden soll. Sobald Sie eine zweite Zahl eingeben und mit Enter bestätigen, wird Ihnen das Ergebnis angezeigt. Wenn Sie das “+” einmal sehen, aber nicht wissen, woher es kommt bzw. welcher Befehl unvollständig war, können Sie einfach irgendetwas in die Konsole eingeben, mit Enter bestätigen und sich dann auf die Suche nach dem Fehler machen.\nMit diesem Wissen könnten wir so ziemlich alle Funktionalitäten von R nutzen, es wäre aber ziemlich unpraktisch. Zwar speichert R den Verlauf unserer Sitzung (verborgen hinter dem Reiter History, oben rechts neben Environment”), das Format ist aber nicht sonderlich gut dazu geeignet, unsere Berechnungen und Analysen wiederverwertbar festzuhalten.\n\n\n1.4.3 Arbeit in Projekten\nEine gute Möglichkeit, Ihre Arbeit in R festzuhalten, sind Projekte. Diese haben den Vorteil, dass alle relevanten Dateien an einem Ort gebündelt und durch eine spezielle R-Datei verbunden werden. Das macht zum Beispiel das Laden von Datensätzen deutlich einfacher. Als Erstes sollten Sie daher ein Projekt für diesen Kurs anlegen.\nDazu klicken Sie zunächst rechts oben auf “Projekte” und dann “Neues Projekt”\n\nAls Nächstes werden Sie gefragt, ob Sie einen neuen Ordner anlegen oder vorhandenen Ordner verwenden möchten. Falls Sie noch keinen Ordner für diesen Kurs angelegt haben, wählen Sie die erste Option, andernfalls die zweite.\n\nSofern Sie einen neuen Ordner angelegt haben, klicken Sie nun auf “Neues Projekt”.\n\nZuletzt müssen Sie dem Ordner noch einen Namen geben und angeben, wo er angelegt werden soll und abschließend das Projekt anlegen.\n\n\n\n1.4.4 Skripte\nSkripte sind Dateien, in denen Sie R-Code schreiben und speichern können. Der große Vorteil daran ist, dass Sie Ihre Arbeit so dokumentieren und jederzeit wiederholen oder verändern können, ohne alles von vorne in der Konsole eingeben zu müssen. Um ein Skript anzulegen, klicken Sie oben links auf File –&gt; New File –&gt; R Script. Nun öffnet sich über der Konsole das (noch leere) Skript. Speichern Sie es am besten direkt ab, entweder über die Menüleiste (File –&gt; Save) oder wie in anderen Programmen per Tastatur:  + SS. Praktischerweise schlägt R direkt den Ordner vor, in dem wir gerade arbeiten, sprich den vorhin angelegten Projektordner.\nIm Skript können Sie nun R-Code schreiben. Um eine Code-Zeile auszuführen, können Sie entweder gleichzeitig  + EnterEnter drücken, oder oben rechts direkt über dem Skript auf “Run” klicken. R führt den Befehl in der aktuellen Zeile aus und springt zum nächsten Befehl. Manchmal erstrecken sich Befehle über mehrere Zeilen, aber das ist kein Problem. R ist ziemlich gut darin, zu erkennen, wann ein Befehl aufhört und wo der nächste beginnt. Zumindest solange Sie keinen Fehler in Ihrem Code haben.\nDas Ergebnis wird Ihnen in der Konsole angezeigt. Probieren Sie es doch mal mit der Addition von oben aus: 3+2\nDamit Sie zukünftig in Ihren Skripten nicht den Überblick verlieren, sollten Sie sich angewöhnen, Kommentare zu schreiben. Dazu können Sie das Hashtag- bzw. Doppelkreuzzeichen `#` verwenden. Alles was in einer Zeile hinter diesem Zeichen steht, wird von R nicht interpretiert, sondern dient lediglich Ihnen und allen anderen, die den Code lesen, als Erklärung oder Erinnerungsstütze. Jetzt am Anfang mag das noch etwas albern wirken, aber unterschätzen Sie nicht, wie wertvoll es sein kann, nach einer längeren Pause an einem Skript eine gute Dokumentation vorzufinden!\nAb der kommenden Woche werden wir intensiv(er) in Skripten arbeiten. Die ersten Kapitel können Sie noch gut in einem Skript bearbeiten. Für die späteren Kapitel empfiehlt es sich, jeweils ein neues Skirpt anzulegen, damit Sie den Überblick nicht verlieren.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zum Kurs</span>"
    ]
  },
  {
    "objectID": "Einleitung.html#ressourcen",
    "href": "Einleitung.html#ressourcen",
    "title": "1  Willkommen zum Kurs",
    "section": "1.5 Ressourcen",
    "text": "1.5 Ressourcen\nR ist sehr komplex und kann nicht innerhalb eines Semesters gemeistert werden. Wie oben erwähnt, lernen Sie in dieser Veranstaltung einige wichtige Grundlagen. Die ersten davon haben Sie heute schon gelernt. Dennoch werden Sie in den nächsten Wochen und Monaten einiges an Informationen verarbeiten müssen. Ihre erste Anlaufstelle dafür ist dieses Dokument und die Präsenzübungen. Aber niemand nimmt es Ihnen übel, wenn Sie darüber hinaus weitere Hilfe benötigen oder in Anspruch nehmen!\nDie meisten Probleme, die Sie haben werden, hatten vor Ihnen schon unzählige andere R-Lerner:innen und glücklicherweise hat R eine sehr aktive und hilfsbereite Community, die Ihnen jederzeit weiterhelfen kann. Beispielsweise finden sich in einigen sozialen Netzwerken wie X/Twitter oder Bluesky (#rstats) und Reddit (/r/rstats) informelle R-Gruppen, die einander Fragen beantworten. Wenn Sie ein Problem oder eine Fehlermeldung googeln, werden Sie früher oder später auch Ergebnisse von StackOverflow finden, einem Forum für Programmierer:innen. Und keine Sorgen: Niemand erwartet von Ihnen, dass Sie sich aktiv in die Community einbringen!\nDarüber hinaus gibt es zahlreiche Lehrbücher und Online-Kurse über R. Untenstehend finden Sie einige davon:\n\nR für Einsteiger von Maike Luhmann (Zugriff über das Uninetz)\nLearning Statistics with R von Danielle Navarro\nEine Sammlung Häufig verwendeter Datenvisualisierungen\nÄhnlicher R-Kurs aus Hamburg\n\nFalls Sie in diesen Quellen nicht fündig werden, spricht prinzipiell auch nichts gegen den Einsatz von KI, also zum Beispiel große Sprachmodelle wie ChatGPT. Hiermit erhalten Sie ganz offiziell die Erlaubnis, davon im Rahmen dieses Kurses Gebrauch zu machen! Seien Sie bitte trotzdem vorsichtig: Nicht jeder von ChatGPT und ähnlichen Anwendungen erstellte R-Code tut das, was Sie wollen oder sich vorgestellt haben. Prüfen Sie den Code daher stets auf Herz und Nieren, bevor Sie ihn als richtig akzeptieren.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zum Kurs</span>"
    ]
  },
  {
    "objectID": "Objekte, Daten, Funktionen.html",
    "href": "Objekte, Daten, Funktionen.html",
    "title": "2  Objekte, Daten, Funktionen",
    "section": "",
    "text": "2.1 Objekte\nIm letzten Kapitel haben Sie bereits zwei wichtige Dinge über Objekte erfahren:\nIn diesem Kapitel lernen Sie, wie Sie Objekte deklarieren, wie Objekte benannt sein sollten und welche Objekttypen es gibt.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objekte, Daten, Funktionen</span>"
    ]
  },
  {
    "objectID": "Objekte, Daten, Funktionen.html#objekte",
    "href": "Objekte, Daten, Funktionen.html#objekte",
    "title": "2  Objekte, Daten, Funktionen",
    "section": "",
    "text": "Objekte können erst einmal alles Mögliche sein, z. B. einzelne Variablen oder ganze Datensätze.\nObjekte werden im Environment angezeigt.\n\n\n\n2.1.1 Objekte deklarieren\nSie können Objekte erstellen und ihnen einen Wert zuzuweisen, indem Sie einen Namen für das Objekt in Ihr Skript schreiben, daneben einen Pfeil (&lt;-) und danach den Wert des Objektes deklarieren.1\nSo wie hier:\n\n# Erstellt die Objekte x und y mit den Werten 3 und 6\n\nx &lt;- 3\ny &lt;- 6\n\nMit diesem Code haben wir zwei Objekte erstellt: Das Objekt x mit dem Wert 3 und das Objekt y mit dem Wert 6. Mit diesen Objekten können wir jetzt weiterarbeiten, z. B. indem wir sie addieren. Indem wir z noch mal einzeln in eine Zeile schreiben, können wir uns den Wert direkt in der Konsole anzeigen lassen.\n\n# Berechnet das Objekt z aus den Objekten x und y\nz &lt;- x+y\n\n# Zeigt den Wert von z in der Konsole an\nz\n\n[1] 9\n\n\nWenn wir jetzt den Wert von x oder y ändern, ändert sich durch erneutes Ausführen des Befehls auch der Wert von z.\n\n# Der Wert von x wird geändert\nx &lt;- 7\n\n# z wird neu berechnet\nz &lt;- x+y\nz\n\n[1] 13\n\n\nDie Objekte x, y und z sind bisher jeweils natürliche (also ganze) Zahlen. Sie können aber auch andere Zahlen erstellen, z.B. negative Zahlen oder Zahlen mit Dezimalstellen:\n\n# x und y werden neue Werte zugewiesen, die keine ganze Zahlen sind. \nx &lt;- -7\ny &lt;- 6.5\n\n\n# z wird neu berechnet\nz &lt;- x+y\nz\n\n[1] -0.5\n\n\nJetzt haben wir x den Wert -7 und y den Wert 6,5 zugewiesen. Beachten Sie, dass wir einen Punkt als Dezimalzeichen genutzt haben! Entsprechend dieser Zuweisungen ist z nun -0,5.\nObjekte können auch andere Werte als Zahlen enthalten, z. B. Zeichenketten, sogenannte Strings.\n\n# Ein String der \"name\" heißt wird erstellt und mit dem Wert \"Hans\" versehen\n\nname &lt;- \"Hans\"\n\n\n\n\n\n\n\nTipp\n\n\n\nWenn Sie in RStudio Anführungszeichen setzen wollen, geht das um einiges einfacher als z. B. in Word. Wenn Sie in einem Skript z.B. ShiftShift + 22drücken, werden gleich Anführungszeichen für den Anfang und das Ende des Strings gesetzt, sodass Sie dazwischen Ihren Text platzieren können. Sie können aber auch jeden vorhandenen Text markieren und dann ShiftShift + 22 drücken und es werden Anführungszeichen um den Text herum gesetzt.\n\n\nOhne weitere Funktionen haben wir allerdings deutlich weniger Möglichkeiten, mit Strings zu arbeiten. Wir können Sie z.B. nicht ohne Weiteres kombinieren.\n\n\n2.1.2 Objektnamen\nBisher haben wir unseren Objekten sehr einfache Namen gegeben, die nicht sonderlich hilfreich sind und würden wir nur bei einfachen Buchstaben bleiben, gingen uns bald die Namen aus. Darum empfiehlt es sich, andere Namen zu verwenden. Welche das konkret sind, ist Ihnen überlassen. R gibt allerdings einige Regeln vor:\n\nGrundsätzlich sind Zahlen, Punkte, Unterstriche und Buchstaben erlaubt.\nUmlaute, Sonderzeichen (z. B. Bindestriche, Prozentzeichen) und Leerzeichen sind nicht erlaubt.\nObjektnamen müssen mit einem Buchstaben beginnen.\nObjektnamen beachten Groß- und Kleinschreibung.\nObjektnamen sind einzigartig. Das heißt, wenn Sie zwei Objekten nacheinander gleich benennen, wird das zweite das erste überschreiben.\nTRUE und FALSE sind als Namen verboten, da es sich hierbei um sogenannte boolesche Operatoren handelt. Da Sie mit T und F abgekürzt werden können, sollten auch diese Namen vermieden werden. Was genau es damit auf sich hat, werden wir im Lauf der Veranstaltung lernen.\n\nDarüber hinaus gibt es einige sinnvolle Konventionen, an die Sie sich halten sollten:\n\nAnschließend an den Ausschluss von TRUE und FALSE bzw. T und F, sollten Namen vermieden werden, die schon anderweitig vergeben sind, z.B. durch Funktionen.\nObjektnamen sollten so gewählt sein, dass sie auch nach einer längeren Pause noch wissen, was sich hinter einem Objekt verbirgt. In einigen Fällen ist das relativ einfach. Wenn Sie zum Beispiel im Rahmen einer Befragung das Alter der Befragten erhoben haben, können Sie die entsprechende Variable einfach Alter nennen. Manchmal wird es aber auch schwieriger. Wenn Sie zum Beispiel die Einstellung der Befragten zur Statistik über mehrere Fragen erhoben haben, empfiehlt es sich, einen gemeinsamen Präfix zu verwenden, z. B. einstellungStatistik und dann eine Zahl. So wissen Sie zwar nicht mehr unbedingt, was der genaue Wortlaut der Frage war, aber Sie sollten die Variable schnell wiedererkennen können.\nEs gibt verschiedene Konventionen zu längeren Objektnamen. Im vorherigen Punkt wurde z.B. der Konvention gefolgt, das erste Wort klein und das darauffolgende (bzw. alle weiteren Wörter) groß zu schreiben. Genauso gut könnten Sie die Wörter anders kenntlich machen, z.B. so: einstellung_statistik. Manchmal sieht man auch Dinge wie einstellung.statistik, EINSTELLUNG_STATISTIK, oder EinstellungStatistik. Wie sie es machen, ist Ihnen überlassen, aber versuchen Sie sich an eine dieser Konventionen zu halten.\n\n\n\n2.1.3 Objekttypen\nBisher haben wir unseren Objekten nur einfache Zahlenwerte bzw. einen String zugewiesen. Man kann diese Objekte auch einfach Variablen nennen. Aber Vorsicht: Der Begriff ist gewissermaßen zweideutig: In der Programmierlogik von R bezeichnen wir Objekte als Variable, wenn wir dort etwas Speichern, das wir irgendwie variieren können. In der Statistik meint der Begriff dagegen in der Regel eine Sache, über die wir Daten gesammelt haben. Also zum Beispiel eine Frage im Fragebogen oder eine Kategorie in der Inhaltsanalyse.\nDie sehr einfachen Variablen, die wir oben angelegt haben, kommen in der Praxis relativ selten vor. Stattdessen haben wir es häufig mit einer ganzen Abfolge von Zahlen (oder Strings) zu tun, z.B. wenn wir Daten einer Stichprobe erhoben haben.\nGlücklicherweise müssen wir nicht für jede Antwort jeder Person ein eigenes Objekt erstellen, sondern können sogenannte Vektoren verwenden. Hierbei handelt es sich um Objekte, die aus verschiedenen Elementen zusammengesetzt sind. Vektoren sind so etwas wie das Rückgrat von R, da viele Dinge intern als Vektoren behandelt werden, z. B. einzelne Zeilen oder Spalten in einer Tabelle. Entsprechend sind auch viele Funktionen in R darauf ausgelegt, auf Vektoren angewendet zu werden. Wir werden sie uns daher etwas genauer ansehen.\nVektoren können zum Beispiel mit der Funktion (dazu unten mehr) c() erstellt werden. Das c steht dabei für “combine”. Die einzelnen Elemente werden mit einem Komma getrennt.\n\n# Erstellt den Vektor \"Zahlen\", der die Zahlen von 1 bis 10 enthält\n\nzahlen &lt;- c(1,2,3,4,5,6,7,8,9,10)\n\nIn diesem Beispiel haben wir einen Vektor namens zahlen erstellt, der die Zahlen von 1 bis 10 enthält. In solch einfachen Fällen gibt es übrigens einen kleinen Trick und zwar den Doppelpunkt:\n\n# Erstellt den Vektor \"zahlenAnders\", der ebenfalls die Zahlen von 1 bis 10 enthält\n\nzahlenAnders &lt;- 1:10\n\nMit diesem Befehl sagen wir R, dass alle Zahlen von 1 bis einschließlich 10 in einem Vektor kombiniert werden sollen, ohne jede Zahl einzeln aufschreiben zu müssen.\nVielleicht ist Ihnen aufgefallen, dass die beiden Vektoren zahlen und zahlenAnders im Environment leicht unterschiedlich dargestellt werden. Für den Moment können wir das jedoch ignorieren.\n\nDie einzelnen Zahlen in den oben angelegten Vektoren, werden als Elemente bezeichnet. Sie können einzeln angewählt und ggf. manipuliert werden. Dafür nutzen wir sogenannte Indizes. Hier kommt die [1] ins Spiel, die wir im letzten Kapitel ignoriert haben. Damit hat uns R signalisiert, dass das Ergebnis hinter dieser [1] das erste Element eines Vektors war. Da es nur ein Element gab, wirkt das zunächst etwas überflüssig. In manchen Situationen kann es aber vorkommen, dass das Ergebnis eines Befehls mehrere Elemente enthält. Und in wieder anderen Situationen kann es sinnvoll sein, einzelne Elemente eines Vektors direkt anzusprechen. Das geht ebenfalls mit eckigen Klammern. Beispielsweise lassen wir hier das dritte Element des Vektors zahlen anzeigen.\n\n# Das dritte Element von \"zahlen\" wird ausgegeben\n\nzahlen[3]\n\n[1] 3\n\n\nWir können das Element auch ändern:\n\n# Das dritte Element von \"zahlen\" wird geändert und dann ausgegeben\n\nzahlen[3] &lt;- 9\nzahlen[3]\n\n[1] 9\n\n\nUnd genauso, wie wir oben die Zahlen von 1 bis 10 in einen Vektor geschrieben haben, können wir uns auch mehrere Elemente eines Vektors anzeigen lassen, z. B. die ersten drei Elemente:\n\n# Zeigt die ersten drei Elemente von \"zahlen\" an\n\nzahlen[1:3]\n\n[1] 1 2 9\n\n\nOder das erste, zweite und fünfte Element, indem wir die c()-Funktion von oben verwenden:\n\n# Zeigt die ersten beiden und das fünfte Element von \"zahlen\" an\n\nzahlen[c(1:2, 5)]\n\n[1] 1 2 5\n\n\nWir können auch mehrere Elemente auf einmal ändern. Dabei müssen wir aber allerdings ein paar Dinge beachten:\n\nWir können entweder alle Elemente durch einen Wert ersetzen:\n\n\n# Ändert alle Werte in \"zahlen\" zu 1\n\nzahlen[1:10] &lt;- 1\nzahlen\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\n\nOder so viele Werte, dass die Anzahl der alten Werte ein Vielfaches der Anzahl der neuen Werte sind (z. B. 5 neue auf die 10 alten Werte). Die neuen Werte werden dann so lange wiederholt, bis der Vektor wieder dieselbe Länge hat:\n\n\n# Die Werte in \"zahlen\" werden durch die Werte von 1 bis 5 ersetzt und dann angezeigt\n\nzahlen[1:10] &lt;- 1:5\nzahlen[1:10]\n\n [1] 1 2 3 4 5 1 2 3 4 5\n\n# Die Werte in \"zahlen\" werden durch die Werte von 1 und 2 ersetzt und dann angezeigt\n\nzahlen[1:10] &lt;- 1:2\nzahlen[1:10]\n\n [1] 1 2 1 2 1 2 1 2 1 2\n\n\n\nOder genauso viele Werte angeben, wie wir ersetzen möchten:\n\n\n# Die Werte in \"zahlen\" werden durch die Werte von 10 bis 1 ersetzt und dann angezeigt\n\nzahlen[1:10] &lt;- c(10,9,8,7,6,5,4,3,2,1)\nzahlen[1:10]\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nNach denselben Regeln können wir auch mit den Vektoren rechnen. Wenn Sie in Ihrem Skript die obigen Befehle der Reihe nach ausgeführt haben, sollten Ihre beiden Vektoren nun die Werte von 10 bis 1 (zahlen) bzw. 1 bis 10 (zahlenAnders) haben:\n\n# Zeigt die Vektoren \"zahlen\" und \"zahlenAnders\" an\n\nzahlen\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\nzahlenAnders\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWir können nun entweder eine einzelne Zahl zu einem Vektor addieren, davon subtrahieren, den Vektor durch die Zahl teilen oder die beiden multiplizieren:\n\n# Addiert y (6,5) zu den Elementen von \"zahlen\"\nzahlen+y\n\n [1] 16.5 15.5 14.5 13.5 12.5 11.5 10.5  9.5  8.5  7.5\n\n# Subtrahiert y von den Elementen von \"zahlen\"\nzahlen-y\n\n [1]  3.5  2.5  1.5  0.5 -0.5 -1.5 -2.5 -3.5 -4.5 -5.5\n\n# Multipliziert y mit den Elementen von \"zahlen\"\nzahlen*y\n\n [1] 65.0 58.5 52.0 45.5 39.0 32.5 26.0 19.5 13.0  6.5\n\n# Dividiert die Elementen von \"zahlen\" durch y \nzahlen/y\n\n [1] 1.5384615 1.3846154 1.2307692 1.0769231 0.9230769 0.7692308 0.6153846\n [8] 0.4615385 0.3076923 0.1538462\n\n\nOder eine Anzahl, die so groß ist, dass die Anzahl der Elemente des Vektors ein Vielfaches dieser Zahl ist:\n\n# Subtrahiert 1 bsi 5 von den Elementen von \"zahlen\"\n\nzahlen - 1:5\n\n [1]  9  7  5  3  1  4  2  0 -2 -4\n\n\nOder wir verwenden einen Vektor, der genauso lang ist:\n\n# Addiert \"zahlen\" und \"zahlenAnders\"\n\nzahlen + zahlenAnders\n\n [1] 11 11 11 11 11 11 11 11 11 11\n\n\nWie oben bereits erwähnt, kommt man in R schon ziemlich weit, wenn man ein gutes Verständnis von Vektoren mitbringt. In diesem Abschnitt haben Sie einige Basics gelernt, die Ihnen bei der Arbeit mit R immer wieder begegnen werden.\nNeben Vektoren gibt es noch zwei weitere Objekttypen, die Ihnen in Ihrer Arbeit mit R begegnen werden. Der erste ist die Liste. Der Name ist relativ selbsterklärend: Es handelt sich dabei um eine Liste von Elementen. Der Unterschied zum Vektor ist, dass Listenelemente alles Denkbare sein können. Also z. B. einzelne Werte wie x, y, z und name von oben, aber auch Vektoren wie zahlen und zahlenAnders. Sogar ganze Datensätze können Teil einer Liste sein! Am häufigsten werden Ihnen Listen als Ergebnis von statistischen Berechnungen begegnen, aber Sie können mit der list()-Funktion auch selber welche anlegen. Hier in Beispiel:\n\n# Erstellt eine Liste aller bisher erstellten Elemente\n\nwasBisherGeschah &lt;- list(x, y, z, name, zahlen, zahlenAnders)\n\nIn dieser Liste haben wir alle bisher erstellten Objekte zusammengefasst. Ähnlich wie bei den Vektoren, können wir auch Listenelemente direkt über einen Index ansprechen. Diesmal nutzen wir aber doppelte Klammren: [[]]. Zum Beispiel können wir so das vierte Element ausgeben lassen:\n\n# Zeigt das vierte Element der Liste an\n\nwasBisherGeschah[[4]]\n\n[1] \"Hans\"\n\n\nWir können auch erst ein bestimmtes Listenelement ansprechen und dann ein darin enthaltenes Element:\n\n# Zeigt das vierte Element des fünften Elements der Liste an\n\nwasBisherGeschah[[5]][4]\n\n[1] 7\n\n\nEin Vorteil von Listen ist, dass die Elemente darin benannt sein können. Und tatsächlich werden wir benannte Listen in der Realität deutlich häufiger antreffen. Wenn Sie selber eine benannte Liste erstellen wollen, gehen Sie ähnlich vor wie beim Deklarieren von Objekten. Allerdings nutzen Sie dabei nicht den Pfeil, sondern ein Gleichheitszeichen. In diesem Beispiel behalten wir die Namen von oben bei:\n\n# Erstellt eine Liste mit Namen\n\nwasBisherWirklichGeschah &lt;- list(x = x, y = y, z = z,\n                                 name = name, zahlen = zahlen,\n                                 zahlenAnders = zahlenAnders)\n\nNamen für Listenelemente haben den großen Vorteil, dass wir uns nicht merken müssen, an welcher Stelle ein bestimmtes Element auftaucht. Stattdessen können wir es ganz einfach über das Dollarzeichen ($) ansprechen. Dazu schreiben Sie erst den Namen der Liste, dann ein $ und dann den Namen des Elements. Auch hier können Sie wieder einen Index nutzen, um nur bestimmte Elemente anzusprechen:\n\n# zeigt das Element \"zahlenAnders\" der Liste \"wasBisherWirklichGeschah\" an\n\nwasBisherWirklichGeschah$zahlenAnders\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# zeigt das fünfte Element des Element \"zahlenAnders\" der Liste \"wasBisherWirklichGeschah\" an\n\nwasBisherWirklichGeschah$zahlenAnders[5]\n\n[1] 5\n\n\n\n\n\n\n\n\nObjekte inspizieren\n\n\n\nSie haben verschiedene Möglichkeiten, mehr über Objekte im Environment zu erfahren. Insbesondere im Kontext von Listen kann das manchmal sehr nützlich sein. Die beiden soeben erstellten Listen haben im Environment einen kleinen blauen Pfeil neben ihrem Namen. Dort können Sie draufklicken, um die Elemente der Liste zu sehen:\n\nSie können aber auch auf den Namen klicken. Dann öffnet sich oben im Editor (da, wo Sie Ihr Skript schreiben) ein neues Tab, in dem Sie sich das Objekt genauer ansehen können.\n\n\nDer letzte Objektyp ist der, wegen dem wir eigentlich hier sind: Datensätze. Wir werden Sie uns nun etwas genauer ansehen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objekte, Daten, Funktionen</span>"
    ]
  },
  {
    "objectID": "Objekte, Daten, Funktionen.html#datensätze",
    "href": "Objekte, Daten, Funktionen.html#datensätze",
    "title": "2  Objekte, Daten, Funktionen",
    "section": "2.2 Daten(sätze)",
    "text": "2.2 Daten(sätze)\nIn der Wissenschaft haben wir es in der Regel mit Daten zu tun. Soweit so gut. Aber was sind Daten eigentlich genau? Die Frage mag auf den ersten Blick trivial erscheinen, ist aber erstaunlich komplex. Beispielsweise könnten Sie im Rahmen einer qualitativen Studie Interviews mit Menschen führen, diese transkribieren und hätten dann Daten in Textform vorliegen. Aber wäre das Ihr erster Gedanke, wenn Sie danach gefragt werden, was Daten eigentlich genau sind? Tatsächlich haben diese Art von Daten, also qualitative Daten, relativ wenig mit dem zu tun, was wir in diesem Kurs machen. Wir beschäftigen uns mit quantitativen Daten. Weil dieser Ausdruck auf Dauer etwas sperrig ist, kürzen wir ihn hier aber etwas ab und sprechen nur von Daten.\n\n2.2.1 Struktur von Datensätzen\nWas also sind denn jetzt Daten im Sinne dieses Kurses? In erster Linie meinen wir mit Daten Informationen, die in Zahlenform vorliegen oder zumindest so repräsentiert werden können. Das erlaubt uns, damit zu rechnen. In R werden diese Zahlen meistens als eine Art Tabelle gespeichert. Die meisten von ihnen folgen einem einfachen Muster: Jede Zeile ist ein Fall, jede Spalte eine Variable. Ein Fall ist z.B. ein ausgefüllter Fragebogen, ein codierter Medieninhalt oder ein Versuchsdurchlauf eines Experiments. Eine Variable ist z.B. eine Frage aus einem Fragebogen, eine Kategorie aus einem Codebuch oder ein im Experiment gemessener Wert.\n\n\n2.2.2 Datensätze erstellen\nWir schauen uns so eine Tabelle mal an einem einfachen Beispiel an, indem wir die data.frame()-Funktion nutzen. Ähnlich wie die list()-Funktion, können wir data.frame() nutzen, um ein Objekt aus mehreren vorhandenen Objekten zu erstellen. Auch hier können wir einzelnen Elementen einen Namen geben. Wir nennen diesen Datensatz df (Abkürzung von data frame), ein generischer Name, dem Sie in Beispielen immer mal wieder begegnen werden. Andere geläufige Namen sind dat oder data.\n\n# Erstellt den Datensatz df, der 3 Variablen enthält\n\ndf &lt;- data.frame(var1 = zahlen, var2 = zahlenAnders, var3 = zahlenAnders/zahlen)\n\nWenn wir nun den Namen unseres Datensatzes eingeben und die entsprechende Zeile im Skript ausführen, wird uns die Tabelle angezeigt:\n\n# zeigt df an\n\ndf\n\n   var1 var2       var3\n1    10    1  0.1000000\n2     9    2  0.2222222\n3     8    3  0.3750000\n4     7    4  0.5714286\n5     6    5  0.8333333\n6     5    6  1.2000000\n7     4    7  1.7500000\n8     3    8  2.6666667\n9     2    9  4.5000000\n10    1   10 10.0000000\n\n\n\n\n2.2.3 Elemente von Datensätzen\nDadurch, dass Datensätze Zeilen und Spalten haben, sind sie zweidimensional. Die Spalten können Sie so ansprechen, wie wir es schon mit der benannten Liste oben gemacht haben (wasBisherWirklichGeschah), sprich mit einem $-Zeichen gefolgt vom Namen:\n\n# zeigt die erste Variable in df an\n\ndf$var1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nAb und zu kann es auch vorkommen, dass Sie eine bestimmte Zeile ansprechen wollen. Dazu können wir wieder Indizes benutzen, allerdings etwas anders als zuvor. Zum Beispiel liefert der Befehl df[1] dasselbe Ergebnis wie df$var1, nur etwas anders dargestellt.\n\n# zeigt jeweils die erste Variable in df an\n\ndf[1]\n\n   var1\n1    10\n2     9\n3     8\n4     7\n5     6\n6     5\n7     4\n8     3\n9     2\n10    1\n\ndf$var1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nEinzelne Zeilen können wir ansprechen, indem wir hinter die Zahl im Index ein Komma schreiben. df[1,] Wir sagen R damit, dass wir am ersten Element der ersten Dimension interessiert sind, die immer links vom Komma steht.\n\n# zeigt die erste Zeile in df an\n\ndf[1,]\n\n  var1 var2 var3\n1   10    1  0.1\n\n\nWir können auch rechts vom Komma eine Zahl ergänzen, z.B. wenn wir nur den dritten Wert aus der zehnten Zeile sehen wollen:\n\n# zeigt den Wert aus Zeile 10, Spalte 3 in df an\n\ndf[10,3]\n\n[1] 10\n\n\n\n\n2.2.4 Datensätze laden\nAufbauend auf diesen Grundlagen könnten Sie sich schon einen ganz guten Überblick über einen Datensatz machen. Aber: Woher bekommen Sie eigentlich Daten und wie werden Daten in R eingelesen? Im weiteren Verlauf dieses Kurses werden wir mit Daten arbeiten, die hier am Seminar für Medien- und Kommunikationswissenschaften entstanden sind. Im späteren Verlauf Ihres Studiums werden Sie dann mit Ihren eigenen Daten arbeiten. Das Einlesen in R ist relativ einfach. Sie brauchen dazu nur einen Datensatz und müssen wissen, wo auf Ihrem System er gespeichert ist. Am einfachsten ist es, wenn Sie die Daten im selben Ordner ablegen, wie Ihr R-Projekt. Damit Sie nicht den Überblick verlieren, ist es sinnvoll, zunächst einen Unterordner anzulegen, in dem Sie die Daten ablegen können.\nHier laden wir mit der read.csv()-Funktion einen Datensatz, der von Prof. Dogruel erhoben wurde. Es handelt sich dabei um eine Befragung zur Nutzung von Lokalmedien von Menschen aus Thüringen und Rheinland-Pfalz. Sie finden den Datensatz sowie ein zugehöriges Codebook bei Moodle. Letzteres enthält Informationen über die einzelnen Spalten im Datensatz. Laden Sie den Datensatz herunter und legen Sie ihn dort ab, wo Sie das Projekt aus der letzten Sitzung gespeichert haben.\nDamit der unten dargestellte Code funktioniert, muss R wissen, wo es nach dem Datensatz suchen muss. Dazu sollte das Projekt aus der letzten Woche geöffnet sein. Das können Sie oben rechts in RStudio kontrollieren. Wenn es ungefähr so aussieht, wie auf dem Bild unten, ist alles in Ordnung. Andernfalls müssen Sie das Projekt erst öffnen (siehe Kapitel 1).\n\n\n\n\n\nWenn Sie nun versuchen, eine Datei zu laden, wir R in Ihrem Projektordner danach suchen. Damit Sie im Lauf des Semesters nicht den Überblick verlieren, ist es ratsam, einen Unterordner für Daten anzulegen. Der untenstehende Code geht davon aus, dass Sie das getan haben:\n\n# Lädt den Datensatz \"lokalkommunikation\" aus dem Ordner \"Daten\" und speichert ihn als Objekt namens \"df_lokal\"\n\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\nMit diesem Datensatz werden wir vorerst weiterarbeiten. Im weiteren Verlauf des Kurses werden aber noch weitere Daten dazukommen. Zunächst benötigen wir aber noch ein paar Basics zur Arbeit in R: Grundwissen über sogenannte Funktionen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objekte, Daten, Funktionen</span>"
    ]
  },
  {
    "objectID": "Objekte, Daten, Funktionen.html#funktionen",
    "href": "Objekte, Daten, Funktionen.html#funktionen",
    "title": "2  Objekte, Daten, Funktionen",
    "section": "2.3 Funktionen",
    "text": "2.3 Funktionen\nFast alle Aufgaben in R lassen sich mit Hilfe von Funktionen lösen. Einige haben Sie in den vorherigen Abschnitten bereits kennengelernt, z.B. c() und list(). In diesem Abschnitt werden Sie einige weitere kennenlernen, Sie sollen aber vor allem lernen, wie Funktionen eigentlich funktionieren.\n\n2.3.1 Argumente\nDas wichtigste Konzept, das Sie dazu verstehen müssen, sind die sogenannten Argumente. Damit sind Informationen gemeint, die wir einer Funktion übergeben und mit denen sie arbeitet. Argumente werden in Klammern hinter dem Namen der Funktion aufgeführt. Als wir oben die c()-Funktion genutzt haben, waren dementsprechend die Zahlen, die wir in die Klammern geschrieben haben die Argumente. Und als Sie die Liste erstellt haben, haben Sie die vorher angelegten Objekte als Argumente übergeben.\nBeides waren relativ simple Fälle, die Ihnen im R-Alltag aber immer wieder begegnen werden, denn die allermeisten Funktionen benötigen ein Objekt, mit dem sie arbeiten können. Durch andere Argumente können wir bestimmen, wie die Funktion genau arbeiten soll. Im weiteren Verlauf dieses Kapitels und in den kommenden Wochen werden wir einige solcher Argumente kennenlernen. Zunächst genügt es, festzuhalten, dass Funktionen mit Argumenten darüber informiert werden, an welchem Objekt sie ihre Aufgabe ausüben sollen und ggf., was sie dabei zu beachten haben.\n\n\n2.3.2 Rückgabe\nEin weiteres wichtiges Konzept ist die Rückgabe von Funktionen. Oder anders gesagt: ihr Ergebnis. In der Regel empfiehlt es sich, Funktionen nicht einfach nur auszuführen, sondern das Ergebnis in einem neuen Objekt zu speichern. Der Vorteil davon ist, dass Sie dann mit dem Objekt weiterarbeiten können, ohne jedes Mal wieder die Funktion aufrufen zu müssen. Das wird besonders an der read.csv()-Funktion deutlich, die wir oben genutzt haben, um den Datensatz zu laden. Nur so können wir überhaupt sinnvoll damit weiterarbeiten.\n\n\n\n\n\n\nHilfe für Funktionen\n\n\n\nWenn Sie einmal nicht wissen, wie eine Funktion funktioniert oder Sie z.B. wissen wollen, welche Argumente eine Funktion benötigt oder was sie zurückgibt, können Sie ganz einfach Hilfe erhalten. Dazu geben Sie einfach ein ? gefolgt vom Namen der Funktion ein, z. B. ?read.csv(). Im Bereich unten rechts zeigt R Ihnen dann die Dokumentation der Funktion an.\n\n\n\n\n2.3.3 Beispiel - Erstellen einer Häufigkeitstabelle\nDie oben genannten Konzepte schauen wir uns nun anhand der table()-Funktion an. Damit können wir - wie der Name vermuten lässt - eine Tabelle erstellen. Der Datensatz df_lokal enthält eine Spalte, die das Geschlecht der Befragten angibt (A602). Der Wert 1 steht für “männlich”, 2 für “weiblich”, 3 für “divers/non-binär” und 4 für “keine Angabe”, eine sogenannte Ausweichkategorie. Wenn wir diese Spalte an die table()-Funktion übergeben, zeigt R uns in der Konsole eine Häufigkeitstabelle an. Wir erfahren also, wie häufig jede Antwort im Datensatz vorkommt.\n\n# Erstellt eine Häufigkeitstabelle der Geschlechtsabfrage\n\ntable(df_lokal$A602)\n\n\n  1   2   3   4 \n813 996   7  18 \n\n\nEs ist etwas mühselig, die Werte gedanklich immer wieder neu zu ordnen. Um dies zu vermeiden, können wir die factor()-Funktion benutzen. Diese wandelt einen Vektor (die Spalte A602) in einen Faktor mit beschrifteten Werten um. Faktor bedeutet in diesem Kontext, dass wir wenige distinkte Werte haben. Es handelt sich um eine sogenannte kategorische Variable. Keine Sorge, darüber werden wir in den kommenden Wochen noch mehr reden.\nFür die factor()-Funktion müssen wir neben der Spalte auch das labels-Argument nutzen, also die Beschriftung festlegen. Die Rückgabe der Funktion ist ein Vektor, der genauso lang ist, wie unser Ausgangsvektor (die Spalte A602), aber beschriftete Werte hat. Das können wir nutzen, um uns Arbeit zu sparen. Statt die Rückgabe der Funktion erst in einem neuen Objekt zu speichern, können wir factor() direkt in der table()-Funktion aufrufen.\n\n# Erstellt eine Häufigkeitstabelle der Geschlechtsabfrage, die vorher in einen beschrifteten Faktor umgewandelt wird\n\ntable(factor(df_lokal$A602, labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\")))\n\n\n    männlich     weiblich       divers keine Angabe \n         813          996            7           18 \n\n\nTatsächlich gibt es kein Limit, wie viele Funktionen wir innerhalb von Funktionen aufrufen können. Beispielsweise können wir die prop.table()-Funktion nutzen, um den Anteil der jeweiligen Geschlechter in der Stichprobe auszurechnen.\n\n# Erstellt eine Tabelle der relativen Häufigkeiten der Geschlechtsabfrage, die vorher in einen beschrifteten Faktor umgewandelt wird\n\nprop.table(table(factor(df_lokal$A602, labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\"))))\n\n\n    männlich     weiblich       divers keine Angabe \n 0.443293348  0.543075245  0.003816794  0.009814613 \n\n\nUm das Ergebnis als Prozentwert zu lesen, können wir es mit 100 multiplizieren.\n\n# Multipliziert die relativen Häufigkeiten mit 100, um Prozentwerte zu erhalten\n\nprop.table(table(factor(df_lokal$A602, labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\"))))*100\n\n\n    männlich     weiblich       divers keine Angabe \n  44.3293348   54.3075245    0.3816794    0.9814613 \n\n\nUnd um das Ergebnis noch besser lesbar zu machen, können wir die round()-Funktion nutzen, um das Ergebnis zu runden. Das digits-Argument gibt dabei die Anzahl der Nachkommastellen an.\n\n# Rundet die Prozentwerte auf 2 Nachkommastellen\n\nround(prop.table(table(factor(df_lokal$A602, labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\"))))*100, digits = 2)\n\n\n    männlich     weiblich       divers keine Angabe \n       44.33        54.31         0.38         0.98 \n\n\n\n\n\n\n\n\nTip\n\n\n\nÜbrigens: Funktionen erwarten Argumente in einer festen Reihenfolge, die Sie mit der Hilfsfunktion (? gefolgt vom Namen der Funktion) erfahren können. Solange wir die Argumente in dieser Reihenfolge nuzten, müssen wir sie nicht benennen. Im vorherigen Befehl können wir z.B. das digits = einfach weglassen und erhalten dasselbe Ergebnis.\n\n# Rundet die Prozentwerte auf 2 Nachkommastellen, lässt aber den Namen des digits-Arguments weg.\n\nround(prop.table(table(factor(df_lokal$A602, labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\"))))*100, 2)\n\n\n    männlich     weiblich       divers keine Angabe \n       44.33        54.31         0.38         0.98 \n\n\n\n\n\n\n2.3.4 Pipes\nDas Verschachteln von Funktionen im Beispiel oben funktioniert zwar gut, wird aber schnell schwer nachvollziehbar. Man sagt auch, dass der Code nicht sonderlich lesbar ist. Um diesem Problem zu begegnen, können wir sogenannte Pipes (im Sinne von Pipeline) verwenden. Dazu nutzen wir den |&gt; Operator. Die Funktionsweise ist zugegebenermaßen etwas gewöhnungsbedürftig, mit etwas Übung erleichtert er unser Leben aber sehr. Pipes nehmen ein Objekt, das links von ihnen steht und übergeben es als erstes Argument in eine Funktion, die rechts davon bzw. in der Regel in einer neuen Zeile darunter steht. Wenn wir einen Ausdruck wie im Beispiel mit der Tabelle mit Pipes replizieren wollen, gehen wir von innen nach außen vor. Schauen wir uns das mal an, indem wir zunächst die Spalte A602 in einen Faktor umwandeln. Dazu übergeben wir die Spalte mit dem |&gt; Operator an die factor()-Funktion. R erkennt automatisch, dass das Objekt (also die Spalte) als erstes Argument genutzt werden muss. Wir müssen also nur das labels-Argument ergänzen. Vorsicht, da wir das Ergebnis nicht in einem neuen Objekt speichern, wird es in der Konsole angezeigt und ist sehr lang!\n\n# Wandelt die Geschlechtsabfrage in einen beschrifteten Faktor um\ndf_lokal$A602 |&gt;\n  factor(labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\")) \n\n   [1] weiblich     männlich     männlich     männlich     männlich    \n   [6] männlich     männlich     weiblich     weiblich     weiblich    \n  [11] männlich     weiblich     weiblich     männlich     männlich    \n  [16] männlich     weiblich     männlich     männlich     männlich    \n  [21] männlich     weiblich     männlich     männlich     männlich    \n  [26] weiblich     weiblich     männlich     männlich     männlich    \n  [31] weiblich     männlich     männlich     männlich     weiblich    \n  [36] männlich     männlich     weiblich     männlich     männlich    \n  [41] männlich     weiblich     männlich     männlich     weiblich    \n  [46] männlich     weiblich     männlich     männlich     männlich    \n  [51] weiblich     weiblich     weiblich     männlich     weiblich    \n  [56] männlich     weiblich     weiblich     männlich     männlich    \n  [61] männlich     weiblich     männlich     männlich     männlich    \n  [66] weiblich     weiblich     divers       weiblich     weiblich    \n  [71] weiblich     männlich     männlich     weiblich     männlich    \n  [76] weiblich     weiblich     weiblich     weiblich     weiblich    \n  [81] weiblich     männlich     männlich     weiblich     männlich    \n  [86] weiblich     männlich     divers       männlich     weiblich    \n  [91] weiblich     weiblich     weiblich     weiblich     weiblich    \n  [96] weiblich     weiblich     weiblich     weiblich     männlich    \n [101] männlich     männlich     männlich     männlich     männlich    \n [106] weiblich     weiblich     weiblich     männlich     männlich    \n [111] männlich     weiblich     weiblich     weiblich     männlich    \n [116] männlich     weiblich     weiblich     weiblich     männlich    \n [121] weiblich     männlich     männlich     weiblich     männlich    \n [126] männlich     weiblich     männlich     weiblich     weiblich    \n [131] weiblich     männlich     weiblich     männlich     männlich    \n [136] weiblich     weiblich     männlich     weiblich     männlich    \n [141] weiblich     männlich     weiblich     weiblich     weiblich    \n [146] männlich     männlich     weiblich     weiblich     weiblich    \n [151] männlich     weiblich     männlich     männlich     weiblich    \n [156] männlich     männlich     weiblich     weiblich     weiblich    \n [161] weiblich     männlich     weiblich     weiblich     &lt;NA&gt;        \n [166] männlich     männlich     weiblich     männlich     männlich    \n [171] divers       männlich     männlich     weiblich     männlich    \n [176] weiblich     weiblich     weiblich     weiblich     weiblich    \n [181] weiblich     weiblich     männlich     weiblich     weiblich    \n [186] männlich     divers       männlich     weiblich     weiblich    \n [191] männlich     weiblich     weiblich     weiblich     männlich    \n [196] weiblich     männlich     männlich     männlich     weiblich    \n [201] männlich     weiblich     weiblich     männlich     männlich    \n [206] weiblich     weiblich     männlich     männlich     weiblich    \n [211] männlich     weiblich     männlich     weiblich     weiblich    \n [216] weiblich     männlich     männlich     weiblich     männlich    \n [221] männlich     weiblich     weiblich     weiblich     männlich    \n [226] weiblich     weiblich     männlich     männlich     weiblich    \n [231] weiblich     weiblich     männlich     weiblich     weiblich    \n [236] weiblich     weiblich     weiblich     weiblich     weiblich    \n [241] weiblich     männlich     weiblich     weiblich     männlich    \n [246] männlich     männlich     weiblich     weiblich     weiblich    \n [251] weiblich     männlich     weiblich     weiblich     weiblich    \n [256] männlich     weiblich     weiblich     weiblich     weiblich    \n [261] weiblich     männlich     männlich     weiblich     männlich    \n [266] weiblich     männlich     männlich     weiblich     weiblich    \n [271] männlich     weiblich     männlich     weiblich     weiblich    \n [276] weiblich     männlich     männlich     weiblich     weiblich    \n [281] weiblich     weiblich     männlich     männlich     männlich    \n [286] weiblich     männlich     weiblich     männlich     männlich    \n [291] weiblich     männlich     weiblich     weiblich     männlich    \n [296] weiblich     männlich     weiblich     männlich     männlich    \n [301] männlich     männlich     weiblich     weiblich     männlich    \n [306] männlich     männlich     weiblich     weiblich     weiblich    \n [311] weiblich     weiblich     männlich     weiblich     weiblich    \n [316] weiblich     männlich     weiblich     männlich     männlich    \n [321] weiblich     männlich     weiblich     weiblich     männlich    \n [326] weiblich     weiblich     weiblich     weiblich     weiblich    \n [331] weiblich     weiblich     weiblich     weiblich     männlich    \n [336] weiblich     weiblich     männlich     männlich     weiblich    \n [341] weiblich     männlich     weiblich     weiblich     weiblich    \n [346] weiblich     weiblich     weiblich     männlich     weiblich    \n [351] weiblich     weiblich     männlich     weiblich     männlich    \n [356] weiblich     männlich     männlich     männlich     weiblich    \n [361] männlich     weiblich     weiblich     weiblich     weiblich    \n [366] weiblich     weiblich     weiblich     weiblich     männlich    \n [371] weiblich     männlich     weiblich     männlich     weiblich    \n [376] männlich     weiblich     weiblich     männlich     männlich    \n [381] weiblich     weiblich     weiblich     weiblich     männlich    \n [386] weiblich     weiblich     männlich     männlich     männlich    \n [391] weiblich     männlich     weiblich     weiblich     weiblich    \n [396] weiblich     weiblich     männlich     weiblich     männlich    \n [401] weiblich     männlich     männlich     weiblich     weiblich    \n [406] weiblich     männlich     männlich     männlich     männlich    \n [411] weiblich     männlich     weiblich     männlich     männlich    \n [416] männlich     weiblich     weiblich     weiblich     weiblich    \n [421] männlich     männlich     weiblich     weiblich     weiblich    \n [426] männlich     weiblich     weiblich     weiblich     weiblich    \n [431] männlich     männlich     weiblich     männlich     weiblich    \n [436] männlich     männlich     weiblich     weiblich     männlich    \n [441] weiblich     männlich     männlich     männlich     männlich    \n [446] weiblich     weiblich     männlich     männlich     weiblich    \n [451] männlich     männlich     weiblich     männlich     männlich    \n [456] weiblich     weiblich     weiblich     weiblich     männlich    \n [461] weiblich     weiblich     weiblich     weiblich     männlich    \n [466] weiblich     weiblich     weiblich     männlich     weiblich    \n [471] männlich     weiblich     männlich     weiblich     weiblich    \n [476] weiblich     weiblich     weiblich     weiblich     weiblich    \n [481] weiblich     weiblich     weiblich     männlich     weiblich    \n [486] weiblich     weiblich     männlich     weiblich     weiblich    \n [491] weiblich     weiblich     männlich     weiblich     divers      \n [496] weiblich     weiblich     männlich     weiblich     weiblich    \n [501] weiblich     weiblich     männlich     weiblich     weiblich    \n [506] weiblich     männlich     weiblich     männlich     weiblich    \n [511] weiblich     männlich     männlich     weiblich     weiblich    \n [516] weiblich     männlich     weiblich     weiblich     männlich    \n [521] weiblich     weiblich     weiblich     weiblich     weiblich    \n [526] weiblich     weiblich     weiblich     männlich     weiblich    \n [531] männlich     weiblich     männlich     männlich     männlich    \n [536] männlich     weiblich     weiblich     weiblich     weiblich    \n [541] männlich     weiblich     männlich     weiblich     weiblich    \n [546] weiblich     männlich     männlich     &lt;NA&gt;         keine Angabe\n [551] männlich     weiblich     männlich     weiblich     männlich    \n [556] weiblich     weiblich     männlich     weiblich     weiblich    \n [561] männlich     weiblich     männlich     weiblich     männlich    \n [566] männlich     weiblich     männlich     weiblich     weiblich    \n [571] weiblich     männlich     männlich     weiblich     weiblich    \n [576] männlich     weiblich     weiblich     männlich     weiblich    \n [581] weiblich     weiblich     männlich     männlich     weiblich    \n [586] männlich     weiblich     männlich     weiblich     weiblich    \n [591] weiblich     weiblich     männlich     weiblich     männlich    \n [596] männlich     weiblich     weiblich     weiblich     männlich    \n [601] weiblich     weiblich     männlich     weiblich     weiblich    \n [606] weiblich     weiblich     weiblich     weiblich     weiblich    \n [611] männlich     weiblich     männlich     weiblich     männlich    \n [616] männlich     weiblich     weiblich     männlich     männlich    \n [621] weiblich     männlich     keine Angabe weiblich     männlich    \n [626] männlich     weiblich     männlich     männlich     männlich    \n [631] männlich     weiblich     weiblich     weiblich     männlich    \n [636] weiblich     weiblich     weiblich     weiblich     weiblich    \n [641] männlich     weiblich     männlich     weiblich     weiblich    \n [646] weiblich     weiblich     weiblich     weiblich     männlich    \n [651] männlich     weiblich     männlich     männlich     weiblich    \n [656] weiblich     weiblich     weiblich     weiblich     männlich    \n [661] weiblich     weiblich     weiblich     weiblich     weiblich    \n [666] männlich     männlich     weiblich     weiblich     männlich    \n [671] weiblich     weiblich     weiblich     weiblich     weiblich    \n [676] weiblich     weiblich     männlich     männlich     weiblich    \n [681] weiblich     männlich     weiblich     männlich     weiblich    \n [686] weiblich     weiblich     männlich     weiblich     männlich    \n [691] weiblich     weiblich     weiblich     weiblich     männlich    \n [696] weiblich     männlich     weiblich     weiblich     männlich    \n [701] weiblich     weiblich     weiblich     männlich     männlich    \n [706] weiblich     weiblich     männlich     männlich     männlich    \n [711] männlich     männlich     weiblich     männlich     weiblich    \n [716] weiblich     weiblich     männlich     männlich     männlich    \n [721] weiblich     weiblich     weiblich     männlich     männlich    \n [726] männlich     weiblich     weiblich     männlich     männlich    \n [731] männlich     weiblich     weiblich     weiblich     weiblich    \n [736] männlich     weiblich     weiblich     weiblich     weiblich    \n [741] männlich     weiblich     weiblich     weiblich     weiblich    \n [746] weiblich     männlich     &lt;NA&gt;         männlich     weiblich    \n [751] weiblich     weiblich     weiblich     weiblich     weiblich    \n [756] weiblich     männlich     weiblich     männlich     weiblich    \n [761] weiblich     männlich     männlich     männlich     weiblich    \n [766] weiblich     weiblich     weiblich     weiblich     weiblich    \n [771] weiblich     weiblich     männlich     männlich     weiblich    \n [776] männlich     weiblich     weiblich     männlich     männlich    \n [781] weiblich     weiblich     männlich     weiblich     männlich    \n [786] weiblich     divers       weiblich     weiblich     weiblich    \n [791] weiblich     weiblich     männlich     weiblich     männlich    \n [796] weiblich     weiblich     männlich     weiblich     männlich    \n [801] männlich     männlich     männlich     &lt;NA&gt;         weiblich    \n [806] keine Angabe männlich     männlich     männlich     männlich    \n [811] weiblich     weiblich     weiblich     weiblich     männlich    \n [816] männlich     männlich     männlich     männlich     weiblich    \n [821] weiblich     männlich     weiblich     weiblich     weiblich    \n [826] weiblich     männlich     weiblich     weiblich     männlich    \n [831] weiblich     weiblich     männlich     weiblich     männlich    \n [836] weiblich     weiblich     weiblich     männlich     weiblich    \n [841] weiblich     weiblich     weiblich     weiblich     keine Angabe\n [846] weiblich     weiblich     männlich     männlich     weiblich    \n [851] weiblich     weiblich     weiblich     männlich     keine Angabe\n [856] keine Angabe weiblich     männlich     weiblich     männlich    \n [861] männlich     weiblich     weiblich     männlich     weiblich    \n [866] weiblich     weiblich     männlich     weiblich     weiblich    \n [871] männlich     männlich     weiblich     männlich     männlich    \n [876] weiblich     weiblich     männlich     weiblich     weiblich    \n [881] männlich     weiblich     männlich     weiblich     männlich    \n [886] männlich     männlich     männlich     weiblich     männlich    \n [891] weiblich     weiblich     männlich     männlich     weiblich    \n [896] weiblich     weiblich     weiblich     weiblich     weiblich    \n [901] männlich     weiblich     weiblich     weiblich     männlich    \n [906] weiblich     weiblich     weiblich     weiblich     männlich    \n [911] weiblich     männlich     weiblich     weiblich     weiblich    \n [916] männlich     weiblich     weiblich     männlich     männlich    \n [921] männlich     männlich     männlich     weiblich     männlich    \n [926] männlich     weiblich     weiblich     weiblich     männlich    \n [931] männlich     weiblich     männlich     weiblich     männlich    \n [936] weiblich     weiblich     weiblich     weiblich     männlich    \n [941] männlich     weiblich     weiblich     weiblich     männlich    \n [946] männlich     keine Angabe weiblich     männlich     männlich    \n [951] weiblich     männlich     weiblich     weiblich     &lt;NA&gt;        \n [956] männlich     weiblich     weiblich     weiblich     männlich    \n [961] männlich     männlich     weiblich     weiblich     männlich    \n [966] weiblich     männlich     weiblich     männlich     männlich    \n [971] weiblich     männlich     männlich     männlich     männlich    \n [976] weiblich     weiblich     weiblich     weiblich     weiblich    \n [981] männlich     weiblich     weiblich     männlich     weiblich    \n [986] männlich     männlich     weiblich     männlich     männlich    \n [991] männlich     weiblich     weiblich     männlich     männlich    \n [996] männlich     weiblich     weiblich     weiblich     weiblich    \n[1001] &lt;NA&gt;         weiblich     weiblich     weiblich     weiblich    \n[1006] weiblich     weiblich     weiblich     männlich     männlich    \n[1011] weiblich     weiblich     männlich     weiblich     weiblich    \n[1016] weiblich     weiblich     männlich     männlich     weiblich    \n[1021] männlich     männlich     weiblich     weiblich     männlich    \n[1026] weiblich     männlich     männlich     männlich     männlich    \n[1031] männlich     weiblich     weiblich     männlich     männlich    \n[1036] weiblich     weiblich     weiblich     männlich     weiblich    \n[1041] weiblich     weiblich     weiblich     weiblich     weiblich    \n[1046] männlich     weiblich     weiblich     weiblich     weiblich    \n[1051] männlich     männlich     männlich     männlich     männlich    \n[1056] weiblich     weiblich     männlich     weiblich     weiblich    \n[1061] männlich     weiblich     weiblich     männlich     männlich    \n[1066] weiblich     weiblich     weiblich     weiblich     männlich    \n[1071] männlich     männlich     weiblich     weiblich     weiblich    \n[1076] männlich     weiblich     weiblich     weiblich     männlich    \n[1081] weiblich     männlich     weiblich     weiblich     männlich    \n[1086] männlich     weiblich     männlich     männlich     weiblich    \n[1091] weiblich     männlich     weiblich     männlich     weiblich    \n[1096] &lt;NA&gt;         weiblich     weiblich     weiblich     weiblich    \n[1101] weiblich     männlich     weiblich     männlich     weiblich    \n[1106] weiblich     weiblich     weiblich     weiblich     weiblich    \n[1111] männlich     männlich     weiblich     weiblich     weiblich    \n[1116] weiblich     weiblich     weiblich     weiblich     weiblich    \n[1121] weiblich     männlich     weiblich     weiblich     männlich    \n[1126] weiblich     weiblich     männlich     weiblich     männlich    \n[1131] männlich     männlich     männlich     weiblich     männlich    \n[1136] männlich     weiblich     männlich     männlich     männlich    \n[1141] weiblich     männlich     männlich     weiblich     keine Angabe\n[1146] männlich     männlich     männlich     männlich     männlich    \n[1151] männlich     weiblich     weiblich     männlich     weiblich    \n[1156] männlich     weiblich     weiblich     weiblich     weiblich    \n[1161] weiblich     männlich     männlich     keine Angabe männlich    \n[1166] weiblich     männlich     weiblich     weiblich     männlich    \n[1171] weiblich     weiblich     männlich     männlich     männlich    \n[1176] männlich     weiblich     männlich     weiblich     weiblich    \n[1181] männlich     männlich     männlich     weiblich     weiblich    \n[1186] weiblich     weiblich     keine Angabe weiblich     männlich    \n[1191] männlich     weiblich     männlich     männlich     weiblich    \n[1196] männlich     männlich     männlich     männlich     männlich    \n[1201] weiblich     männlich     weiblich     männlich     keine Angabe\n[1206] weiblich     männlich     weiblich     weiblich     weiblich    \n[1211] männlich     weiblich     männlich     männlich     weiblich    \n[1216] weiblich     weiblich     männlich     weiblich     männlich    \n[1221] männlich     weiblich     weiblich     weiblich     männlich    \n[1226] weiblich     männlich     weiblich     männlich     weiblich    \n[1231] weiblich     männlich     männlich     männlich     weiblich    \n[1236] weiblich     männlich     weiblich     männlich     weiblich    \n[1241] männlich     weiblich     männlich     männlich     männlich    \n[1246] männlich     weiblich     weiblich     männlich     männlich    \n[1251] weiblich     weiblich     männlich     weiblich     männlich    \n[1256] weiblich     weiblich     weiblich     &lt;NA&gt;         weiblich    \n[1261] weiblich     weiblich     männlich     männlich     männlich    \n[1266] weiblich     männlich     männlich     männlich     weiblich    \n[1271] männlich     männlich     männlich     weiblich     weiblich    \n[1276] keine Angabe männlich     männlich     männlich     männlich    \n[1281] weiblich     männlich     weiblich     weiblich     männlich    \n[1286] männlich     weiblich     männlich     &lt;NA&gt;         weiblich    \n[1291] weiblich     männlich     weiblich     männlich     männlich    \n[1296] weiblich     männlich     weiblich     männlich     weiblich    \n[1301] männlich     männlich     weiblich     weiblich     männlich    \n[1306] weiblich     männlich     männlich     männlich     weiblich    \n[1311] weiblich     männlich     weiblich     männlich     weiblich    \n[1316] männlich     weiblich     weiblich     männlich     weiblich    \n[1321] weiblich     weiblich     männlich     weiblich     weiblich    \n[1326] männlich     weiblich     weiblich     männlich     weiblich    \n[1331] weiblich     weiblich     weiblich     weiblich     weiblich    \n[1336] weiblich     weiblich     weiblich     weiblich     männlich    \n[1341] weiblich     weiblich     weiblich     männlich     männlich    \n[1346] weiblich     männlich     weiblich     männlich     männlich    \n[1351] weiblich     weiblich     männlich     männlich     männlich    \n[1356] weiblich     männlich     weiblich     männlich     männlich    \n[1361] weiblich     weiblich     weiblich     weiblich     männlich    \n[1366] männlich     weiblich     männlich     männlich     männlich    \n[1371] weiblich     weiblich     weiblich     weiblich     männlich    \n[1376] weiblich     männlich     männlich     weiblich     weiblich    \n[1381] männlich     männlich     weiblich     männlich     weiblich    \n[1386] männlich     männlich     keine Angabe weiblich     männlich    \n[1391] weiblich     weiblich     männlich     weiblich     männlich    \n[1396] weiblich     männlich     weiblich     weiblich     weiblich    \n[1401] männlich     &lt;NA&gt;         männlich     männlich     männlich    \n[1406] männlich     weiblich     weiblich     männlich     weiblich    \n[1411] weiblich     weiblich     männlich     männlich     männlich    \n[1416] männlich     männlich     männlich     weiblich     männlich    \n[1421] männlich     weiblich     weiblich     männlich     männlich    \n[1426] männlich     männlich     männlich     männlich     männlich    \n[1431] weiblich     männlich     weiblich     männlich     weiblich    \n[1436] weiblich     weiblich     weiblich     weiblich     männlich    \n[1441] weiblich     männlich     männlich     männlich     männlich    \n[1446] männlich     weiblich     weiblich     männlich     männlich    \n[1451] weiblich     männlich     männlich     weiblich     männlich    \n[1456] männlich     männlich     &lt;NA&gt;         weiblich     männlich    \n[1461] männlich     weiblich     weiblich     männlich     männlich    \n[1466] männlich     weiblich     weiblich     männlich     männlich    \n[1471] weiblich     keine Angabe männlich     weiblich     weiblich    \n[1476] weiblich     keine Angabe weiblich     weiblich     männlich    \n[1481] weiblich     weiblich     männlich     männlich     weiblich    \n[1486] männlich     weiblich     weiblich     männlich     männlich    \n[1491] weiblich     männlich     männlich     männlich     weiblich    \n[1496] weiblich     männlich     männlich     weiblich     männlich    \n[1501] weiblich     männlich     männlich     weiblich     männlich    \n[1506] männlich     männlich     männlich     weiblich     männlich    \n[1511] männlich     weiblich     weiblich     weiblich     männlich    \n[1516] männlich     männlich     männlich     weiblich     männlich    \n[1521] weiblich     weiblich     weiblich     männlich     männlich    \n[1526] männlich     männlich     weiblich     männlich     weiblich    \n[1531] weiblich     männlich     männlich     männlich     weiblich    \n[1536] weiblich     männlich     männlich     männlich     männlich    \n[1541] männlich     weiblich     männlich     männlich     männlich    \n[1546] weiblich     weiblich     männlich     männlich     männlich    \n[1551] männlich     weiblich     weiblich     weiblich     weiblich    \n[1556] männlich     weiblich     weiblich     weiblich     weiblich    \n[1561] männlich     männlich     weiblich     weiblich     weiblich    \n[1566] männlich     männlich     weiblich     weiblich     weiblich    \n[1571] männlich     männlich     männlich     männlich     weiblich    \n[1576] weiblich     weiblich     weiblich     weiblich     männlich    \n[1581] männlich     weiblich     männlich     männlich     weiblich    \n[1586] weiblich     &lt;NA&gt;         männlich     weiblich     weiblich    \n[1591] weiblich     männlich     männlich     weiblich     männlich    \n[1596] weiblich     männlich     weiblich     männlich     divers      \n[1601] weiblich     männlich     weiblich     männlich     weiblich    \n[1606] männlich     weiblich     weiblich     männlich     weiblich    \n[1611] weiblich     weiblich     weiblich     männlich     weiblich    \n[1616] männlich     weiblich     weiblich     weiblich     männlich    \n[1621] männlich     weiblich     männlich     männlich     männlich    \n[1626] weiblich     männlich     männlich     weiblich     männlich    \n[1631] weiblich     männlich     weiblich     männlich     männlich    \n[1636] weiblich     weiblich     weiblich     männlich     männlich    \n[1641] weiblich     männlich     weiblich     männlich     männlich    \n[1646] männlich     weiblich     weiblich     männlich     männlich    \n[1651] männlich     männlich     weiblich     weiblich     weiblich    \n[1656] männlich     weiblich     männlich     weiblich     männlich    \n[1661] weiblich     weiblich     weiblich     weiblich     weiblich    \n[1666] weiblich     weiblich     weiblich     männlich     weiblich    \n[1671] männlich     weiblich     männlich     weiblich     weiblich    \n[1676] männlich     männlich     männlich     weiblich     männlich    \n[1681] männlich     männlich     männlich     weiblich     weiblich    \n[1686] männlich     männlich     männlich     weiblich     männlich    \n[1691] weiblich     männlich     männlich     männlich     männlich    \n[1696] männlich     weiblich     weiblich     männlich     männlich    \n[1701] männlich     männlich     männlich     weiblich     weiblich    \n[1706] männlich     weiblich     männlich     weiblich     männlich    \n[1711] männlich     weiblich     weiblich     männlich     weiblich    \n[1716] männlich     weiblich     männlich     männlich     männlich    \n[1721] keine Angabe männlich     männlich     weiblich     männlich    \n[1726] männlich     männlich     weiblich     keine Angabe männlich    \n[1731] männlich     weiblich     männlich     männlich     weiblich    \n[1736] männlich     männlich     männlich     männlich     weiblich    \n[1741] weiblich     männlich     männlich     weiblich     weiblich    \n[1746] weiblich     weiblich     männlich     weiblich     männlich    \n[1751] männlich     weiblich     weiblich     weiblich     weiblich    \n[1756] männlich     männlich     männlich     weiblich     weiblich    \n[1761] männlich     weiblich     weiblich     weiblich     männlich    \n[1766] männlich     männlich     weiblich     weiblich     männlich    \n[1771] männlich     weiblich     weiblich     männlich     weiblich    \n[1776] weiblich     männlich     weiblich     männlich     weiblich    \n[1781] weiblich     weiblich     keine Angabe weiblich     männlich    \n[1786] weiblich     männlich     männlich     männlich     männlich    \n[1791] männlich     männlich     weiblich     weiblich     weiblich    \n[1796] männlich     männlich     weiblich     männlich     männlich    \n[1801] weiblich     weiblich     männlich     männlich     männlich    \n[1806] weiblich     männlich     weiblich     weiblich     weiblich    \n[1811] männlich     weiblich     männlich     männlich     weiblich    \n[1816] weiblich     weiblich     weiblich     weiblich     weiblich    \n[1821] weiblich     männlich     weiblich     weiblich     männlich    \n[1826] weiblich     männlich     weiblich     männlich     männlich    \n[1831] weiblich     weiblich     weiblich     weiblich     männlich    \n[1836] männlich     weiblich     männlich     männlich     männlich    \n[1841] männlich     weiblich     männlich     weiblich     männlich    \n[1846] männlich    \nLevels: männlich weiblich divers keine Angabe\n\n\nIm nächsten Schritt übergeben wir diesen neuen Faktor mit |&gt; an die table()-Funktion, die keine weiteren Argumente benötigt.\n\n# Wandelt die Geschlechtsabfrage in einen beschrifteten Faktor um und übergibt diesen an die table()-Funktion.\ndf_lokal$A602 |&gt;\n  factor(labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\")) |&gt;\n  table()\n\n\n    männlich     weiblich       divers keine Angabe \n         813          996            7           18 \n\n\nDiese Häufigkeitstabelle können wir nun an die prop.table()-Funktion übergeben, um eine Tabelle mit relativen Häufigkeiten zu bekommen.\n\n# Wandelt die Geschlechtsabfrage in einen beschrifteten Faktor um und übergibt diesen an die table()-Funktion und erstellt dann eine Tabelle mit relativen Häufigkeiten\ndf_lokal$A602 |&gt;\n  factor(labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\")) |&gt;\n  table() |&gt;\n  prop.table()\n\n\n    männlich     weiblich       divers keine Angabe \n 0.443293348  0.543075245  0.003816794  0.009814613 \n\n\nAls letztes haben wir oben die Werte dieser Tabelle mit 100 multipliziert und das Ergebnis auf zwei Nachkommastellen gerundet. Wenn wir versuchen, diesen Schritt umzusetzen, sehen wir zwar, dass die Prozentwerte richtig angegeben werden, allerdings funktioniert das Runden nicht. Kurz gesagt liegt das daran, dass R es nicht schafft den Ausdruck prop.table()*100 weiterzuleiten.\n\n# Wandelt die Geschlechtsabfrage in einen beschrifteten Faktor um und übergibt diesen an die table()-Funktion und erstellt dann eine Tabelle mit relativen Häufigkeiten, die in Prozentwerte umgerechnet werden; das Runden funktioniert nicht\ndf_lokal$A602 |&gt;\n  factor(labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\")) |&gt;\n  table() |&gt;\n  prop.table() * 100 |&gt;\n  round(2)\n\n\n    männlich     weiblich       divers keine Angabe \n  44.3293348   54.3075245    0.3816794    0.9814613 \n\n\nWir können Abhilfe schaffen, indem wir das Ergebnis der prop.table()-Funktion direkt an die round()-Funktion weitergeben und erst deren Ergebnis runden. Wichtig ist dabei, dass wir erst auf 4 Nachkommastellen runden und diese dann mit 100 multiplizieren, sodass wir am Ende eine Zahl mit 4 Stellen (2 vor und 2 nach dem Komma) bekommen.\n\n# Wandelt die Geschlechtsabfrage in einen beschrifteten Faktor um und übergibt diesen an die table()-Funktion und erstellt dann eine Tabelle mit relativen Häufigkeiten, die in Prozentwerte umgerechnet werden; das Ergebnis wird gerundet\ndf_lokal$A602 |&gt;\n  factor(labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\")) |&gt;\n  table() |&gt;\n  prop.table() |&gt;\n  round(4)*100\n\n\n    männlich     weiblich       divers keine Angabe \n       44.33        54.31         0.38         0.98",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objekte, Daten, Funktionen</span>"
    ]
  },
  {
    "objectID": "Objekte, Daten, Funktionen.html#footnotes",
    "href": "Objekte, Daten, Funktionen.html#footnotes",
    "title": "2  Objekte, Daten, Funktionen",
    "section": "",
    "text": "In manchen Quellen werden Sie statt des Pfeils ein Gleichheitszeichen sehen. Im Prinzip sind die beiden äquivalent. Wir nutzen hier den Pfeil, werden aber später, wenn wir uns mit Funktionen befassen, auch das Gleichheitszeichen verwenden.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objekte, Daten, Funktionen</span>"
    ]
  },
  {
    "objectID": "Pakete und Datentransformationen.html",
    "href": "Pakete und Datentransformationen.html",
    "title": "3  Pakete und Datentransformationen",
    "section": "",
    "text": "3.1 Pakete\nZwar kann R direkt nach der Installation schon relativ viel, aber gleichzeitig gibt es viele Aufgaben, die wir nur lösen können, indem wir R erweitern. Dazu nutzen wir sogenannte Pakete. Vereinfacht gesagt handelt es sich dabei um Sammlungen von Funktionen. Im Laufe des Kurses werden wir einige Pakete benötigen, die wir entsprechend nach und nach kennenlernen werden. Zunächst schauen wir uns an, wie wir Pakete installieren und laden können.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pakete und Datentransformationen</span>"
    ]
  },
  {
    "objectID": "Pakete und Datentransformationen.html#pakete",
    "href": "Pakete und Datentransformationen.html#pakete",
    "title": "3  Pakete und Datentransformationen",
    "section": "",
    "text": "3.1.1 Pakete installieren\nUm ein Paket zu installieren, müssen Sie zunächst dessen Namen kennen. Dann haben Sie zwei Möglichkeiten:\n\nSie können die Funktion install.packages() nutzen. Als Argument nimmt die Funktion den Namen mindestens eines Pakets in Anführungszeichen, oder mehrere Pakete durch Kommata getrennt. Für den weiteren Verlauf des Kapitels werden wir das Paket tidyverse nutzen. Damit wir das Paket nicht jedes Mal aufs Neue installieren, wenn wir ein Skript ausführen, können wir die Funktion mit dem folgenden Befehl aufrufen.\n\n# Prüft, ob das Paket \"tidyverse\" installiert ist. Falls nicht, wird es installiert und geladen\nif(!require(tidyverse)){\n  install.packages(\"tidyverse\")\n  library(tidyverse)\n} \n\nLade nötiges Paket: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nDie Funktionsweise ist für Sie nicht entscheidend, aber für den Fall, dass es Sie interessiert, wird sie hier noch kurz erklärt: Wir starten mit einem sogenannten if-Befehl, dessen Aufbau immer gleich ist. In der Klammer steht eine Bedingung, die geprüft wird. Sofern sie zutrifft, wird der Teil in der in den geschweiften Klammern ({}) ausgeführt. Die Bedingung in einem if-Befehl muss immer wahr oder falsch sein bzw. TRUE oder FALSE zurückgeben. Wird TRUE zurückgegeben, gilt die Bedingung als erfüllt, andernfalls nicht. In diesem Fall wird die Bedingung !require(tidyverse) geprüft. require() ist eine Funktion, die versucht ein Paket zu laden. Falls es aber nicht installiert ist, gibt sie FALSE zurück. Durch das Ausrufezeichen wird dieser Ausdruck negiert. Etwas, das nicht FALSE ist, ist in der R-Logik TRUE. In der Summe bedeutet das also, dass wir erst prüfen, ob das Paket vorhanden ist, und falls nicht, wird es installiert.\nAlternativ können Sie im Bereich unten rechts auf den Reiter Packages und dort auf Install klicken.\n\n\n\n\n\n\nAnschließend öffnet sich ein Fenster, in dem Sie den oder die Namen der gewünschten Pakete eintragen können.\n\n\n\n\n\n\n\n3.1.2 Pakete laden\nAuch zum Laden von Paketen haben Sie mehrere Möglichkeiten:\n\nSie können den Befehl von oben nutzen, da die require()-Funktion versucht, das angegebene Paket zu laden. Sofern das Paket bereits installiert ist, wird es einfach nur geladen.\nSie können auch die library()-Funktion verwenden. Auch hier müssen Sie nur den Namen des Pakets als Argument übergeben - diesmal allerdings ohne Anführungszeichen. Der Unterschied zu require() ist, dass die Funktion eine Fehlermeldung zurückgibt, falls das Paket nicht vorhanden ist.\n\n# Lädt das Paket \"tidyverse\"\nlibrary(tidyverse)\n\nAlternativ können Sie wieder auf den Reiter Packages unten rechts navigieren, den Namen des Pakets im Suchfeld eingeben und anschließend einen Haken im entsprechenden Feld setzen.\n\n\n\n\n\n\n\n\n3.1.3 Das tidyverse\nIn den beiden vorherigen Abschnitten haben wir das tidyverse-Paket installiert und geladen. Streng genommen handelt es sich dabei nicht um ein einzelnes Paket, sondern gleich eine ganze Sammlung. Das sehen Sie auch, nachdem Sie das Paket geladen haben. In der Konsole wird uns gleich eine ganze Reihe an Paketen angezeigt, die geladen wurden:\n\n\n\n\n\nSchauen wir uns einmal kurz an, worum es sich bei diesen Paketen handelt:\n\ndplyr ist gewissermaßen das Herzstück des tidyverse. Das Paket enthält zahlreiche Funktionen, die wir für die Datentransformation benötigen. Sie können damit z. B. neue Variablen berechnen, Datensätze filtern, einzelne Spalten selektieren und vieles mehr. Wir werden das Paket hauptsächlich nutzen, um Daten in ein Format zu bringen, mit dem wir weiterarbeiten können und um Datensätze deskriptiv auszuwerten.\nforcats erleichtert die Arbeit mit kategorischen Variablen, also z. B. dem Geschlecht der Befragten aus dem letzten Kapitel.\nggplot2 ist ein Paket zum Erstellen von Grafiken. Wir werden es in den kommenden Wochen näher kennenlernen.\nlubridate enthält Funktionen, die das Arbeiten mit Zeit- und Datumsvariablen erleichtern. In diesem Kurs werden wir solchen Daten allerdings nicht begegnen.\npurrr ist eher etwas für fortgeschrittene Programmierer:innen. Es enthält Funktionen, die das Arbeiten anderen R-Funktionen optimieren können. Keine Sorge: Wir bleiben in diesem Kurs bei den Basics und werden uns nicht damit befassen.\nreadr ist ein Paket zum Laden von Daten, zum Beispiel enthält es die read_csv()-Funktion als Alternative zur read.csv()-Funktion aus dem letzten Kapitel. Das mag redundant wirken, zeigt aberin erster Linie, dass es in R in der Regel viele verschiedene Möglichkeiten gibt, ein gegebenes Problem zu lösen. Der Grund, warum readr entwickelt wurde, ist, dass die enthaltenen Funktionen oftmals schneller sind als ihre in R enthaltenen Pendants. Allerdings zeigt sich dieser Geschwindigkeitsvorteil hauptsächlich bei sehr großen Datensätzen.\nstringr ist ein Paket, um Strings zu manipulieren. Man kann damit z. B. prüfen, ob eine Zeichenfolge bestimmte Zeichen enthält, die wiederum entfernt oder geändert werden können.\ntibble ist nicht nur der Name des Pakets, sondern auch der Name des Objekttyps, den das Paket erzeugt. Im Prinzip ist die Kernfunktion tibble() eine moderne Version der data.frame()-Funktion aus dem letzten Kapitel.\ntidyr enthält Funktionen, mit denen wir Datensätze bzw. tibbles in ein bestimmtes Format bringen können, das schlicht tidy genannt wird. Dieses Format haben Sie im letzten Kapitel bereits kennengelernt, wenn auch nicht unter diesem Namen. Es bedeutet nicht mehr als die Idee, dass in einem gut strukturierten Datensatz jede Zeile einem Fall und jede Spalte einer Variable entspricht.\n\nDer Vorteil des tidyverse ist, dass Sie immer nur ein Paket laden müssen, um einen umfangreichen Werkzeugkasten nutzen zu können. Gleichwohl kann das Paket am Anfang aufgrund seines Umfangs etwas abschreckend und unzugänglich wirken. Hier können sogenannte Cheatsheets, also Spickzettel, Abhilfe schaffen. Darauf werden die wichtigsten Funktionen der einzelnen Pakete vorgestellt und erklärt. Für die meisten tidyverse-Pakete finden Sie solche Cheatsheets auf der Webseite von posit, dem Unternehmen, das auch RStudio entwickelt: https://posit.co/resources/cheatsheets/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pakete und Datentransformationen</span>"
    ]
  },
  {
    "objectID": "Pakete und Datentransformationen.html#datentransformationen",
    "href": "Pakete und Datentransformationen.html#datentransformationen",
    "title": "3  Pakete und Datentransformationen",
    "section": "3.2 Datentransformationen",
    "text": "3.2 Datentransformationen\nMit Hilfe des tidyverse und insbesondere den in dplyr enthaltenen Funktionen können wir Datensätze transformieren. Das ist etwas vereinfacht ausgedrückt, denn eigentlich verbirgt sich hinter der Transformation von Daten und Datensätzen eine Vielzahl von Dingen. Einige davon werden wir uns nun anschauen Ganz konkret das Umbenennen von Spalten, das Hinzufügen neuer Spalten, das Selektieren bestimmter Spalten und das Filtern von Fällen, also Zeilen.\n\n3.2.1 Spalten umbenennen\nUm eine oder mehrere Spalten umzubenennen, können wir die rename()-Funktion nutzen. Das Schema ist dabei relativ einfach: neuerName = alterName. In der Praxis sieht das so aus:\nZuerst lesen wir die Daten ein. Falls Sie das Skript aus dem letzten Kapitel nutzen, sollten Sie diesen Befehl schon im Skript haben und können ihn entsprechend ausführen.\n\n# Einlesen der Daten \n\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\nDamit der neue Name im Datensatz gespeichert wird, müssen wir das Ergebnis der rename()-Funktion in einem Objekt speichern. In solchen Fällen ist es häufig sinnvoll, das vorhandene Datensatzobjekt zu überschreiben. Mit der Pipe (|&gt;) übergeben wir den Datensatz an die rename()-Funktion.\n\n# Benennt die Spalte A101_01 in Themeninteresse_lokal um\n\ndf_lokal &lt;- df_lokal |&gt;\n  rename(Themeninteresse_lokal = A101_01)\n\n\n\n3.2.2 Neue Spalten hinzufügen\nEs gibt zwei Situationen, in denen es sinnvoll ist, einem Datensatz eine neue Spalte hinzuzufügen:\n\nWenn Sie eine neue Variable berechnen wollen oder\nwenn Sie eine bestehende Variable verändern wollen.\n\nDazu können wir die dplyr-Funktion mutate() nutzen. Als Argument übergeben wir der Funktion den Namen der neuen Spalte und deren Inhalt. Das Schema sieht so aus: mutate(nameDerNeuenSpalte = fester Wert, Berechnung oder Veränderung einer bestehenden Spalte). Schauen wir uns die Funktion einmal am Beispiel aus dem letzten Kapitel an. Dort haben wir die Spalte A602 in einen Faktor umgewandelt und eine Häufigkeitstabelle ausgegeben. Das geht auch mit der mutate()-Funktion. Dieser sagen wir, dass wir eine neue Spalte namens geschlecht erstellen wollen. Diese ist gleich einem Faktor aus der Spalte A602, mit den entsprechenden Wertbeschriftungen.\n\n# Erstellt einen Faktor aus der Spalte A602 und speichert diesen in der neuen Spalte Geschlecht\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geschlecht = factor(A602, labels = c(\"männlich\", \"weiblich\", \"divers\", \"keine Angabe\")))\n\n# Häufigkeitstabelle der neuen Spalte\n\ntable(df_lokal$geschlecht)\n\n\n    männlich     weiblich       divers keine Angabe \n         813          996            7           18 \n\n\nOben haben Sie erfahren, dass man mit mutate() neue Spalten hinzufügen kann. Sie können aber auch bestehende Spalten verändern. Das sollten aber die absolute Aussanahme sein! Eine davon ist, wenn Sie zum Erstellen neuer Spalten mehrere Schritte benötigen. Zum Beispiel können wir mit der Antwort \"keine Angabe\" auf die Geschlechtsabfrage wenig anfangen. Für viele Analysen wäre es daher sinnvoll, wenn diese Werte als fehlende Werte codiert wären. In R nennen wir solche Werte NAs (für not available). Um diese Werte entsprechend als fehlend zu codieren, können wir in zwei Schritten vorgehen. Erst codieren wir den Wert 4 aus der Ausgangsspalte A602 als fehlend. Das geht mit der na_if()-Funktion aus dem dyplr-Paket. Dieser Funktion übergeben wir ein Objekt und den Wert, der in NA umgewandelt werden soll. Im zweiten Schritt können wir dann einen Faktor mit den drei Kategorien \"männlich\", \"weiblich\" und \"divers\" erstellen.\n\n# Erstellt eine Spalte aus der Geschlechtsabfrage. Erst wird der Wert \"keine Angabe\" als fehlend deklariertm dann wird ein Faktor mit den übrigen drei Kategorien erstellt\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geschlechtMitNAs = na_if(A602, 4)) |&gt;\n  mutate(geschlechtMitNAs = factor(geschlechtMitNAs, labels = c(\"männlich\", \"weiblich\", \"divers\")))\n\n# Häufigkeitstabelle der neuen Spalte\n\ntable(df_lokal$geschlechtMitNAs)\n\n\nmännlich weiblich   divers \n     813      996        7 \n\n\nSchauen wir uns noch ein weiteres Beispiel an. In der Spalte A601_01 ist das Geburtsjahr der Befragten angegeben, das in ein offenes Textfeld eingetragen werden sollte. Mit der unique()-Funktion können wir uns alle Werte anzeigen lassen, die mindestens einmal in der Spalte vorkommen. Anders als die table()-Funktion erhalten wir aber keine Häufigkeiten. unique() ist also praktisch, wenn wir nur wissen wollen, welche Werte eine Spalte eigentlich enthält.\n\n# Zeigt alle Werte von A601_01, die mindestens einmal vorkommen\n\nunique(df_lokal$A601_01)\n\n [1] \"1981\"         \"1957\"         \"1962\"         \"1997\"         \"1967\"        \n [6] \"1984\"         \"1961\"         \"1989\"         \"1978\"         \"1964\"        \n[11] \"1971\"         \"1993\"         \"1973\"         \"1991\"         \"Deutschland \"\n[16] \"2005\"         \"1972\"         \"1988\"         \"1965\"         \"1966\"        \n[21] \"1982\"         \"1958\"         \"1985\"         \"1953\"         \"1974\"        \n[26] \"1979\"         \"1963\"         \"1970\"         \"1959\"         \"1954\"        \n[31] \"1990\"         \"1975\"         \"1976\"         \"1960\"         \"1956\"        \n[36] \"1999\"         \"1968\"         \"1969\"         \"1987\"         \"2004\"        \n[41] \"1983\"         \"1980\"         \"2002\"         \"1977\"         \"1952\"        \n[46] \"1950\"         \"1995\"         \"01.11.1967\"   \"1996\"         \"1944\"        \n[51] \"1994\"         \"1986\"         \"1955\"         \"1948\"         \"1951\"        \n[56] \"2001\"         \"1942\"         \"1949\"         \"2000\"         \"1947\"        \n[61] \"1942 \"        \"1941\"         \"1939\"         \"w000\"         \"2003\"        \n[66] \"1998\"         \"1992\"         \"1946\"         \"1975 \"        NA            \n[71] \"1945\"         \"Deutschland\"  \"Keine Angabe\" \"1964 \"        \"1943\"        \n[76] \"1936\"         \"1938\"         \"2007\"         \"1957 \"        \"1935\"        \n[81] \"1972 \"        \"1940\"         \"1965 \"        \"1934\"         \"16.02.1957\"  \n[86] \"Datenschutz \" \"&lt;60\"          \"1933\"         \"24.10.1945\"   \"I 966\"       \n[91] \"1937\"         \"1961 \"       \n\n\nWie es aussieht, haben einige Befragte die Frage nicht ganz verstanden und unsinnige Werte (z. B. Deutschland) oder ihren genauen Geburtstag angegeben. Wieder andere haben ein Leerzeichen am Ende der Jahresangabe angehängt. All das führt dazu, dass die Spalte nicht als Zahl, sondern als String gespeichert ist. Das ist ziemlich unpraktisch!\nUm diese Spalte etwas aufzuräumen, müssen wir in mehreren Schritten vorgehen. Das Ziel ist, pro Person entweder eine Zahl mit vier Ziffern oder ein NA zu erhalten.\nWir fangen mit den Leerzeichen an. Diese können wir mit der str_trim()-Funktion aus dem stringr-Paket entfernen. Dazu übergeben wir der Funktion einfach die entsprechende Spalte. Das Ergebnis speichern wir in der neuen Spalte geburtsjahr.\n\n# Entfernt Leerzeichen am Anfang und Ende der Strings und speichert das Ergebnis in der neuen Spalte \"geburtsjahr\"\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geburtsjahr = str_trim(A601_01))\n\nunique(df_lokal$geburtsjahr)\n\n [1] \"1981\"         \"1957\"         \"1962\"         \"1997\"         \"1967\"        \n [6] \"1984\"         \"1961\"         \"1989\"         \"1978\"         \"1964\"        \n[11] \"1971\"         \"1993\"         \"1973\"         \"1991\"         \"Deutschland\" \n[16] \"2005\"         \"1972\"         \"1988\"         \"1965\"         \"1966\"        \n[21] \"1982\"         \"1958\"         \"1985\"         \"1953\"         \"1974\"        \n[26] \"1979\"         \"1963\"         \"1970\"         \"1959\"         \"1954\"        \n[31] \"1990\"         \"1975\"         \"1976\"         \"1960\"         \"1956\"        \n[36] \"1999\"         \"1968\"         \"1969\"         \"1987\"         \"2004\"        \n[41] \"1983\"         \"1980\"         \"2002\"         \"1977\"         \"1952\"        \n[46] \"1950\"         \"1995\"         \"01.11.1967\"   \"1996\"         \"1944\"        \n[51] \"1994\"         \"1986\"         \"1955\"         \"1948\"         \"1951\"        \n[56] \"2001\"         \"1942\"         \"1949\"         \"2000\"         \"1947\"        \n[61] \"1941\"         \"1939\"         \"w000\"         \"2003\"         \"1998\"        \n[66] \"1992\"         \"1946\"         NA             \"1945\"         \"Keine Angabe\"\n[71] \"1943\"         \"1936\"         \"1938\"         \"2007\"         \"1935\"        \n[76] \"1940\"         \"1934\"         \"16.02.1957\"   \"Datenschutz\"  \"&lt;60\"         \n[81] \"1933\"         \"24.10.1945\"   \"I 966\"        \"1937\"        \n\n\nAls nächstes können wir die Fälle behandeln, die ihr vollständiges Geburtsdatum angegeben haben. Dazu nutzen wir die str_sub()-Funktion, die es uns erlaubt, Teile von Strings auf Basis ihrer Position zu extrahieren. Indem wir -4 angeben, sagen wir der Funktion, dass wir nur die letzten vier Stellen behalten wollen.\n\n# Entfernt alle Zeichen bis auf die letzten 4\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geburtsjahr = str_sub(geburtsjahr, -4))\n\nunique(df_lokal$geburtsjahr)\n\n [1] \"1981\" \"1957\" \"1962\" \"1997\" \"1967\" \"1984\" \"1961\" \"1989\" \"1978\" \"1964\"\n[11] \"1971\" \"1993\" \"1973\" \"1991\" \"land\" \"2005\" \"1972\" \"1988\" \"1965\" \"1966\"\n[21] \"1982\" \"1958\" \"1985\" \"1953\" \"1974\" \"1979\" \"1963\" \"1970\" \"1959\" \"1954\"\n[31] \"1990\" \"1975\" \"1976\" \"1960\" \"1956\" \"1999\" \"1968\" \"1969\" \"1987\" \"2004\"\n[41] \"1983\" \"1980\" \"2002\" \"1977\" \"1952\" \"1950\" \"1995\" \"1996\" \"1944\" \"1994\"\n[51] \"1986\" \"1955\" \"1948\" \"1951\" \"2001\" \"1942\" \"1949\" \"2000\" \"1947\" \"1941\"\n[61] \"1939\" \"w000\" \"2003\" \"1998\" \"1992\" \"1946\" NA     \"1945\" \"gabe\" \"1943\"\n[71] \"1936\" \"1938\" \"2007\" \"1935\" \"1940\" \"1934\" \"hutz\" \"&lt;60\"  \"1933\" \" 966\"\n[81] \"1937\"\n\n\nDiese Vorarbeit reicht, um fast alle problematischen Fälle mit einem weiteren Schritt zu bereinigen. Wir können nun die as.integer()-Funktion benutzen, um die Spalte in eine Zahl umzuwandeln. Alles, was nicht umgewandelt werden kann, wird automatisch zu einem NA.\n\n# Wandelt geburtsjahr in eine Zahl um\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geburtsjahr = as.integer(geburtsjahr))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `geburtsjahr = as.integer(geburtsjahr)`.\nCaused by warning:\n! NAs durch Umwandlung erzeugt\n\nunique(df_lokal$geburtsjahr)\n\n [1] 1981 1957 1962 1997 1967 1984 1961 1989 1978 1964 1971 1993 1973 1991   NA\n[16] 2005 1972 1988 1965 1966 1982 1958 1985 1953 1974 1979 1963 1970 1959 1954\n[31] 1990 1975 1976 1960 1956 1999 1968 1969 1987 2004 1983 1980 2002 1977 1952\n[46] 1950 1995 1996 1944 1994 1986 1955 1948 1951 2001 1942 1949 2000 1947 1941\n[61] 1939 2003 1998 1992 1946 1945 1943 1936 1938 2007 1935 1940 1934 1933  966\n[76] 1937\n\n\nDas sieht schon deutlich besser aus! Nur ein einziger Fall bleibt problematisch: Eine Person hat den Wert 966. Da es recht unwahrscheinlich ist, dass eine so alte Person an der Befragung teilgenommen hat, müssen wir entscheiden, wie wir mit dem Wert umgehen. Wenn wir uns die vorherigen Ausgaben der unique()-Funktion ansehen, sehen wir, dass die Person ursprünglich \"I 966\" angegeben hatte. Wir könnten nun eher streng sein und den Wert als NA deklarieren, da die Eingabe unsinnig ist. Oder wir gehen davon aus, dass 1966 gemeint war. In dem Fall könnten wir den einzelnen Wert einfach umcodieren oder 1000 addieren. Das ist ein schönes Beispiel dafür, dass es bei der Datenaufbereitung nicht immer eindeutig richtige oder falsche Entscheidungen gibt. In diesem konkreten Fall wandeln wir die 966 in eine 1966 um. Dazu nutzen wir die ifelse()-Funktion, die immer drei Argumente benötigt:\n\nEine Bedingung, die geprüft werden soll.\nWas getan werden soll, falls die Bedingung zutrifft.\nUnd was getan werden soll, falls die Bedingung nicht zutrifft.\n\nUm nur den Wert 966 zu ändern, können wir als Bedingung erfragen, ob der aktuelle Wert kleiner als 1000 ist. Falls dem so ist, können wir 1000 addieren und ansonsten den alten Wert übernehmen. All das natürlich in einem mutate()-Aufruf.\n\n# Addiert 1000 zu allen Werten von geburtsjahr unter 1000\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geburtsjahr = ifelse(geburtsjahr &lt; 1000, geburtsjahr+1000, geburtsjahr))\n\nunique(df_lokal$geburtsjahr)\n\n [1] 1981 1957 1962 1997 1967 1984 1961 1989 1978 1964 1971 1993 1973 1991   NA\n[16] 2005 1972 1988 1965 1966 1982 1958 1985 1953 1974 1979 1963 1970 1959 1954\n[31] 1990 1975 1976 1960 1956 1999 1968 1969 1987 2004 1983 1980 2002 1977 1952\n[46] 1950 1995 1996 1944 1994 1986 1955 1948 1951 2001 1942 1949 2000 1947 1941\n[61] 1939 2003 1998 1992 1946 1945 1943 1936 1938 2007 1935 1940 1934 1933 1937\n\n\nUm jetzt aus dem Geburtsjahr das Alter zu berechnen, können wir wieder mutate() nutzen, um eine neue Variable zu berechnen. Die Befragung wurde 2022 durchgeführt, also können wir davon den Wert aus geburtsjahr subtrahieren. Damit bekommen wir zwar streng genommen nicht das Alter zum Zeitpunkt der Befragung, sondern zum Jahresende, aber genauer liegen die Daten nicht vor, sodass wir damit leben müssen.\n\n# Berechnet das Alter der Befragten\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(alter = 2022 - geburtsjahr)\n\n\n\n3.2.3 Spalten auswählen\nManchmal enthalten Datensätze Spalten, die wir nicht benötigen. Um Speicherplatz zu sparen oder insgesamt effizienteren Code zu haben, können wir einzelne Spalten selektieren. Dazu nutzen wir die dplyr-Funktion select(). Sie können der Funktion einfach die Namen derjenigen Spalten übergeben, die Sie behalten wollen. Wenn wir beispielsweise nur Alter und Geschlecht behalten wollen, sieht das wie folgt aus:\n\n# Erstell einen Datensatz, der nur aus Alter und Geschlecht der Befragten besteht\n\ndf_lokal_alterUndGeschlecht &lt;- df_lokal |&gt;\n  select(alter, geschlechtMitNAs)\n\nAlternativ können Sie auch mit einem Minuszeichen einzelne Spalten ausschließen. Wenn wir z. B. die Zwischenschritte unserer Datentransformationen oben entfernen möchten, geht das deutlich leichter über den Ausschluss einiger weniger Spalten als den Einschluss aller anderen. Dabei können wir wieder mal die c()-Funktion nutzen, um gleich mehrere Spalten auf einmal loszuwerden:\n\n# Schließt die Zwischenschritte aus der Transformation aus dem Datensatz aus\n\ndf_lokal_transformiert &lt;- df_lokal |&gt;\n  select(-c(A602, geschlecht, A601_01, geburtsjahr))\n\n\n\n3.2.4 Zeilen nach Inhalt filtern\nIn anderen Fällen kann es sinnvoll sein, bestimmte Fälle - also Zeilen - aus dem Datensatz auszuschließen oder umgekehrt: nur bestimmte Fälle zu behalten. Dazu können wir die filter()-Funktion benutzen. Als Argument übergeben wir hier eine oder mehrere Bedingungen, die erfüllt sein müssen, damit ein Fall behalten wird. Beispielsweise können wir einen Datensatz erstellen, in dem nur Befragten enthalten sind, die jünger als 60 sind:\n\n# Entfernt Befragte 60+\n\ndf_lokal_u60 &lt;- df_lokal |&gt;\n  filter(alter &lt; 60)\n\nWir können auch die is.na()-Funktion nutzen (bzw. deren Negation mit !), um Befragte auszuschließen, die ihr Geschlecht nicht angegeben haben.\n\n# Entfernt Befragte ohne Angaben zum Geschlecht\n\ndf_lokal_geschlecht &lt;- df_lokal |&gt;\n  filter(!is.na(geschlechtMitNAs))\n\nManchmal kann es auch sinnvoll sein, zwei Bedingungen zu kombinieren. Das geht mit &:\n\n# Entfernt Befragte die unter 60 und nicht männlich sind\n\ndf_lokal_alteMaenner &lt;- df_lokal |&gt;\n  filter(geschlechtMitNAs == \"männlich\" & alter &gt;= 60)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pakete und Datentransformationen</span>"
    ]
  },
  {
    "objectID": "Skalenniveaus und deskriptive Datenanalyse.html",
    "href": "Skalenniveaus und deskriptive Datenanalyse.html",
    "title": "4  Skalenniveaus und deskriptive Datenanalyse",
    "section": "",
    "text": "4.1 Skalenniveaus und zentrale Lagemaße",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Skalenniveaus und deskriptive Datenanalyse</span>"
    ]
  },
  {
    "objectID": "Skalenniveaus und deskriptive Datenanalyse.html#deskiptive-datenanalyse",
    "href": "Skalenniveaus und deskriptive Datenanalyse.html#deskiptive-datenanalyse",
    "title": "4  Skalenniveaus und deskriptive Datenanalyse",
    "section": "4.2 Deskiptive Datenanalyse",
    "text": "4.2 Deskiptive Datenanalyse\nIn diesem Abschnitt werden wir uns anschauen, wie wir die verschiedenen Lagemaße aus dem Video in R berechnen können. Außerdem lernen Sie, wie Sie Daten in Abhängigkeit ihres Skalenniveaus visualisieren können.\n\n4.2.1 Modus\nR hat nach der Installation keine Funktion, die den Modus (oder die Modi) einer Verteilung ermittelt. Wir können aber das Paket DescTools installieren, die eine solche Funktion enthält.\n\n# Installiert das Paket \"DescTools\", falls es noch nicht installiert ist und lädt es anschließend. Andernfalls wird es nur geladen.\n\nif(!require(DescTools)){\n  install.packages(\"DescTools\")\n  library(DescTools)\n  }\n\nLade nötiges Paket: DescTools\n\n\nAnschließend laden wir wieder den Datensatz und wandeln die Geschlechtsabfrage um. Den Code kennen Sie schon aus dem letzten Kapitel. Da für dieses Kapitel ein neues Skript sinnvoll ist, führen wir auch den Code noch mal aus.\n\n# Lädt das tidyverse\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Einlesen der Daten \n\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Erstellt eine Spalte aus der Geschlechtsabfrage. Erst wird der Wert \"keine Angabe\" als fehlend deklariertm dann wird ein Faktor mit den übrigen drei Kategorien erstellt\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geschlechtMitNAs = na_if(A602, 4)) |&gt;\n  mutate(geschlechtMitNAs = factor(geschlechtMitNAs, labels = c(\"männlich\", \"weiblich\", \"divers\")))\n\nAnschließend können wir die Mode()-Funktion aus dem DescTools-Paket verwenden. Um damit den Modus zu berechnen, müssen wir die Funktion innerhalb der dplyr-Funktion summarise() aufrufen. Diese verdichtet Datensätze. Schauen wir uns einmal an, was passiert, wenn wir Mode() einfach entsprechended ausführen:\n\n#Versucht den Modus der Spalte geschlechtMitNAs zu berechnen\n\nmodus_geschlecht &lt;- df_lokal |&gt;\n  summarise(Modus = Mode(geschlechtMitNAs))\n\nmodus_geschlecht\n\n  Modus\n1    NA\n\n\nWie Sie sehen, gibt die Funktion ein NA zurück. Das ist Rs Art uns zu sagen, dass eine Berechnung nicht durchgeführt werden kann, weil die Daten fehlende Werte enthalten. Wir können das Problem beheben, indem wir der Mode()-Funktion das Argument na.rm = TRUE übergeben. na.rm steht für NA remove. Mit dem Wert TRUE sagen wir also, dass fehlende Werte vor der Berechnung entfernt werden sollen.\n\n#Berechnet den Modus der Spalte geschlechtMitNAs \n\nmodus_geschlecht &lt;- df_lokal |&gt;\n  summarise(Modus = Mode(geschlechtMitNAs, na.rm = TRUE))\n\nmodus_geschlecht\n\n     Modus\n1 weiblich\n\n\nNun sehen wir, dass mit 996 Fällen der Wert weiblich am häufigsten vorkommt.\nSchauen wir uns als nächstes an, wie Sie nominale Daten bzw. den Modus visualisieren können. Im letzten Kapitel wurde in Kürze das tidyverse vorgestellt und auf das Paket ggplot2 verwiesen, mit dem wir Plots erstellen können.\nIm Folgenden werden Schritt für Schritt ein einfaches Balkendiagramm erstellen und es nach und nach verbessern. Wir fangen damit an, dass wir das tidyverse laden. Wir erstellen dann ein neues Objekt für unseren Plot. Dazu geben wir unseren Datensatz mit der Pipe an die Funktion ggplot() weiter. Das ist die Hauptfunktion des Pakets. Üblicherweise wird innerhalb dieser Funktion die Funktion aes() (für aesthetics) aufgerugen, in der wir je nach Diagramm angeben, welche Spalten auf der x- und y-Achse dargestellt werden soll. Für ein Balkendiagramm benötigen wir nur die x-Achse, geben also x = geschlechtMitNAs an. Damit sagen wir erstmal nur, welche Spalte wir darstellen wollen, aber noch nichts darüber, was für eine Darstellung es werden soll. Um die Art des Diagramms festzulegen, gibt es in ggplot2 unzählige Funktionen, die alle mit geom_ beginnen (für geometry). Hinter dem Unterstrich folgt dann der (englische) Name des Diagrammtyps. In unserem Fall eines Balkendiagramms, heißt die entsprechende Funktion geom_bar(). Anders als sonst, verbinden wir die Funktionen in ggplot2 nicht über die Pipe, sondern ein Pluszeichen. In der Summe sieht der Code dann so aus:\n\n# Erstellt ein einfaches Balkendiagramm der Geschlechtsabfrage\n\nplotGeschlecht &lt;- df_lokal |&gt;\n  ggplot(aes(x = geschlechtMitNAs))+\n  geom_bar()\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nDas ist - nun ja - sagen wir mal, es ist nicht sonderlich hübsch. Folgende Dinge fallen auf:\n\nEs gibt einen Balken für NAs, dabei enthalten diese ja per definitionem keine Informationen.\nDie Angabe absoluter Häufigkeiten ist etwas problematisch; besser wären relative Häufigkeiten.\nDie Werte der einzelnen Kategorien können wir aktuell nur schätzen.\nDer Hintergrund und die Farbe der Balken sind nicht sonderlich ansehnlich.\nDie Beschriftungen der Achsen (geschlechtMitNAs und count) sind nicht gerade selbsterklärend.\n\nDiese Liste können wir nun einfach abarbeiten. Manchmal ist es sinnvoll, die Daten noch ein bisschen aufzubereiten, bevor wir sie an ggplot() übergeben. Dafür deklarieren wir ein neues Objekt, das die Daten enthalten soll. Als erstes können wir dann mit der filter()-Funktion, die Sie im letzten Kapitel kennengelernt haben, die NAs entfernen. Anschließend nutzen wir die group_by()-Funktion aus dplyr. Diese Funktion sorgt dafür, dass die nachfolgenden Funktionen nicht auf den gesamten Datensatz angewendet werden, sondern auf die jeweiligen Gruppen (männlich, weiblich, divers). Anschließend nutzen wir wieder die summarise()-Funktion. Dort können wir die Prozentwerte der einzelnen Gruppen mit der folgenden Formel berechnen: n()/nrow(df_lokal)*100. n() gibt die Anzahl der Fälle zurück. Durch das Aufrufen von group_by() sind es hier die Fälle pro Ausprägung von geschlechtMitNAs. Diesen Wert teilen wir durch nrow(df_lokal). Diese Funktion gibt die Anzahl der Zeilen im kompletten Datensatz zurück. Das Ergebnis multiplizieren wir mit 100, um einen Prozentwert zu erhalten. Als letztes nutzen wir mutate(), um die Prozentwerte auf zwei Nachkommastellen zu runden. Das Ergebnis sehen Sie unten:\n\n# Erstellt einen reduzierten Datensatz, der die relativen Häufigkeiten der Ausprägungen von geschlechtMitNAs enthält\n\ndf_plot_geschlecht &lt;- df_lokal |&gt;\n  filter(!is.na(geschlechtMitNAs)) |&gt;\n  group_by(geschlechtMitNAs) |&gt;\n  summarise(prozent = n()/nrow(df_lokal)*100) |&gt;\n  mutate(prozent = round(prozent, 2))\n\ndf_plot_geschlecht\n\n# A tibble: 3 × 2\n  geschlechtMitNAs prozent\n  &lt;fct&gt;              &lt;dbl&gt;\n1 männlich           44.0 \n2 weiblich           54.0 \n3 divers              0.38\n\n\nWenn wir diesen Datensatz an ggplot() übergeben und versuchen, ein Balkendiagramm zu erstellen, haben wir zwar keinen Balken für NA mehr, dafür ein neues Problem: Alle Balken sind gleich hoch.\n\n# Versucht, ein Balkendiagramm der Geschlechtsabfrage ohne NAs zu erstellen\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs))+\n  geom_bar()\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nUm dieses Problem zu beheben, können wir angeben, dass die y-Achse den Wert aus der Spalte prozent darstellen soll. Zusätzlich müssen wir beim Aufrufen von geom_bar() das Argument stat = \"identity\" angeben. Damit sagen wir der Funktion, dass sie nichts berechnen muss, sondern wir schon die finalen Werte als y-Wert angegeben haben.\n\n# Erstellt ein Balkendiagramm der relativen Häufigkeiten der Geschlechtsabfrage ohne NAs\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = prozent))+\n  geom_bar(stat = \"identity\")\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nAls nächstes wollen wir die Werte beschriften. Dazu fügen wir unserem Plot eine neue geom-Funktion hinzu, nämlich geom_text(). Dieser Funktion übergeben wir auch wieder aes(), um zu bestimmen, was angezeigt werden soll. Das Argument für Beschriftungen lautet label. Als Wert können wir die Spalte prozent angeben. Die Standardeinstellung ist, dass der Wert an seine Position auf der y-Achse geschrieben wird. Das ist etwas unpraktisch, da dort ja auch die Balken aufhören. Indem wir geom_text() das Argument vjust (für vertical justification) und den Wert -0,5 (in R reicht -.5) übergeben, können wir die Beschriftung leicht nach oben verschieben.\n\n# Erstellt ein beschriftetes Balkendiagramm der relativen Häufigkeiten der Geschlechtsabfrage ohne NAs\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = prozent))+\n  geom_bar(stat = \"identity\")+\n  geom_text((aes(label = prozent)), vjust = -.5)\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nAls nächstes passen wir die Farben etwas an. Gleich vorab: ggplot2 bietet nahezu unendlich viele Möglichkeiten, das Aussehen von Diagrammen anzupassen. Hier machen wir es uns relativ einfach und nutzen vorhandene Funktionen. So wie es viele geom_Funktionen gibt, gibt es auch einige theme_ Funktionen. Hier ergänzen wir theme_minimal() zu unserem Diagramm. Damit wird zwar der Hintergrund, nicht aber die Farbe der Balken angepasst. Das können wir tun, indem wir der geom_bar()-Funktion das Argument fill und eine Farbe übergeben. Diese müssen auf Englisch angegeben werden. Hier verwenden wir ein helles grau.\n\n# Erstellt ein beschriftetes Balkendiagramm der relativen Häufigkeiten der Geschlechtsabfrage ohne NAs\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = prozent))+\n  geom_bar(stat = \"identity\", fill = \"lightgrey\")+\n  geom_text((aes(label = prozent)), vjust = -.5)+\n  theme_minimal()\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nAls letztes ändern wir die Achsenbeschriftungen. Dazu fügen wir die labs()-Funktion hinzu. Mit den Argumenten x und y können wir die Beschriftung anpassen.\n\n# Erstellt ein beschriftetes Balkendiagramm der relativen Häufigkeiten der Geschlechtsabfrage ohne NAs\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = prozent))+\n  geom_bar(stat = \"identity\", fill = \"lightgrey\")+\n  geom_text((aes(label = prozent)), vjust = -.5)+\n  theme_minimal()+\n  labs(x = \"Geschlecht\", y = \"relative Häufigkeit in Prozent\")\n\nplotGeschlecht\n\n\n\n\n\n\n\n\n\n\n4.2.2 Median\nAnders als beim Modus gibt es für den Median eine R-Funktion, die wir ohne Weiteres nutzen können: die median()-Funktion. Auch hier müssen wir darauf achten, na.rm = TRUE anzugeben, damit der Median berechnet werden kann. Im folgenden Beispiel berechnen wr den Median der Spalte A203_06. In der Spalte ist codiert, wie häufig die Befragten Lokalzeitungen lesen (von 1 = “nie” bis 6 = “mehrmals täglich”).\n\n# Benenntdie Spalte A203_06 in lokalzeitung um\ndf_lokal &lt;- df_lokal |&gt;\n rename(lokalzeitung = A203_06) \n\n# Berechnet den Median der Lokalzeiungsnutzung\nmedian_lokalzeitung &lt;- df_lokal |&gt;\n  summarise(median = median(lokalzeitung, na.rm = TRUE))\n\nmedian_lokalzeitung\n\n  median\n1      4\n\n\nDer Median ist 4, was einer Nutzung mehrmals pro Woche entspricht. Es gibt viele verschiedene Möglichkeiten, den Median bzw. die Verteilung von ordinalen Daten zu visualisieren. Bei wenigen Ausprägungen, so wie im Fall der Nutzung von Lokalzeitungen, können wir ähnlich vorgehen wie beim Modus oben. Dazu müssen wir die Spalte in einen Faktor umwandeln, bevor wir sie an ggplot() übergeben. Anschließend gehen wir ähnlich vor wie oben, allerdings bleiben wir der einfachheithalber bei absoluten Häufigkeiten. Eine relevante Ergänzung nehmen wir aber vor: Mit der geom_vline()-Funktion (für vertical line) und dem Argument xintercept = 4, können wir eine Linie hinzufügen, die den Median anzeigt.\n\n# Wandelt die Nutzung von Lokalzeitungen in einen Faktor um und plottet die Daten als Balkendiagramm\nbalkenPlot_lokalzeitung &lt;- df_lokal |&gt;\n  mutate(lokalzeitungFaktor = factor(lokalzeitung, labels = c(\"nie\", \"weniger als ein Mal im Monat\",\n                                                              \"mehrmals im Monat\", \"mehrmals in der Woche\",\n                                                              \"täglich\", \"mehrmals täglich\"))) |&gt;\n  filter(!is.na(lokalzeitungFaktor)) |&gt;\n  ggplot(aes(x = lokalzeitungFaktor))+\n  geom_bar(fill = \"lightgrey\")+\n  theme_minimal()+\n  labs(x = \"Nutzung von Lokalzeitungen\", y = \"Häufigkeit\")+\n  geom_vline(xintercept = 4, linetype = \"dashed\")\n\nbalkenPlot_lokalzeitung\n\n\n\n\n\n\n\n\nDas ist schon sehr nah an einer akzeptablen Darstellung, aber die Wertbeschriftungen sehen furchtbar aus! Hier müssen wir etwas Hand anlegen und Zeilenumbrüche einfügen. Die Beschriftungen können wir mit der Funktion scale_x_discrete() und darin mit dem labels-Argument anpassen. Um einen Zeilenumbruch hinzuzufügen, können wir an einer beliebigen Stelle in einem string \\n ergänzen. Diese Funktion können wir mit einem + unserem bisherigen Objekt balkenPlot_lokalzeitung hinzufügen:\n\n# Verändert die Beschriftungen der Balken\n\nbalkenPlot_lokalzeitung &lt;- balkenPlot_lokalzeitung+\n  scale_x_discrete(labels=c(\"nie\" = \"nie\", \"weniger als ein Mal im Monat\" = \"weniger als\\n ein Mal im Monat\",\n                              \"mehrmals im Monat\" = \"mehrmals\\n im Monat\", \"mehrmals in der Woche\" = \n                              \"mehrmals\\n in der Woche\", \"täglich\" = \"täglich\",\n                              \"mehrmals täglich\" = \"merhmals\\n täglich\"))\n\nbalkenPlot_lokalzeitung\n\n\n\n\n\n\n\n\nEine andere häufige Visualisierung von ordinalen Daten ist der sogenannte Boxplot, der mit der Funktion geom_boxplot() erstellt wird. Beachten Sie im Beispiel unten, dass wir die Spalte lokalzeitung im Aufruf von ggplot() bzw. darin aes() als y-Wert definieren. Wir könnten auch den x-Wert wählen, dann würde der Boxplot auf der Seite liegen.\n\nboxplot_lokalzeitung &lt;- df_lokal |&gt;\n  filter(!is.na(lokalzeitung)) |&gt;\n  ggplot(aes(y = lokalzeitung))+\n  geom_boxplot()+\n  theme_minimal()+\n  labs(y = \"Häufigkeit der Lokalzeitungsnutzung\")\n\nboxplot_lokalzeitung\n\n\n\n\n\n\n\n\nWas Sie hier sehen ist erstmal nicht sonderlich hübsch, sollte aber dennoch kurz erklärt werden: Die dicke schwarze Linie beim Wert 4 zeigt den Median. Die eingekasteten Bereiche darüber und darunter zeigen das 75. bzw. das. 25. Quartil. Oder einfach gesagt: 25 % der Befragten haben den Wert 2 oder weniger angegeben und weitere 25 % den Wert 5 oder mehr. Die Linien nach unten und oben gehen bis zum Minimum bzw. Maximum.\nFolgende Probleme hat die Darstellung:\n\nDer Boxplot ist sehr breit. Das sieht furchtbar aus!\nDie y-Achse ist nur spärlich beschriftet.\nDie x-Achse hat eine Beschriftung, die überhaupt nicht nachvollziehbar ist.\nDas Koordinatensystem hat senkrechte Linien, die keinen Sinn ergeben, da wir ja eigentlich gar nichts auf der x-Achse abbilden.\n\nFangen wir mit den ersten beiden Problemen an. Wir sehen auf der Grafik oben, dass der auf der x-Achse der Bereich von x = -0,4 bis x = 0,4 (was auch immer diese Werte bedeuten mögen!) dargestellt ist und der Boxplot genau diesen Bereich einnimmt. Wir können die Funktion xlim() nutzen, um den dargestellten Bereich zu erweitern. Dazu müssen wir einfach nur zwei Werte angeben, z.B. -1 und 1.\nUm die y-Achse etwas schöner zu machen, können wir eine ähnliche Funktion nutzen, wie im Balkendiagramm oben: scale_y_continuous(). Mit dem Argument breaks können wir angeben, welche Werte beschriftet sein sollen. Hier können wir 1:6 angeben, um alle ganzen Zahlen zwischen 1 und 6 anzeigen zu lassen.\n\nboxplot_lokalzeitung &lt;- boxplot_lokalzeitung+  \n  xlim(-1,1)+\n  scale_y_continuous(breaks = 1:6)\n\n\nboxplot_lokalzeitung\n\n\n\n\n\n\n\n\nDas sieht schon etwas besser aus. Wir haben nun aber ein neues Problem: Auf der y-Achse werden nun waagerechte Linien zwischen den ganzen Zahlen angezeigt, dabei konnte die Variable diese Werte gar nicht annhemen.\nDieses Problem können wir gemeinsam mit den übrigen Punkte von oben in einem Rutsch erledigen, indem wir die theme()-Funktion nutzen. Diese Funktion kann zugegebenermaßen etwas abschreckend sein. Sie können damit die Darstellung aller einzelnen Elemente eines Plots anpassen oder - und das ist für uns hier aber auch generell häufig entscheidend - sie entfernen! Das Schema ist immer gleich: Sie geben ein Element an und schreiben hinter ein Gleichheitszeichen, wie es dargestellt werden soll. Geben Sie dort element_blank() an, wird das Element entfernt. Das nutzen wir hier um die folgenden Elemente zu entfernen:\n\nDie Beschriftung der x-Achse –&gt; das Element heißt axis.text.x\nDie vertikalen Linien im Koordinatensystem –&gt; die Elemente heißen panel.grid.major.x und panel.grid.minor.x\nDie horizontalen Linien zwischen den ganzen Zahlen –&gt; das Element heißt panel.grid.minor.y\n\n\nboxplot_lokalzeitung &lt;- boxplot_lokalzeitung+\n  theme(axis.text.x = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank())\n\nboxplot_lokalzeitung\n\n\n\n\n\n\n\n\nDeutlich häufiger werden Sie Boxplots begegnen (oder selbst erstellen), wenn es um die Darstellung mehrerer Gruppen geht. Im folgenden Beispiel sehen Sie einen Boxplot für die Nutzung von Lokalzeitungen nach Geschlecht der Befragten. Im wesentlichen ist er sehr ähnlich wie der Plot oben, allerdings müssen wir diesmal keine Elemente entfernen. Vor allem aber müssen wir im Aufruf von ggplot() bzw. darin aes() angeben, dass das Geschlecht der Befragten auf der x-Achse dargestellt werden soll.\n\nboxplot_lokalzeitung_geschlecht &lt;- df_lokal |&gt;\n  filter(!is.na(lokalzeitung)) |&gt;\n  filter(!is.na(geschlechtMitNAs)) |&gt;\n  ggplot(aes(y = lokalzeitung, x = geschlechtMitNAs))+\n  geom_boxplot()+\n  theme_minimal()+\n  scale_y_continuous(breaks = 1:6)+\n  labs(y = \"Häufigkeit der Lokalzeitungsnutzung\", x = \"Geschlecht\")\n\nboxplot_lokalzeitung_geschlecht\n\n\n\n\n\n\n\n\nDas Ergebnis sieht schon sehr ordentlich aus. Der einsame Punkt in der Spalte divers steht für einen Ausreißer: Eine Person liest deutlich höufiger Lokalzeitungen als andere Menschen, die sich nicht-binär identifizieren. In erster Linie liegt das an der sehr geringen Fallzahl in der Gruppe (vgl. das Balkendiagramm von oben). Um das sichtbar zu machen, können wir die Rohdaten anzeigen lassen. Das geht grundsätzlich mit geom_point(), hat aber den Nachteil, dass dann alle Datenpunkte an derselben Stelle dargestellt werden:\n\nboxplot_lokalzeitung_geschlecht+\n  geom_point()\n\n\n\n\n\n\n\n\nDas hilft uns nicht wirklich weiter. Eine bessere Möglichkeit ist geom_jitter(). Damit werden die Rohdaten etwas gestreut geplottet. Durch das Argument alpha = .25 können wir die Punkte zusätzlich etwas transparent machen.\n\nboxplot_lokalzeitung_geschlecht &lt;- boxplot_lokalzeitung_geschlecht+\n  geom_jitter(alpha = .25)\n\nboxplot_lokalzeitung_geschlecht\n\n\n\n\n\n\n\n\n\n\n4.2.3 Mittelwert und Standardabweichung\nFür Mitelwert und Standardabweichung gibt es ebenfalls zwei R-Funktionen, die wir direkt nutzen können: mean() und sd(). Auch hier müssen wir darauf achten, dass wir na.rm = TRUE angeben.\nIm folgenden Beispiel berechnen wir zunächst das Alter der Befragten mit dem Code aus dem letzten Kapitel. Anschließend nutzen wir summarse(), um den Datensatz auf Mittelwert und Standardabweichung des Alters zu reduzieren. Innerhalb von summarise() berechnen wir entsprechen mit mean() den Mittelwert und mit sd() die Standardabweichung. Beides runden wir mit round() auf zwei Nachkommastellen.\n\n# Berechnet das Alter der Befragten\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geburtsjahr = str_trim(A601_01)) |&gt;\n  mutate(geburtsjahr = str_sub(geburtsjahr, -4)) |&gt;\n  mutate(geburtsjahr = as.integer(geburtsjahr)) |&gt;\n  mutate(geburtsjahr = ifelse(geburtsjahr &lt; 1000, geburtsjahr+1000, geburtsjahr)) |&gt;\n  mutate(alter = 2022 - geburtsjahr)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `geburtsjahr = as.integer(geburtsjahr)`.\nCaused by warning:\n! NAs durch Umwandlung erzeugt\n\n# Berechnet Mittelwert und Standardabweichung des Alters\n\ndf_alter &lt;- df_lokal |&gt;\n  summarise(MWAlter = round(mean(alter, na.rm = TRUE), 2),\n            SDAlter = round(sd(alter, na.rm = TRUE), 2))\n\ndf_alter\n\n  MWAlter SDAlter\n1   49.66   15.31\n\n\nWir sehen, dass die Befragten im Mittel 49,66 also fast 50 Jahre als waren, bei einer Standardabweichung von 15,31 Jahren. Im Video oben haben Sie erfahren, dass bei einer Normalverteilung ca. 68 % aller Werte innerhalb der Region Mittelwert - 1 Standardabweichung bis Mittelwert + 1 Standardabweichung liegen. Hier wäre das entsprechend der Bereich von 34,35 bis 64.97 Jahren, zumindest sofern die Variable normalverteilt ist. Um das zu prüfen und genereall um metrische Daten zu visualisieren, können wir ein sogenanntes Histogramm zeichnen. Dazu nutzen wir geom_histogramm(). Im Prinzip handelt es sich dabei um eine Art Balkendiagramm für metrische Daten, bei der einzelne Ausprägungen zusammengefasst werden,\n\nhistogramm_alter &lt;- df_lokal |&gt;\n  filter(!is.na(alter)) |&gt;\n  ggplot(aes(x = alter))+\n  geom_histogram(fill = \"lightgrey\", bins = 40)+\n  theme_minimal()+\n  labs(x = \"Alter der Befragten\", y = \"Häufigkeit\")\n  \n\nhistogramm_alter\n\n\n\n\n\n\n\n\nDem Histogramm können wir entnehmen, dass die Daten annähnernd normalverteilt sind. Das linke Ende der Verteilung ist etwas steiler. Das ist zu erwarten, denn üblicherweise gibt es ein Mindestalter zur Teilnahme an Befragungen. Wir sehen auch, dass Menschen um die 60 Jahre und älter relativ stark vertreten sind. Gemessen an der Bevölkerung ist auch das nicht sonderlich. Für die meisten statistischen Zweck könnten Sie bei so einer Verteilung aber davon ausgehen, dass Sie zumindest nah genug an einer Normalverteilung dran sind.\nWährend uns das Histogramm einen guten Überblick über die Verteilung als Ganze gibt, ist es manchmal sinnvoll, Mittelwert und Standardabweichung direkt darzustellen. Das gilt insbesondere dann, wenn Sie mehrere Gruppen vergleichen wollen. Im folgenden Beispiel stelle wir das Durchschnittsalter (also den Mittelwert) der Befragten nach Geschlecht dar. Dazu wollen wir die Streuung um den Mittelwert der jeweiligen Gruppen darstellen. Hier machen wir das, indem wir die Standardabweichung ebenfalls darstellen. In wissenschaftlichen Arbeiten werden Sie auch immer mal Darstellungen begegnen, in denen andere Maße genutzt werden: entweder der Standardfehler oder sogenannte Konfidenzintervalle. Die Begriffe werden wir noch kennenlernen, ignorieren sie aber in diesem Kapitel noch.\nFür unser Beispiel berechnen wir als erstes Mittelwert und Standardabweichung der jeweiligen Gruppen. Alles, was wir dafür brauchen, haben wir schon kennengelernt: Wir nutzen filter() um fehlende Werte auszuschließen, nutzen group_by(), um die Daten zu Gruppieren und rufen dann mean() und sd() innerhalb von summarise() auf.\n\n# Berechnet Mittelwert und Standardabweichung nach Geschecht\ndf_lokal_alterUndGeschlecht &lt;- df_lokal |&gt;\n  filter(!is.na(geschlechtMitNAs)) |&gt;\n  filter(!is.na(alter)) |&gt;\n  group_by(geschlechtMitNAs) |&gt;\n  summarise(MWAlter = mean(alter),\n            SDAlter = sd(alter))\n\nUm die Gruppenmittelwerte mit jeweiliger Standardabweichug abzubilden, nutzen wir zwei neue geom_ Funktionen: geom_point() für die Mittelwerte und geom_errorbar() für die Standardabweichungen. Aber fangen wir obne an: zunächst legen wir in ggplot() und aes() fest, dass wir das Alter auf der y-Achse und das Geschlecht auf der x-Achse darstellen wollen. geom_point() sorgt dafür, dass die Gruppenmittelwerte jeweils durch einen Punkt dargestellt werden. geom_errorbar() zeichnet Balken um diese Punkte. Dazu müssen wir wieder in aes() angeben, wo diese Balken anfangen und aufhören sollen. Den Startpunkt legen wir mit ymin=MWAlter-SDAlter und den Endpunkt mit ymax=MWAlter+SDAlter fest.\n\n# Plottet die Gruppenmittelwerte und Standardabweichungen\nalter_nach_geschlecht &lt;- df_lokal_alterUndGeschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = MWAlter))+\n  geom_point()+\n  geom_errorbar(aes(ymin=MWAlter-SDAlter, ymax=MWAlter+SDAlter))+\n  labs(x = \"Geschlecht\", y = \"Alter\")+\n  theme_minimal()\n\nalter_nach_geschlecht\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrafiken speichern\n\n\n\nWenn Sie einen Plot speichern wollen, können Sie die ggsave()-Funktion verwenden. Folgende Argumente sollten Sie der Funktion übergeben:\n\nfilename = Den Namen, den die Datei tragen soll (in Anführungszeichen).\nplot = Den Namen des Objekts, das Sie speichern wollen.\nwidth = Die gewünschte Breite der Grafik.\nheight = Die gewünschte Höhe der Grafik.\nunits = Die Maßeinheit in der Breite und Höhe angegeben werden: “mm”, “cm”, “px” oder “in” für millimeter centimeter, pixel oder inch.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Skalenniveaus und deskriptive Datenanalyse</span>"
    ]
  },
  {
    "objectID": "Hypothesen und Testtheorie.html",
    "href": "Hypothesen und Testtheorie.html",
    "title": "5  Hypothesen und Testtheorie",
    "section": "",
    "text": "5.1 Hypothesen\nGanz allgemein gesprochen, verstehen wir unter einer Hypothese eine allgemeine Aussage über einen vermuteten Zusammenhang zwischen empirischen oder logischen Sachverhalten. In unserem Kontext gelten drei Anforderungen an Hypothesen:\nGrundsätzlich können Sie immer eine Hypothese aufstellen, wenn Sie eine ganz konkrete Annahme haben. Allerdings entspricht es guter wissenschaftlicher Praxis, dass Sie Ihre Hypothesen aus dem aktuellen Forschungsstand Ihres Themas, d.h. aktuellen Theorien und verwandten empirischen Befunden, ableiten. Beispielsweise entspricht die Hypothese “Am Freitag schmeckt das Essen in der Mensa der Uni Erfurt schlechter als an den anderen Wochentagen” den oben genannten Anforderungen. Allerdings handelt es sich dabei um nicht viel mehr als einen auf meiner Wahrnehmung beruhenden flüchtigen Gedanken.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesen und Testtheorie</span>"
    ]
  },
  {
    "objectID": "Hypothesen und Testtheorie.html#hypothesen",
    "href": "Hypothesen und Testtheorie.html#hypothesen",
    "title": "5  Hypothesen und Testtheorie",
    "section": "",
    "text": "Sie dürfen keine Einzelfälle beschreiben.\nSie müssen die Struktur eines sinnvollen Konditionalsatzes aufweisen (z. B. wenn-dann, je-desto). Diese ist logisch notwendig, muss aber nicht zwingend expliziert formuliert sein.\nSie müssen falsifizierbar sein.\n\n\n\n\n\n\n\nFalsifizierbarkeit\n\n\n\nFalsifizierbarkeit bedeutet im Kern nichts anderes, als dass wir in der Lage sein müssen, die Hypothese zu widerlegen. In der Forschungspraxis bedeutet das konkret, dass wir in der Lage sein müssen, Daten zu erheben, anhand derer wir die Hypothese testen können.\n\n\n\n\n5.1.1 Arten von Hypothesen\nIn den nachfolgenden Abschnitten werden wir uns anschauen, wie verschiedene Hypothesen differenziert werden können.\n\n5.1.1.1 Null- und Alternativhypothese\nEinen ersten Unterschied, den Sie kennen sollten, ist der zwischen Null- und Alternativhypothese. Manchmal werden die beiden auch als H0 und H1 bezeichnet. Letztere ist immer das, was Sie inhaltlich eigentlich annehmen. Die Nullhypothese besagt dagegen, dass Ihre Annahme nicht zutrifft. Die Nullhypothese ist deshalb wichtig, weil die statistischen Verfahren, die Sie in dieser Veranstaltung lernen werden, nicht die Alternativ-, sondern die Nullhypothese testen. Streng genommen gelten die oben genannten Kriterien also in erster Linie für die Nullhypothese. Das gilt insbesondere für die Falsifizierbarkeit!\nStellen wir uns einmal vor, dass Sie annehmen, dass jüngere Menschen mehr Zeit mit der Nutzung sozialer Medien verbringen als ältere Menschen. Eine mögliche Formulierung der Alternativhypothese lautet:\n\nH1: Es besteht ein Zusammenhang zwischen dem Alter und der Nutzung sozialer Netzwerke.\n\nDagegen lautet die Nullhypothese:\n\nH0: Es besteht kein Zusammenhang zwischen dem Alter und der Nutzung sozialer Netzwerke.\n\n\n\n5.1.1.2 Gerichtete und ungerichtete Hypothesen\nEin weiterer wichtiger Aspekt, nach dem Sie Hypothesen unterscheiden können, ist, ob diese gerichtet oder ungerichtet sind. Gerichtet sind Hypothesen immer dann, wenn Sie eine Vermutung über die Richtung eines Zusammenhangs haben. Ungerichtet bedeutet dagegen, dass Sie einfach nur annehmen, dass es einen Zusammenhang gibt, ohne zu vermuten, wie genau dieser ausfällt. Nehmen wir noch einmal das Beispiel von oben. Im Text haben wir gesagt, dass Sie vermuten, dass jüngere Menschen mehr Zeit mit der Nutzung verbringen als ältere. Das ist ein Beispiel für eine gerichtete Hypothese, da wir eine Vermutung darüber haben, welche Personen mehr bzw. weniger Zeit mit der Nutzung verbringen. Die Formulierung der H1 im Beispiel oben ist dagegen ungerichtet, da nur gesagt wird, dass ein Zusammenhang besteht. Formulieren wir die Hypothese also um:\n\nH1: Jüngere Menschen verbringen mehr Zeit mit der Nutzung sozialer Netzwerke als ältere Menschen.\n\nEntsprechend ändert sich auch die H0:\n\nH0: Jüngere Menschen verbringen nicht mehr Zeit mit der Nutzung sozialer Netzwerke als ältere Menschen.\n\nBeachten Sie, dass diese Nullhypothese nicht nur dann falsifiziert wird, wenn es keinen Zusammenhang zwischen dem Alter und der Nutzung sozialer Netzwerke gibt, sondern auch, falls es zwar einen Zusammenhang gibt, dieser aber in die entgegengesetzte Richtung läuft. Also wenn Menschen mehr Zeit mit der Nutzung verbringen, je älter sie sind.\n\n\n5.1.1.3 Hypothesen über Zusammenhänge und Unterschiede\nDie meisten Hypothesen, die Sie während Ihres Studiums aufstellen und testen werden, befassen sich entweder mit Zusammenhängen, so wie im bisherigen Beispiel, oder mit Unterschieden zwischen (mindestens) zwei Gruppen. Solche Hypothesen sind immer dann sinnvoll, wenn Sie entweder an Differenzen zwischen natürlich auftretenden Gruppen interessiert sind (z. B. Studierende vs. Azubis, Arbeitnehmerinnen und Arbeitnehmer vs. Selbstständige, Menschen aus Europa vs. den USA) oder wenn Sie im Rahmen eines Experiments die Unterschiede zwischen einer Experimental- und einer Kontrollgruppe untersuchen. Oder anders gesagt: Hypothesen über Unterschiede sind immer dann sinnvoll, wenn Ihre Hypothese eine Aussage über Gruppen enthält, die Sie mit einer nominalen Variable messen können.\nSchauen wir uns mal ein Beispiel für eine Hypothese über Unterschiede an. Es gelten dieselben Kriterien wie oben:\n\nH1: Menschen im Ruhestand und Menschen, die nicht im Ruhestand sind, schauen unterschiedlich oft lineares Fernsehen.\n\nDiese Hypothese beschreibt einen vermuteten Unterschied zwischen Rentnerinnen und Rentnern und allen anderen Menschen. Gleichzeitig ist es eine ungerichtete Hypothese, da sie keine Annahme darüber enthält, welche der beiden Gruppen häufiger (bzw. seltener) lineares Fernsehen schaut. Die zugehörige H0 lautet dementsprechend:\n\nH0: Menschen im Ruhestand und Menschen, die nicht im Ruhestand sind, schauen gleich oft lineares Fernsehen wie Menschen.\n\nWürden wir stattdessen davon ausgehen, dass Rentnerinnen und Rentner häufiger lineares Fernsehen schauen (z. B., weil sie einfach mehr Zeit haben), könnten wir die folgende gerichtete Alternativhypothese mit zugehöriger Nullhypothese aufstellen.\n\nH1: Menschen im Ruhestand schauen häufiger lineares Fernsehen als Menschen, die nicht im Ruhestand sind.\n\n\nH0: Menschen im Ruhestand schauen nicht häufiger lineares Fernsehen als Menschen, die nicht im Ruhestand sind.\n\n\n\n\n5.1.2 Statistische Hypothesen\nBisher haben wir uns in erster Linie mit inhaltlichen Aspekten von Hypothesen beschäftigt. Als Nächstes werfen wir einen Blick darauf, welche statistischen Annahmen hinter den Hypothesen stecken.\nIm Falle von Hypothesen über Zusammenhänge wird in der Regel ein Maß berechnet, das etwas über die Stärke eines Zusammenhangs aussagt, z. B. einen sogenannten Korrelationskoeffizienten (dazu im übernächsten Kapitel mehr). Bei ungerichteten Hypothesen lautet die Nullhypothese immer, dass dieses Maß genau den Wert 0 annimmt. Schauen wir uns das am Beispiel von oben an. Inhaltlich haben wir dort die folgende Alternativ- bzw. Nullhypothese aufgestellt:\n\nH1: Es besteht ein Zusammenhang zwischen dem Alter und der Nutzung sozialer Netzwerke.\n\n\nH0: Es besteht kein Zusammenhang zwischen dem Alter und der Nutzung sozialer Netzwerke.\n\nDie zugehörigen statistischen Hypothesen sehen Sie untenstehend. r ist dabei ein Platzhalter für das Zusammenhangsmaß:\n\nstatistische H1: \\(r_{Alter,~Nutzung~sozialer~Medien} \\neq 0\\)\n\n\nstatistische H0: \\(r_{Alter,~Nutzung~sozialer~Medien} = 0\\)\n\nWie oben angedeutet, verhält es sich bei gerichteten Hypothesen etwas anders, da die Nullhypothese lediglich aussagt, dass die Richtung nicht zutrifft. Dort haben wir die folgenden Hypothesen aufgestellt:\n\nH1: Jüngere Menschen verbringen mehr Zeit mit der Nutzung sozialer Netzwerke als ältere Menschen.\n\n\nH0: Jüngere Menschen verbringen nicht mehr Zeit mit der Nutzung sozialer Netzwerke als ältere Menschen.\n\nIn diesem Fall gehen wir von einem negativen Zusammenhang aus. Das bedeutet nichts anderes, als dass kleinere Messwerte in der einen Variable (das Alter) mit höheren Werten in der anderen Variable (die Nutzung sozialer Medien) einhergehen. Würden wir das Gegenteil vermuten, also dass eher ältere Menschen mehr Zeit mit der Nutzung sozialer Medien verbringen, würden wir einen positiven Zusammenhang annehmen (hohe Werte gehen mit hohen Werten einher). Daraus folgt, dass die Nullhypothese nicht aussagt, dass es keinen Zusammenhang gibt, sondern, dass es entweder keinen oder einen positiven Zusammenhang gibt. Anders gesagt: keinen negativen Zusammenhang. Die statistischen Hypothesen lauten also wie folgt:\n\nstatistische H1: \\(r_{Alter,~Nutzung~sozialer~Medien} &lt; 0\\)\n\n\nstatistische H0: \\(r_{Alter,~Nutzung~sozialer~Medien} \\ge 0\\)\n\nIm Falle von Hypothesen über Unterschiede verhält es sich im Prinzip ähnlich. Allerdings müssen wir hier definieren, was genau wir eigentlich mit einem Gruppenunterschied meinen. In der Regel ist das der Mittelwert. Wir gehen also davon aus, dass der Mittelwert der einen Gruppe größer oder kleiner ist als der Mittelwert der anderen Gruppe. Für unsere ungerichtete Hypothese von oben sieht das entsprechend so aus, wobei M eine Abkürzung für Mittelwert ist:\n\nstatistische H1: \\(M_{Fernsehnutzung,~Ruhestand} \\neq M_{Fernsehnutzung,~kein~Ruhestand}\\)\n\n\nstatistische H0: \\(M_{Fernsehnutzung,~Ruhestand} = M_{Fernsehnutzung,~kein~Ruhestand}\\)\n\nDie gerichtete Version dieser Hypothese lautet dagegen wie folgt:\n\nstatistische H1: \\(M_{Fernsehnutzung,~Ruhestand} &gt; M_{Fernsehnutzung,~kein~Ruhestand}\\)\n\n\nstatistische H0: \\(M_{Fernsehnutzung,~Ruhestand} \\le M_{Fernsehnutzung,~kein~Ruhestand}\\)\n\nDie Grundlagen der statistischen Hypothesen sind wichtig, weil sie das abbilden, was Sie mit statistischen Verfahren eigentlich testen! Allerdings ist eher unüblich, statistische Hypothesen in Abschlussarbeiten oder wissenschaftlichen Veröffentlichungen zu schreiben. Selbst die Nullhypothese werden Sie dort in der Regel nicht antreffen. Stattdessen reicht es in der Regel, die Alternativhypothese aufzuschreiben. Sofern diese anständig formuliert ist, impliziert sie sowohl die statistische als auch die Nullhypothese.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesen und Testtheorie</span>"
    ]
  },
  {
    "objectID": "Hypothesen und Testtheorie.html#testtheorie",
    "href": "Hypothesen und Testtheorie.html#testtheorie",
    "title": "5  Hypothesen und Testtheorie",
    "section": "5.2 Testtheorie",
    "text": "5.2 Testtheorie\n\n5.2.1 p-Werte\nIm Lauf der kommenden Wochen werden wir verschiedene Verfahren kennenlernen, mit denen Sie Hypothesen testen können. Die Idee dahinter ist aber immer dieselbe: Unsere Daten bilden (in der Regel) nur eine Stichprobe der Grundgesamtheit ab. Das Ziel eines Hypothesentests liegt darin, zu prüfen, ob die von uns beobachteten Zusammenhänge bzw. Unterschiede auch in der Grundgesamtheit zu erwarten sind. Die Verfahren werden unter dem Begriff Inferenzstatistik zusammengefasst und enthalten jeweils einen sogenannten Signifikanztest. Damit prüfen wir, ob wir H0 verwerfen müssen, H1 wird also immer nur indirekt getestet. Die Entscheidung darüber, ob H0 verworfen wird, basiert auf sogenannten p-Werten, die von den Signifikanztests berechnet werden.\nWas nun folgt, ist eine etwas sperrige Definition (Spoiler: Davon gibt es in der Statistik leider sehr viele!):\n\n\n\n\n\n\np-Wert\n\n\n\nDer p-Wert gibt an, wie wahrscheinlich es ist, die beobachteten Daten oder noch extremere Daten, zu beobachten, falls die Nullhypothese zutrifft.\n\n\nSchauen wir uns diese Definition mal im Detail an: Der p-Wert gibt eine Wahrscheinlichkeit an (das p steht für “probability”) und liegt daher immer zwischen 0 (sehr, sehr unwahrscheinlich) und 1 (extrem wahrscheinlich). Der Definition können wir entnehmen, dass diese Wahrscheinlichkeit aussagt, wie wahrscheinlich die von uns beobachteten Daten sind, falls die Nullhypothese zutrifft. Wenn diese Wahrscheinlichkeit sehr gering ist, üblicherweise kleiner als 5% (der berechnete p-Wert also kleiner als 0,05 ist), sagen wir, dass ein Ergebnis statistisch signifikant ist. Wenn Sie also einmal einen statistischen Test berechnen und einen entsprechend kleinen p-Wert sehen, können Sie sich relativ sicher sein, dass die Nullhypothese verworfen werden kann, also nicht zutrifft. Sie wurde dann falsifiziert. Das bedeutet zwar nicht automatisch, dass unsere Alternativhypothese zutrifft, aber mangels besserer Informationen können wir vorerst so tun, als wäre dies der Fall.\n\n\n5.2.2 Fehlschlüsse\nWarum aber diese 5 %? Diese Zahl ergibt sich aus der Logik, die dieser Art von Statistik zugrunde liegt. Die Idee ist, dass wir als Forscherinnen und Forscher versuchen, langfristig nur in 5% der Fälle fälschlicherweise davon ausgehen wollen, dass es einen Effekt (also einen Unterschied oder einen Zusammenhang) gibt, obwohl dem nicht der Fall ist. Diese Art von Fehlschluss nennen wir auch Alpha-Fehler oder Fehler 1. Art. Die 5 % sind dabei reine Konvention!\nDiese Fehler kommen zustande, weil p-Werte gleichverteilt sind, wenn die H0 zutrifft. Wenn es also keinen Unterschied oder Zusammenhang gibt, werden wir trotzdem in 5 % der Fälle ein statistisch signifikantes Ergebnis bekommen. Stellen wir uns einmal vor, wir würden eine der Hypothesen oben 10.000 Mal testen und die berechneten p-Werte aufschreiben und anschließend grafisch darstellen. Das Ergebnis könnte so aussehen:\n\n\n\n\n\n\n\n\n\nWie Sie sehen, sind die p-Werte gleichverteilt. Das heißt, Werte zwischen 0 und 0,05 sind genauso häufig wie Werte zwischen 0,95 und 1 oder 0,73 und 0,78. Da wir gesagt haben, dass Werte zwischen 0 und 0,05 als statistisch signifikant gelten, würden wir also in ca. 5 % der Fälle einen Fehlschluss ziehen.\n\n\n\n\n\n\n\n\n\nGenauso, wie wir fälschlicherweise zu dem Schluss gelangen können, dass es einen Effekt gibt, obwohl dies nicht der Fall ist, können wir einen realen Effekt nicht finden und entsprechend darauf schließen, dass es ihn nicht gibt (bzw. dass H0 zutrifft). Wir sprechen dann von einem Beta-Fehler oder auch Fehler 2. Art. Diese Art von Fehler kann zwar grundsätzlich verschiedene Ursachen haben, allerdings hängt die Wahrscheinlichkeit, einen solchen Fehler zu begehen, in erster Linie mit der Stichprobengröße zusammen. Es gilt: Je größer die Stichprobe, desto höher ist die Wahrscheinlichkeit, einen Effekt zu finden, sofern dieser tatsächlich existiert. Man spricht auch von der statistischen Power eines Tests.\nAuch hier gibt es wieder eine Konvention: Studien (bzw. die darin enthaltenen Tests) sollten mindestens 80% Power haben, also einen in der Realität existierenden Effekt in 4 von 5 Fällen identifizieren können. Online werden Sie für die Verfahren, die wir in den kommenden Wochen kennenlernen werden, oftmals Faustregeln zur Stichprobengröße finden, um diese Power zu erreichen. z. B.ca. 30 Personen pro Gruppe, wenn ein t-Test gerechnet werden soll (was das ist, werden wir noch lernen!). Solche Faustregeln sind meistens relativ alt, also aus einer Zeit, in der es sehr schwer und kostspielig war, Forschung mit Menschen zu betreiben. Sie sollten daher besser vermieden werden. Zwar gibt es Verfahren, mit denen bestimmt werden kann, wie groß Ihre Stichprobe sein muss, um auf 80% Power zu kommen, aber diese werden wir uns in dieser Veranstaltung nicht anschauen. Wenn Sie im Rahmen eines Forschungsprojekts im Studium eine Stichprobe rekrutieren sollen, sprechen Sie daher am besten mit Ihrem Betreuer oder Ihrer Betreuerin, um zu klären, wie groß die Stichprobe sein sollte und was für Sie in Ihrem Forschungskontext realistisch ist!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesen und Testtheorie</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen nominalen und ordinalen Variablen.html",
    "href": "Zusammenhänge zwischen nominalen und ordinalen Variablen.html",
    "title": "6  Zusammenhänge zwischen nominalen und ordinalen Variablen: Kreuztabellen",
    "section": "",
    "text": "6.1 Statistische Grundlagen",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Kreuztabellen</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen nominalen und ordinalen Variablen.html#deskription-und-visulisierung-der-daten",
    "href": "Zusammenhänge zwischen nominalen und ordinalen Variablen.html#deskription-und-visulisierung-der-daten",
    "title": "6  Zusammenhänge zwischen nominalen und ordinalen Variablen: Kreuztabellen",
    "section": "6.2 Deskription und Visulisierung der Daten",
    "text": "6.2 Deskription und Visulisierung der Daten\nBevor wir starten, müssen wir ein paar Vorkrehrungen treffen. Konkret laden wir das tidyverse laden danach das Paket effectsize, das wir später bei der statistischen Analyse benötigen werden. Anschließend lesen wir die Daten ein. Außerdem nutzen wir die options()-Funktion. Damit können wir die Einstellungen von R ändern. Hier nutzen wir das Argument scipen mit dem Wert 999. Im Prinzip sorgen wir damit nur dafür, dass sehr kleine Zahlen so angezeigt werden, wie Sie es erwarten würden. Das werden wir später bei den satistischen Analysen benötigen.\n\n# Lädt das tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Lädt bzw. installiert und lädt das Paket \"effectsize\"\nif(!require(effectsize)){\n  install.packages(\"effectsize\")\n  library(effectsize)\n}\n\nLade nötiges Paket: effectsize\n\n# Liest die Daten ein\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Stellt ein, dass sehr kleine Zahlen normal dargestellt werden\noptions(scipen = 999)\n\nSchauen wir uns nun einmal eine Kreuztabelle an. Anschließend visualisieren wir die Daten. Wir nehmen dazu die Spalte Bula, die angibt, aus welchem Bundesland die Befragten stammen und die Spalte A502_01, die angibt, ob die Menschen Mitglied in einem Sportverein sind. Um eine einfache Kreuztabelle zu erstellen, können wir die table()-Funktion nutzen, der wir die beiden Spalten übergeben. Vorher wandeln wir A502_01 in einen Faktor um, sodass wir etwas besser damit arbeiten können.\n\n# Wandelt die Spalte A502_01 in einen Faktor mit den Stufen \"nein\" und \"ja\" um.\ndf_lokal &lt;- df_lokal |&gt;\n    mutate(Sportverein = factor(A502_01, labels = c(\"nein\", \"ja\")))\n\n# Erstellt eine einfache 2x2 Tabelle, die wir \"tabelleSportNachBundesland\" nennen\n# select() wählt die Spalten aus, die dann an table() weitergereicht werden\ntabelleSportNachBundesland &lt;- df_lokal |&gt;\n  select(Bula, Sportverein) |&gt;\n  table() \n\ntabelleSportNachBundesland\n\n     Sportverein\nBula  nein  ja\n  RLP  649 385\n  TH   636 172\n\n\nFür sich genommen sagt uns diese Tabelle noch nicht allzu viel. Wir sehen, dass Befragte aus beiden Bundesländern ähnlich oft angegeben haben, nicht Mitglied eines Sportvereins zu sein, allerdings gab in Rheinland-Pfalz ein paar mehr Menschen, die in Vereinen aktiv sind.\nIm nächsten Schritt visualisieren wir diese Daten. Wir gehen dabei ähnlich vor wie bei den Balkendiagrammen mit relativen Häufigkeiten in Kapitel 4. Das heißt, wir erstellen einen reduzierten Datensatz. Dazu nutzen wir erst filter() und rufen darin !is.na() auf. Dabei müssen wir darauf achten, dass wir die Funktion für beide Variablen aufrufen und mit einem & verbingen. Anschließend nutzen wir group_by() und dann summarise(). Das Ergebnis ist ein Datensatz mit vier Zeilen, von denen jede eine Zelle aus der Tabelle oben darstellt.\n\n# Erstellt einen reduzierten Datensatz, der die relativen Häufigkeiten der Ausprägungen enthält\ndf_plot_kreuztabelle &lt;- df_lokal |&gt;\n  filter(!is.na(Sportverein) & !is.na(Bula)) |&gt;\n  group_by(Bula, Sportverein) |&gt;\n  summarise(Anzahl = n())\n\n`summarise()` has grouped output by 'Bula'. You can override using the\n`.groups` argument.\n\ndf_plot_kreuztabelle\n\n# A tibble: 4 × 3\n# Groups:   Bula [2]\n  Bula  Sportverein Anzahl\n  &lt;chr&gt; &lt;fct&gt;        &lt;int&gt;\n1 RLP   nein           649\n2 RLP   ja             385\n3 TH    nein           636\n4 TH    ja             172\n\n\nDiesen Datensatz können wir jetzt für die Visualisierung nutzen. Auch hier gehen wir im Prinzip wie in Kapitel 4 vor. Allerdings mit einem wichtigen Unterschied: Um die vier Zellen aus der Tabelle abbilden zu können, reichen einfache, nebeneinander stehende Balken nicht mehr aus. Zwar könnten wir theoretisch vier Balken zeichnen, allerdings wäre das nicht sonderlich übersichtlich. Stattdessen bietet sich ein sogenanntes Stapeldiagramm an.\nDie gute Nachricht ist, dass ein Stapediagramm sehr ähnlich erstellt wird, wie die Balkendiagramme, die Sie bereits kennen. Beim Aufruf von ggplot() und darin deraes()-Funktion übergeben wir eine Spalte (hier Bundesland) für die x-Achse und eine andere (Anzahl) für die y-Achse. Zusätzlich nutzen wir das fill-Argument und geben dort die Spalte Sportverein an. Damit sagen wir ggplot, dass diese Spalte genutzt werden soll, um die Balken einzufärben. Das heißt, die Ausprägungen “ja” und “nein” erhalten andere Farben. Diese können wir in scale_fill_manual festlegen. In diesem Beispiel geben wir außerdem in geom_bar position = \"fill\" an. Dadurch werden die Werte in relative Häufigkeiten umgewandelt und beide Balken sind gleich hoch.\n\nplot_kreuztabelle &lt;- df_plot_kreuztabelle |&gt;\n  ggplot(aes(x = Bula, y = Anzahl, fill = Sportverein))+\n    geom_bar(position = \"fill\", stat = \"identity\")+\n    theme_minimal()+\n    scale_fill_manual(values = c(\"#F5C000\", \"#0035F5\"))+\n    labs(x = \"Bundesland\", y = \"relative Häufigkeit\", fill = \"Mitglied\\nim Sportverein\")\n\nplot_kreuztabelle\n\n\n\n\n\n\n\n\nDie Visualisierung der Daten macht deutlich, dass in Thüringen prozentual weniger Befragte Mitglied in einem Sportverein sind als in Rheinland-Pfalz. Das legt nahe, dass die beiden Variablen zusammenhängen. Als nächstes prüfen wir mit einem Signifikanztest, ob dies tatsächlich der Fall ist.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Kreuztabellen</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen nominalen und ordinalen Variablen.html#signifikanztests-für-kreuztabellen-in-r-berechnen",
    "href": "Zusammenhänge zwischen nominalen und ordinalen Variablen.html#signifikanztests-für-kreuztabellen-in-r-berechnen",
    "title": "6  Zusammenhänge zwischen nominalen und ordinalen Variablen: Kreuztabellen",
    "section": "6.3 Signifikanztests für Kreuztabellen in R berechnen",
    "text": "6.3 Signifikanztests für Kreuztabellen in R berechnen\nUm zu testen, ob die beiden Variablen zusammenhängen oder unabhängig voneinander sind, nutzen wir - wie oben im Video erläutert - den Chi²-Test, den wir mit der chisq.test()-Funktion ausführen können. Dieser Funktion können wir unsere Tabelle von oben (tabelleSportNachBundesland) übergeben. Die Funktion gibt eine Liste zurück, die wir in einem Objekt speichern sollten. Indem wir das Objekt danach aufrufen, wird uns das Ergebnis angezeigt.\n\n# Berechnet einen Chi²-Test und speichert das Ergebnis in einer Liste namens \"kt_test\"\nkt_test &lt;- chisq.test(tabelleSportNachBundesland)\n\nkt_test\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tabelleSportNachBundesland\nX-squared = 53.925, df = 1, p-value = 0.0000000000002083\n\n\nDas Ergebnis enthält drei Informationen:\n\nDen Chi²-Wert (hier 53,925). Hierbei handelt es sich um die sogenannte Teststatistik des Chi²-Tests. Wie im Video erklärt, wird er aus der Abweichung zwischen beobachteten und erwarteten Werten berechnet.\nDie Anzahl der sogenannten Freiheitsgrade (df für degrees of freedom; hier 1). Freiheitsgrade sind wieder eines der Konzepte in der Statistik, die leider nicht wirklich intuitiv sind. Unten finden Sie eine Definition.\nDer p-Wert (hier 0,0000000000002083). Wie im letzten Kapitel beschrieben, sprechen wir ab p-Werten von 0,05 von statistischer Signifikanz. Üblicherweise wird nur angegeben, ob ein p-Wert größer als 0,05, also nicht-signifikant, kleiner als 0,05, kleiner als 0,01 oder kleiner als 0,001ist. Hier würden wir also lediglich, dass p &lt; 0,001 und entsprechend festhalten, dass der Zusammenhang zwischen dem Bundesland, aus dem die Befragten stammen, und ihrer Mitgliedschaft in einem Sportverein signifikant ist. Übrigens: Hätten wir oben nicht options(scipen = 999) angegeben, würde der p-Wert hier als 2.083e-13 ausgegeben werden. Das steht für die Zahl 2,083, vor die 13 Nullen geschrieben werden.\n\nMit diesen drei Werten können wir den Test wie folgt verschriftlichen:\n\nEin Chi²-Test zeigt, dass ein signifikanter Zusammenhang zwischen dem Bundesland, aus dem die Befragten stammen, und ihrer Mitgliedschaft in einem Sportverein besteht (Chi²(1) = 53,925; p &lt; 0,001).\n\n\n\n\n\n\n\nFreiheitsgrade\n\n\n\nStark simplifiziert sagen sie etwas darüber aus, wie viele Informationen in die Schätzung eines Parameters (hier Chi²) eingeflossen sind. Etwas genauer: Sie geben an, wie viele Werte wir frei variieren können, um auf das tatsächliche Ergebnis zu kommen. Stellen Sie sich z.B. vor, Sie haben 3 Zahlen und wissen nur, dass deren Mittelwert 5 ist. Daraus können Sie schließen, dass die Summe 15 sein muss (denn: 3 x 5 = 15). Nun gibt es aber unendlich viele Möglichkeiten, drei Zahlen auszusuchen, deren Summe genau 15 ist. Die erste Zahl könnte z.B. eine 3 sein und die zweite eine 11. Nun muss die letzte Zahl eine 1 sein, um auf die Summe 15 bzw. den Mittelwert 5 zu kommen. Da wir uns die ersten beiden Zahlen völlig frei aussuchen durften, die dritte dann aber fest vorgegeben war, hat dieser Mittelwert 2 Freiheitsgrade. Die Freiheitsgrade des Chi²-Tests ergeben sich immer aus der Anzahl der Ausprägungen der beiden Variablen: Von beiden wird 1 subtrahiert, dann werden diese Werte miteinander multipliziert. Hier also (2-1) * (2-1) = 1\n\n\nAls nächsten benötigen wir noch die Effektstärke Cramers V. Das effectsize-Paket, das wir oben installiert haben, enthält eine Funktion dazu (cramers_v()), der wir unsere Tabelle übergeben müssen. Zusätzlich geben wir als weiteres Argument alternative = \"two.sided\" an. Damit sagen wir der Funktion, dass wir bevor wir mit unseren Berechnungen angefangen haben, keine gerichtete Hypothese hatten.\n\n# Berechnet die Effektstärke Cramers V\ncramersVSportBundesand &lt;- cramers_v(tabelleSportNachBundesland, alternative = \"two.sided\")\n\ncramersVSportBundesand\n\nCramer's V (adj.) |       95% CI\n--------------------------------\n0.17              | [0.12, 0.22]\n\n\nDas Ergebnis der Funktion besteht aus zwei Teilen: Erstens dem Wert von Cramers V (hier 0,17). Gemäß den Angaben im Video können wir also von einem schwachen Zusammenhang sprechen. Zweitens gibt uns die Funktion einen Bereich von 0,12 bis 0,22 aus, das sogenannte 95% Konfidenzintervall (CI Englisch für confidence interval). Hier haben wir es, mal wieder, mit einem dieser unintuitiven Konzepte der Statistik zu tun.\n\n\n\n\n\n\nKonfidenzintervalle\n\n\n\nKonfidenzintervalle sind ein Maß, das Auskunft darüber gibt, wie viel Unisicherheit mit einer Schätzung (hier z.B. die Schätzung von Cramers V) verbunden ist. Sie basieren auf einer Kernidee der Statistik, nämlich dass jede Schätzung auf einer Stichprobe basiert und wir langfristig nur Sicherheit gewinnen können, indem wir immer wieder verschiedene Stichproben ziehen und bestimmte Parameter (z.B. einen Mittelwert oder eben eine Effektstärke wie Cramers V) schätzen. Die Definition lautet daher, dass langfristig 95% aller berechneten Konfidenzintervalle den wahren Parameter enthalten.\n\n\nMit dem Ergebnis der cramers_v()-Funktion können wir die Verschriftlichung unseres Ergebnisses von oben erweitern:\n\nEin Chi²-Test zeigt, dass ein schwacher signifikanter Zusammenhang zwischen dem Bundesland, aus dem die Befragten stammen, und ihrer Mitgliedschaft in einem Sportverein besteht (Chi²(1) = 53,925; p &lt; 0,001; Cramers V = 0,17; 95%-KI = [0,12; 0,22]).\n\nFür die Darstellung des Chi²-Tests in einem Forschungsbericht reicht dieser Satz im Prinzip aus. Allerdings ist es für Leserinnen und Leser wissenschaftlicher Texte immer leichter, eine Berechnung nachzuvollziehen, wenn Sie deskriptive Angaben machen. Im Fall von Kreuztabellen bietet sich dafür, wer hätte es gedacht, eine Tabelle an.\nDie beobachteten Werte können wir der Tabelle entnehmen, die wir ganz am Anfang erstellt haben (tabelleSportNachBundesland). Wie im Video besprochen sind diese Werte nur bedingt aussagekräftig, sodass wir auch relative Häufigkeiten angeben sollten. Diese können wir relativ einfach in dem Objekt berechnen, dass wir oben für das Stapeldiagramm angelegt haben (df_plot_kreuztabelle). Als Erstes nutzen wir group_by(), um die Daten nach Bundesland zu gruppieren. Anschließend können wir die absoluten Häufigkeiten pro Zelle (gespeichert in Anzahl) durch die Gesamtsumme aller Befragten aus einem Bundesland teilen (sum(Anzahl)). Die erwarteten Werte sind dagegen wieder einfacher, denn sie sind Teil der Liste, die uns die chisq.test()-Funktion zurückgegeben hat. Wir können sie aufrufen, indem wir das entsprechende Listenelement ansprechen: kt_test$expected.\n\n# Beobachtete Werte\ntabelleSportNachBundesland\n\n     Sportverein\nBula  nein  ja\n  RLP  649 385\n  TH   636 172\n\n# Berechnet beobachtete Werte in %\ndf_plot_kreuztabelle &lt;- df_plot_kreuztabelle |&gt;\n  group_by(Bula) |&gt;\n  mutate(Prozent = round((Anzahl / sum(Anzahl))*100,2))\n\ndf_plot_kreuztabelle$Prozent\n\n[1] 62.77 37.23 78.71 21.29\n\n# Erwartete Werte\nkt_test$expected\n\n     Sportverein\nBula      nein       ja\n  RLP 721.3301 312.6699\n  TH  563.6699 244.3301\n\n\nDiese Werte können wir dann in eine Tabelle übertragen:\n\n\n\n\n\n\n\n\n\nBundesland\nkein Mitglied im Sportverein\nMitglied im Sportverein\nGesamt\n\n\n\n\nRheinland-Pfalz\n\n\n\n\n\nBeobachtet\n649\n385\n1034\n\n\nBeobachtet in %\n62,77 %\n37,23 %\n\n\n\nErwartet\n721,33\n312,67\n\n\n\nThüringen\n\n\n\n\n\nBeobachtet\n636\n172\n808\n\n\nBeobachtet in %\n78,71 %\n21,29 %\n\n\n\nErwartet\n563,67\n244,33",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Kreuztabellen</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html",
    "href": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html",
    "title": "7  Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen",
    "section": "",
    "text": "7.1 Statistische Grundlagen",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#deskription-und-visulisierung-der-daten",
    "href": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#deskription-und-visulisierung-der-daten",
    "title": "7  Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen",
    "section": "7.2 Deskription und Visulisierung der Daten",
    "text": "7.2 Deskription und Visulisierung der Daten\nWir starten wie im letzten Kapitel damit, unsere R-Umgebung vorzubereiten, indem wir das tidyverse laden, den Datensatz einlesen und die Optionen so ändern, dass kleine Zahlen in einem uns gewohnten Format angezeigt werden.\n\n# Lädt das tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Liest die Daten ein\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Stellt ein, dass sehr kleine Zahlen normal dargestellt werden\noptions(scipen = 999)\n\nEine gute Möglichkeit, den Zusammenhang zwischen zwei metrischen Variablen grafisch darzustellen, ist durch ein sogenanntes Streudiagramm bzw. eine Punktewolke. Dabei wird eine Variable auf der x- und die andere auf der y-Achse dargestellt. Schauen wir uns das einmal am Besipel der Spalten A202_01 und A202_02 an, in denen das Interesse an Nachrichten über lokale bzw. nationale Geschehnisse abgefragt wurde.1 Zunächst benennen wir beiden Spalten um. Anschließend rufen wir ggplot() auf, geben dort innerhalb von aes() an, welche Variable wir auf der x- bzw. y-Achse darstellen wollen. Dann müssen wir nur noch geom_point() ergänzen. Mit theme_minimal() und labs() verschönern wir unseren Plot gleich noch ein wenig.\n\n# Benennt die Spalten A202_01 und A202_02 um\ndf_lokal &lt;- df_lokal |&gt;  \n  rename(interesseLokal = A202_01,\n         interesseDE = A202_02)\n\n# Erstellt ein Streudiagramm der beiden Spalten\nstreudiagrammInt &lt;- df_lokal |&gt;\n  ggplot(aes(x = interesseLokal, y = interesseDE))+\n  geom_point()+\n  theme_minimal()+\n  labs(x = \"Interesse an lokalen Geschehnissen\", y = \"Interesse an nationalen Geschehnissen\")\n\nstreudiagrammInt\n\n\n\n\n\n\n\n\nDas sieht gar nicht schlecht aus. Wir sehen allerdings, dass relativ viele Befragte die Skala vollständig ausgreizt haben und den jeweiligen Maximalwert (101) angegeben haben. Daran können wir auch ein Verhalten von geom_point() erhnen, das tendenziell problematisch ist. Und zwar werden identische Datenpunkte einfach übereinander gelegt. Wenn also zwei Personen die exakt gleichen Antworten gegeben haben, zeigt geom_point() nur einen Punkt an.\nSchauen wir uns das mal am Beispiel der Spalten A208_01 und A208_03 an. In beiden Spalten wurden Aspekte der politischen Selbstwirksamkeit abgefragt, jeweils bezogen auf den eigenen Wohnort. Die konkreten Formulierungen lauteten:\n\nA208_01: Wichtige Fragen der Lokalpolitik kann ich gut verstehen und einschätzen.\nA208_03: Ich traue mir zu, mich an einem Gespräch über Fragen der Lokalpolitik aktiv zu beteiligen.\n\nSchauen wir uns einmal ein Streudiagramm dieser beiden Variablen an. Da die Items Teil einer größeren Abfrage waren und üblicherweise nicht einzeln ausgwertet werden würden, sind die Beschriftungen hier sehr pragmatisch gewählt.\n\n# Benennt die Spalten A208_01 und A208_03 um\ndf_lokal &lt;- df_lokal |&gt;  \n  rename(polSelbstw1 = A208_01,\n         polSelbstw3 = A208_03)\n\n# Erstellt ein Streudiagramm der beiden Spalten\n\nstreudiagrammSelbstw &lt;- df_lokal |&gt;\n  ggplot(aes(x = polSelbstw1, y = polSelbstw3))+\n  geom_point()+\n  theme_minimal()+\n  labs(x = \"lokale politische Selbstwirksamkeit Item 1\", y = \"lokale politische Selbstwirksamkeit Item 3\")\n\nstreudiagrammSelbstw\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHier wird das Problem an geom_point() deutlich. Die Grafik verrät uns herzlich wenig. Wir haben nach wie vor keinerlei Vorstellung davon, wie stark der Zusammenhang sein könnte. Wir erfahren lediglich, dass jede mögliche Wertekombination im Datensatz enthalten ist. Es gibt also z.B. Leute, die auf eins der Items mit 1 (= “stimme überhaupt nicht zu”) und auf das andere mit 5 (= “stimme sehr zu”) geantwortet haben.\nIn solchen Fällen ist es sinnvoll, die Alternative geom_jitter() zu nutzen. Jitter ist Englisch für zittern und genau das tut die Funktion: Sie verschiebt die einzelen Punkte minimal nach oben, unten, rechts und links (lässt sie also zittern), sodass wir besser erkennen können, wie die Daten verteilt sind. Um das umzusetzen, müssen wir nur geom_point() durch geom_jitter() ersetzen.\n\n# Erstellt ein Streudiagramm der beiden Spalten mit geom_jitter()\n\nstreudiagrammSelbstw &lt;- df_lokal |&gt;\n  ggplot(aes(x = polSelbstw1, y = polSelbstw3))+\n  geom_jitter()+\n  theme_minimal()+\n  labs(x = \"lokale politische Selbstwirksamkeit Item 1\", y = \"lokale politische Selbstwirksamkeit Item 3\")\n\nstreudiagrammSelbstw\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWir sehen nun relativ eindeutig, dass es einen Zusammenhang zwischen den beiden Variablen zu geben scheint.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#korrelationen-berechnen",
    "href": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#korrelationen-berechnen",
    "title": "7  Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen",
    "section": "7.3 Korrelationen berechnen",
    "text": "7.3 Korrelationen berechnen\n\n7.3.1 Pearson Korrelationen\nPearson Korrelationen, also normale Korrelationen zwischen zwei metrischen Variablen, können wir mit der cor.test()-Funktion berechnen, der wir mit den Argumenten x und y zwei Variablen übergeben müssen. Die Funktion gibt uns dann eine Liste zurück. Das Element estimate enthält den Korrelationskoeffizienten. Den dazugehörigen p-Wert können wir über das Element p.value abrufen. Beides speichern wir hier in einem neuen Objekt, indem wir cor.test() innerhalb von summarise() aufrufen. Der Übersicht halber runden wir die jeweiligen Ergebnisse auf 3 Nachkommastellen.\n\n# Erstellt ein neues Objekt in dem der Korrelationskoeffizient und der zugehörigen p-Wert für die Korelation zwischen interesseLokal und interesseDE gespeichert wird.\nkorrelationInt &lt;- df_lokal |&gt;\n  summarise(Korrelation = round(cor.test(x = interesseLokal, \n                                         y = interesseDE)$estimate, 3),\n            pWert = round(cor.test(x = interesseLokal, \n                                   y = interesseDE)$p.value, 3))\n\nkorrelationInt\n\n  Korrelation pWert\n1       0.422     0\n\n\nDie Korrelation beträgt demnach 0,422. Wir können also von einer mäßigen Korrelation sprechen. Der gerundete p-Wert wird als 0 angegeben. Da p-Werte aber immer zwischen 0 und 1 liegen müssen und nie genau 0 (oder 1) sein können, können wir dem entnehmen, dass die Abweichung von 0 erst nach der dritten Nachkommastelle liegt2. Entsprechend würden wir den p-Wert als &lt;0,001 angeben. Bevor wir die Ergebnisse verschriftlichen können, sollten wir noch die Mittelwerte und Standardabweichungen der beiden Variablen berechnen, die immer zusätzlich angegeben werden sollten.\n\n# Berechnet Mittelwert und Standardabweichung der Variablen interesseLokal und interesseDE\n\nMWsInteresse &lt;- df_lokal |&gt;\n  summarise(MWIntLokal = round(mean(interesseLokal, na.rm = TRUE), 2),\n            SDIntLokal = round(sd(interesseLokal, na.rm = TRUE), 2),\n            MWIntDE = round(mean(interesseDE, na.rm = TRUE), 2),\n            SDIntDE = round(sd(interesseDE, na.rm = TRUE), 2))\n\nMWsInteresse\n\n  MWIntLokal SDIntLokal MWIntDE SDIntDE\n1      81.83      20.34   80.73   20.91\n\n\nDamit haben wir alle notwendigen Informationen, um die Ergebnisse zu verschriftlichen:\n\nDurch eine Korrelation wurde geprüft, ob ein Zusammenhang zwischen dem Interesse an lokalen Geschehnissen (M = 81,83; SD = 20,34) und dem Interesse an nationalen Geschehnissen (M = 80,73; SD = 20,91) besteht. Das Ergebnis zeigt, dass ein mäßig starker, signifikanter Zusammenhang besteht (r = 0,422; p &lt; 0,001).\n\n\n\n7.3.2 Rangkorrelationen\nBei der Korrelation, die wir gerade berechnet haben, handelt es sich um eine normale Korrelation zwischen zwei metrischen Variablen. Wie im Video besprochen, können wir auch sogenannte Rangkorrelationen berechnen, wenn mindestens eine der beiden Variablen ordinal skaliert ist. Dazu müssen wir der cor.test()-Funktion lediglich das zusätzliche Argument method mit dem Wert \"spearman\" übergeben.\n\n# Benennt die Spalten A604 und A605 um \n\ndf_lokal &lt;- df_lokal |&gt;\n  rename(Wohndauer = A604,\n         Ortsgroeße = A605)\n\n\n# Erstellt ein neues Objekt in dem der Rangkorrelationskoeffizient und der zugehörigen p-Wert für die Korelation zwischen Wohndauer und Ortsgroeße gespeichert wird.\n\n\nkorrelationOrt &lt;- df_lokal |&gt;\n  summarise(Korrelation = round(cor.test(x = Wohndauer, \n                                         y = Ortsgroeße, \n                                         method = \"spearman\")$estimate, 3),\n            pWert = round(cor.test(x = interesseLokal, \n                                   y = interesseDE,\n                                   method = \"spearman\")$p.value, 3))\n\nWarning: There were 2 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `Korrelation = round(...)`.\nCaused by warning in `cor.test.default()`:\n! Kann exakten p-Wert bei Bindungen nicht berechnen\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nkorrelationOrt\n\n  Korrelation pWert\n1      -0.089     0\n\n\nWenn wir den Befehl ausführen, gibt R eine Warnung aus. Der Kern der Botschaft lautet: Kann exakten p-Wert bei Bindungen nicht berechnen. Damit weist R uns lediglich darauf hin, dass der berechnete p-Wert nur eine Schätzung und keine genaue Berechnung ist. Sie können die Warnung entweder ignorieren, oder dem Aufruf von cor.test() das Argument exact = FALSE hinzufügen. Beachten Sie, dass das nur nötig ist, wenn Sie eine Spearman-Korrelation berechnen.\nBevor wir die Ergebnisse berichten können, müssen wir wieder deskriptive Werte berechnen. Anders als im Beispiel oben, entscheiden wir uns diesmal für den Median, da es sich um ordinale Variablen handelt.\n\n# Berechnet den Median, der Variablen Wohndauer und Ortsgroeße\n\nmedianDauerGroeße &lt;- df_lokal |&gt;\n  summarise(medianDauer = median(Wohndauer, na.rm = TRUE),\n            medianGroeße = median(Ortsgroeße, na.rm = TRUE))\n\nmedianDauerGroeße\n\n  medianDauer medianGroeße\n1           6            3\n\n\nNun haben wir alle relevanten Informationen, um das Ergebnis zu verschriftlichen.\n\nDurch eine Rangkorrelation wurde geprüft, ob ein Zusammenhang zwischen der Wohndauer (Median = 6; “mehr als 20 Jahre”) und der Größe des Wohnorts (Median = 3; “Kleinstadt”) besteht. Die beiden Variablen hängen signifikant, aber sehr schwach, negativ zusammen (r = -0,089; p &lt; 0,001).\n\n\n\n7.3.3 Korrelationskoeffizienten visualisieren\nOben haben wir uns bereits angeschaut, wie wir ein einfaches Streudiagramm von zwei Variablen erstellen können. Wenn wir eine Korrelation berechnen, ist es oft sinnvoll, diese auch grafisch darzustellen. Dazu können wir unser Diagramm von oben durch geom_smooth() mit dem Argument method = \"lm\" ergänzen. Diese Funktion zeichnet eine Linie ein. Durch method = \"lm\" geben wir an, dass wir eine Gerade zeichnen wollen, die der Korrelation entspricht. Um diese Linie herum wird ein grauer Bereich eingezeichnet. Hierbei handelt es sich um das Konfidenzintervall, das wir im letzten Kapitel kennengelernt haben.\n\n# Erstellt ein Streudiagramm der beiden Variablen interesseLokal und interesseDE mit eingezeichneter Korrelationsgerade\n\nstreudiagrammInt &lt;- df_lokal |&gt;\n  ggplot(aes(x = interesseLokal, y = interesseDE))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  theme_minimal()+\n  labs(x = \"Interesse an lokalen Geschehnissen\", y = \"Interesse an nationalen Geschehnissen\")\n\nstreudiagrammInt\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#korrelationen-erkennen-und-einschätzen",
    "href": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#korrelationen-erkennen-und-einschätzen",
    "title": "7  Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen",
    "section": "7.4 Korrelationen erkennen und einschätzen",
    "text": "7.4 Korrelationen erkennen und einschätzen\nKorrelationen anhand von Daten zu erkennen, ist gar nicht so einfach. Gleichzeitig können sehr kleine p-Werte dazu verleiten, die tatsächliche Bedeutung eines Zusammenhangs zu überschätzen. Beispielsweise hat eine Auswertung von Meta-Analysen in der Kommunikationswissenschaft ergeben, dass die durchschnittliche Effektstärke im Fach gerade einmal r = 0,21 beträgt (Rains et al., 2018). Grafisch dargestellt sieht eine solche Korrelation etwa so aus:\n\n\n\n\n\n\n\n\n\nDie beiden fiktiven Variablen hängen zwar zusammen, aber wirklich leicht zu erkennen ist das optisch nicht. Zusammenhänge zu erkennen und Gefühl dafür zu bekommen, wie verschieden starke Korrelationen eigentlich aussehen ist vor allem Übungssache. Wenn Sie Lust haben, können Sie mit dem kleinen Spiel unten etwas üben.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#footnotes",
    "href": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#footnotes",
    "title": "7  Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen",
    "section": "",
    "text": "Auf die Datentransformation aus der Präsenzübung verzichten wir an dieser Stelle.↩︎\nIn diesem konkreten Fall folgt die erste Ziffer, die nicht Null ist an Stelle 81. Der p-Wert lautet: 0,000000000000000000000000000000000000000000000000000000000000000000000000000000008204762↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen</span>"
    ]
  },
  {
    "objectID": "Regressionen I.html",
    "href": "Regressionen I.html",
    "title": "8  Regressionen I",
    "section": "",
    "text": "8.1 Statistische Grundlagen",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regressionen I</span>"
    ]
  },
  {
    "objectID": "Regressionen I.html#vorbereitung",
    "href": "Regressionen I.html#vorbereitung",
    "title": "8  Regressionen I",
    "section": "8.2 Vorbereitung",
    "text": "8.2 Vorbereitung\nWir starten wieder damit, unsere R-Umgebung vorzubereiten, indem wir das tidyverse und das Paket effectsize laden, den Datensatz einlesen und die Optionen so ändern, dass kleine Zahlen in einem uns gewohnten Format angezeigt werden.\n\n# Lädt das tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Lädt effectsize\nlibrary(effectsize)\n\nWarning: Paket 'effectsize' wurde unter R Version 4.4.3 erstellt\n\n# Liest die Daten ein\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Stellt ein, dass sehr kleine Zahlen normal dargestellt werden\noptions(scipen = 999)\n\nAußerdem sollten wir die Variablen, mit denen wir arbeiten umbenennen und ggf. transformieren.\nIm folgenden Beispiel werden wir drei Variablen brauchen, die Sie alle schon kennen: Die Spalten A202_01 und A202_02 enthalten das Interesse an lokalen und nationalen Geschehnissen. Die Spalte enthält A601_01 das Geburtsjahr. Zunächst benennen wir die Spalten um.\n\n# Benennt die Spalten A202_01 und A202_02 um\ndf_lokal &lt;- df_lokal |&gt;  \n  rename(interesseLokal = A202_01,\n         interesseDE = A202_02,\n         geburtsjahr = A601_01)\n\nAls nächstes transformieren wir die Spalten interesseLokal und interesseDE so, dass sie nicht mehr Werte von 1 bis 101 enthalten, sondern von 0 bis 1.\n\n# Transformiert die Spalten so, dass sie Werte von 0 bis 1 statt 1 bis 101 enthalten.\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(interesseLokalRec = (interesseLokal-1)/100,\n         interesseDERec = (interesseDE-1)/100)\n\nAbschließend transformieren wir das Geburtsjahr in das Alter um. Der Vorgang ist relativ komplex, Sie kennen ihn aber bereits aus Kapitel 3.\n\n# Transfomiert das Geburtsjahr zum Alter. Siehe Kapitel 3 für Details\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(alter = str_trim(geburtsjahr)) |&gt;\n  mutate(geburtsjahr = str_sub(geburtsjahr, -4)) |&gt;\n  mutate(geburtsjahr = as.integer(geburtsjahr)) |&gt;\n  mutate(geburtsjahr = ifelse(geburtsjahr &lt; 1000, geburtsjahr+1000, geburtsjahr)) |&gt;\n  mutate(alter = 2022 - geburtsjahr)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `geburtsjahr = as.integer(geburtsjahr)`.\nCaused by warning:\n! NAs durch Umwandlung erzeugt\n\n\nAnschließend können wir mit der Berechnung der Regression beginnen.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regressionen I</span>"
    ]
  },
  {
    "objectID": "Regressionen I.html#regressionen-berechnen",
    "href": "Regressionen I.html#regressionen-berechnen",
    "title": "8  Regressionen I",
    "section": "8.3 Regressionen berechnen",
    "text": "8.3 Regressionen berechnen\nUm Regressionen zu berechnen nutzen wir die lm()-Funktion. lm steht dabei für lineares Modell. Der Funktion müssen wir eine Formel übergeben, die immer nach demselben Schema aufgebaut ist: links steht die abhängige Variable, darauf folgt eine Tilde (~). Rechts von dieser stehen dann alle unabhängigen Variablen. Die Tilde können Sie mit der Tastenkombination  +  schreiben. Das Ergebnis dieser Funktion sollten Sie immer in ein Objekt schreiben.\nWichtig ist, dass Sie der lm()-Funktion immer den Datensatz durch das data-Argument übergeben.\n\n8.3.1 Einfache lineare Regression\nSchauen wir uns zunächst an, wie wir eine einfache Regression mit einer unabhängigen Variable berechnen können.\nKonkret versuchen wir, das Interesse an lokalen Geschehnissen mit dem Alter der Befragten zu erklären. Bevor wir die Regression berechnen, schauen wir uns die Korrelation der beiden Variablen an. Das ist nicht immer zwingend notwendig, aber häufig sinnvoll.\n\n# Berechnet die Korrelation vom Interesse an lokalen Geschehnissen und dem Alter der Befragten\nkorrIntAlter &lt;- df_lokal |&gt;\n  summarise(Korrelation = round(cor.test(x = interesseLokalRec, \n                                         y = alter)$estimate, 3),\n            pWert = round(cor.test(x = interesseLokalRec, \n                                   y = alter)$p.value, 3))\n\nkorrIntAlter\n\n  Korrelation pWert\n1       0.204     0\n\n\nDie Korrelation ist schwach (r = 0,204) und signifikant (p &lt; 0,001).\nAls Nächstes berechnen wir die Regression. Als Formel übergeben wir der Funktion interesseLokalRec ~ alter.\n\n# Berechnet eine Regression mit dem Interesse an lokalen Geschehnissen als AV und dem Alter als UV\nlmInteresseLokal &lt;- lm(interesseLokalRec ~ alter, data = df_lokal)\n\nMit der summary()-Funktion können wir nun das Ergebnis betrachten:\n\n# Zeigt die Ergebnisse der Regression an\nsummary(lmInteresseLokal)\n\n\nCall:\nlm(formula = interesseLokalRec ~ alter, data = df_lokal)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.87735 -0.08972  0.04675  0.14720  0.28495 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 0.6744726  0.0158084  42.666 &lt;0.0000000000000002 ***\nalter       0.0027050  0.0003042   8.892 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1988 on 1821 degrees of freedom\n  (23 Beobachtungen als fehlend gelöscht)\nMultiple R-squared:  0.04161,   Adjusted R-squared:  0.04108 \nF-statistic: 79.06 on 1 and 1821 DF,  p-value: &lt; 0.00000000000000022\n\n\nGehen wir das Ergebnis einmal der Reihe nach durch. Ganz oben in der Ausgabe sehen Sie die Überschrift Call:. Darunter sagt R uns, was wir genau berechnet haben.\nAls Nächstes sehen wir die Residuals, also die Schätzfehler. Konkret sagt uns R etwas über die Verteilung. Für heute ignorieren wir diesen Teil.\nDanach wird es unter der Überschrift Coefficients spannend. Hier erfahren wir etwas über die Koeffizienten. Der erste, die sogenannte Regressionskonstante oder Intercept gibt an, welchen Wert die abhängige Variable dem Modell nach hat, wenn die unabhängige Variable den Wert 0 hat. Beachten Sie, dass das in unserem konkreten Fall wenig Sinn ergibt. Weder haben wir Personen im Alter von 0 Jahren befragt, noch ist es vorstellbar, dass das irgendwie möglich ist. Relevanter ist dagegen der Koeffizient für das Alter der Befragten. In der Spalte Estimate sehen Sie den Regressionskoeffizienten B. Auf den ersten Blick sieht dieser Wert sehr klein aus (B = 0,002), aber bedenken Sie, dass die abhängige Variable (Interesse an lokalen Geschehnissen) nur von 0 bis 1 geht. In der zweiten Spalte steht der dazugehörige Standardfehler (hier: 0.0003). Darauf folgt der t-Wert. Hierbei handet es sich um eine Teststatistik, die für den Signifikanztest berechnet wird. Das Prinzip kennen Sie schon vom Chi²-Test, nur dass hier eine andere Statistik berechnet wird. Der t-Wert wird uns hier als 8,892 angegeben. Als letztes sehen Sie den p-Wert. Hier sehen wir direkt, dass der Wert kleiner als 0,001 und somit signifikant ist.\nDie nächsten paar Zeilen können wir überspringen. Relevant wird es erst wieder bei der Angabe von R². Hier sehen wir zwei Werte: Multiple R-squared und Adjusted R-squared. Der erste Wert wurde im Video nur als R² bezeichnet. Und wir dort besprochen entspricht er genau der Korrelation unserer beiden Variablen zum Quadrat:\n\n# Mutipliziert den oben berechneten Korrelationskoeefizienten mit sich selbst:\nkorrIntAlter$Korrelation * korrIntAlter$Korrelation\n\n[1] 0.041616\n\n\nDie unabhängige Variable erklärt also ungefähr 4 Prozent der Varianz der abhängigen Variable. Bei Adjusted R-suqred handelt es sich um den korrigierten R² Wert, den Sie nur angeben müssen, wenn Sie mehr als einen Prädiktor (also mehr als eine unabhängige Variable) nutzen.\nIn der letzten Zeile finden Sie einen Test des Gesamtmodells. Zunächst steht dort die F-Statistic. Ähnlich wie Chi² und oben t, handelt es sich hierbei um eine weitere Teststatistik, diesmal eben für das Gesamtmodell. Der zugehörige Wert ist hier 79,06. Dahinter stehen die zugehörigen Freiheitsgrade (DF für degrees of freedom; 1 und 1821), die etwas über die Anzahl der Prädiktoren und die Stichprobengröße sagen. Schließlich sehen Sie einen Weiteren p-Wert (&lt; 0,001), der uns verrät, dass das Gesamtmodell signifikant ist.\nDamit haben wir schon fast alle relevanten Infos, um das Ergebnis zu verschriftlichen. Was fehlt, ist nur der standardisierte Koeffizient beta für das Alter. Diesen können wir mit der standardize_parameters()-Funktion aus dem effectsize-Paket anzeigen lassen. Dieser Funktion übergeben wir das Objekt, in das wir oben das Ergebnis der Regression gespeichert haben:\n\n# Zeigt den standardisierten Koeffizienten beta für das Alter an. \nstandardize_parameters(lmInteresseLokal)\n\n# Standardization method: refit\n\nParameter   | Std. Coef. |        95% CI\n----------------------------------------\n(Intercept) |  -1.98e-16 | [-0.04, 0.04]\nalter       |       0.20 | [ 0.16, 0.25]\n\n\nIn der Spalte Std. Coef. (für standardized Coefficient) zeigt R uns den standardisierten Koeffizienten Beta an, der hier 0,20 beträgt. Wie im Video angesprochen handelt es sich dabei genau um die Korrelation zwischen den beiden Variablen. Aber beachten Sie, dass das nur bei einfachen Regressionen der Fall ist. Zusätzlich gibt uns die Funktion ein Konfidenzintervall aus.\nTragen wir die Ergebnisse also einmal zusammen: Das Alter hat einen signifikanten, aber schwachen positiven Effekt auf das Interesse an lokalen Geschehnissen. Ältere Menschen tendieren also eher dazu, sich dafür zu interessieren. Außerdem ist das Gesamtmodell signifikant, erklärt aber nur etwa 4 Prozent der Varianz. Etwas formaler können wir die Ergebnisse so verschriftlichen:\n\nDurch eine Regression wurde geprüft, ob das Alter der Befragten (M = 49,66; SD = 15,31) einen Einfluss auf ihr Interesse an lokalen Geschhnissen hat (M = 0,81; SD = 0,20). Das Modell war insgesamt signifikant (F(1, 1821) = 79,06; p &lt; 0,001), konnte aber nur etwa 4 Prozent der Varianz erklären (R² = 0,042). Der Effekt des Alters war signifikant, aber schwach (beta = 0,20; p &lt; 0,001).\n\n\n\n8.3.2 Multiple Regression\nDas Vorgehen bei multiplen Regressionen ist quasi identisch. Um weitere Prädiktoren hinzuzufügen, können wir unsere Formel ganz einfach erweitern. Schauen wir es uns hier einmal an, indem wir der Regression von oben das Interesse an Geschehnissen in Deutschland als Prädiktor hinzufügen:\n\n# Erweitert die Regression von oben um das Interesse an Geschehnissen in Deutschland\nlmInteresseLokalErweitert &lt;- lm(interesseLokalRec ~ alter + interesseDERec, data = df_lokal)\n\nSchauen wir uns das Ergebnis an:\n\n# Zeigt die Ergebnisse der Regression an\nsummary(lmInteresseLokalErweitert)\n\n\nCall:\nlm(formula = interesseLokalRec ~ alter + interesseDERec, data = df_lokal)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89259 -0.08302  0.04036  0.10736  0.51708 \n\nCoefficients:\n                Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)    0.4243107  0.0198217  21.406 &lt; 0.0000000000000002 ***\nalter          0.0015028  0.0002866   5.244          0.000000176 ***\ninteresseDERec 0.3886329  0.0209870  18.518 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1824 on 1820 degrees of freedom\n  (23 Beobachtungen als fehlend gelöscht)\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1927 \nF-statistic: 218.4 on 2 and 1820 DF,  p-value: &lt; 0.00000000000000022\n\n\nVerglichen mit dem Ergebnis von oben sollten Ihnen hier einige Dinge auffallen:\n\nDas (korrigierte) R² ist deutlich höher! Das ist auch nachvollziehbar, schließlich haben wir schon im letzten Kapitel gesehen, dass das Interesse an Geschehnissen in Deutschland und das Interesse an lokalen Geschehnissen mäßig stark korrelieren.\nEntsprechend hat sich auch der Test des Gesamtmodells leicht geändert (anderer F-Wert, andere Freiheitsgrade).\nDer Effekt des Alters ist etwas schwächer als im ersten Modell.\nDer Effekt des Interesses an Geschehnissen in Deutschland deutlich stärker als der des Alters. Aber: Beachten Sie, dass wir bisher nur die unstandardisierten Koeffizienten betrachtet haben!\n\nAußerdem sollten Sie eine wichtige Sache beachten: Die Interpretation der Koeffizienten ändert sich in der multiplen Regression leicht! Im Video haben wir gesagt, dass die Effekte jeweils so interpretiert werden können, dass die Effekte der anderen Variablen “herausgerechnet” werden. Ganz konkret ist es so, dass die Effekte jeweils angeben, wie stark der Einfluss einer Variable ist, wenn alle anderen Variablen den Wert 0 annehmen. Wie oben schon, haben wir hier allerdings das Problem, dass der Wert 0 nicht immer sinnvoll ist, z.B. beim Alter. Dieses Problem haben die standardisierten Koeffizienten nicht.\nSchauen wir uns die also noch an:\n\n# Zeigt den standardisierten Koeffizienten beta für die beiden Prädiktoren an\nstandardize_parameters(lmInteresseLokalErweitert)\n\n# Standardization method: refit\n\nParameter      | Std. Coef. |        95% CI\n-------------------------------------------\n(Intercept)    |  -2.65e-16 | [-0.04, 0.04]\nalter          |       0.11 | [ 0.07, 0.16]\ninteresseDERec |       0.40 | [ 0.36, 0.44]\n\n\nFür die Berechnung der standardisierten Koeffizienten, werden die Variablen selbst standardisiert. Das bedeutet, dass sie so transformiert werden, dass sie den Mittelwert 0 und die Standardabweichung 1 haben. Das heißt, der Effekt einer Variable entspricht nun nicht mehr dem Effekt, wenn alle anderen Variablen den Wert 0 haben, sondern wenn diese Variablen ihren Mittelwert annehmen.\nSchauen wir uns nun aber die eigentlichen Effekte an. Wie wir oben schon erahnen konnten, ist der Effekt des Alters etwas schwächer. Er wird nun nur noch als 0,11 angegeben. Der Effekt des Interesses an Geschehnissen in Deutschland beträgt dagegen 0,40. Er ist also mäßig stark. Beachten Sie, dass die beta-Werte nun nicht mehr der einfachen Korrelation zwischen den jeweiligen Variablen entsprechen.\nVerschriftlichen wir die Ergebnisse noch einmal:\n\nDurch eine Regression wurde geprüft, ob das Alter der Befragten (M = 49,66; SD = 15,31) und das Interesse an Geschehnissen in Deutschland (M = 0,80; SD = 0,21) einen Einfluss auf ihr Interesse an lokalen Geschehnissen haben (M = 0,81; SD = 0,20). Das Modell war insgesamt signifikant (F(2, 1820) = 218,4; p &lt; 0,001) und konnte etwa 19,3 Prozent der Varianz erklären (R² = 0,193). Der Effekt des Alters war signifikant, aber schwach (beta = 0,11; p &lt; 0,001). Dagegen hatte das Interesse an Geschehnissen in Deutschland einen mäßigen Effekt (beta = 0,40; p &lt; 0,001).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regressionen I</span>"
    ]
  },
  {
    "objectID": "Regressionen II.html",
    "href": "Regressionen II.html",
    "title": "9  Regressionen II",
    "section": "",
    "text": "9.1 Vorbereitung\nWir starten wieder damit, unsere R-Umgebung vorzubereiten, indem wir das tidyverse und das Paket effectsize laden, den Datensatz einlesen und die Optionen so ändern, dass kleine Zahlen in einem uns gewohnten Format angezeigt werden.\n# Lädt das tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Lädt effectsize\nlibrary(effectsize)\n\nWarning: Paket 'effectsize' wurde unter R Version 4.4.3 erstellt\n\n# Liest die Daten ein\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Stellt ein, dass sehr kleine Zahlen normal dargestellt werden\noptions(scipen = 999)\nWir verwenden wieder einige Spalten, die Sie mittlerweile kennen. Auf die entsprechenden Transformationen gehen wir daher nicht erneut ein. Die einzige Änderung zu vorherigen Versionen dieser Transformationen ist, dass wir beim Geschlecht auch den Wert “divers” als fehlend deklarieren. Das liegt ganz einfach daran, dass wir nur sehr wenige Fälle in dieser Gruppe haben und die Interpretation dadurch für dieses Beispiel unnötig kompliziert wird.\n# Benennt die Spalten A202_01 und A202_02 um\ndf_lokal &lt;- df_lokal |&gt;  \n  rename(interesseLokal = A202_01,\n         interesseDE = A202_02,\n         geburtsjahr = A601_01)\n\n\n# Transformiert die Spalten so, dass sie Werte von 0 bis 1 statt 1 bis 101 enthalten.\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(interesseLokalRec = (interesseLokal-1)/100,\n         interesseDERec = (interesseDE-1)/100)\n\n\n# Transfomiert das Geburtsjahr zum Alter. Siehe Kapitel 3 für Details\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(alter = str_trim(geburtsjahr)) |&gt;\n  mutate(geburtsjahr = str_sub(geburtsjahr, -4)) |&gt;\n  mutate(geburtsjahr = as.integer(geburtsjahr)) |&gt;\n  mutate(geburtsjahr = ifelse(geburtsjahr &lt; 1000, geburtsjahr+1000, geburtsjahr)) |&gt;\n  mutate(alter = 2022 - geburtsjahr)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `geburtsjahr = as.integer(geburtsjahr)`.\nCaused by warning:\n! NAs durch Umwandlung erzeugt\n\n# Erstellt eine neue Spalte aus der Geschlechtsabfrage. Erst werden die Werte \"keine Angabe\" und \"divers\" als fehlend deklariertm dann wird ein Faktor mit den übrigen  Kategorien erstellt\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geschlecht = ifelse(A602 == 3 | A602 == 4, NA, A602)) |&gt;\n  mutate(geschlecht = factor(geschlecht, labels = c(\"männlich\", \"weiblich\")))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regressionen II</span>"
    ]
  },
  {
    "objectID": "Regressionen II.html#multiple-regressionen-mit-nominalem-prädiktor",
    "href": "Regressionen II.html#multiple-regressionen-mit-nominalem-prädiktor",
    "title": "9  Regressionen II",
    "section": "9.2 Multiple Regressionen mit nominalem Prädiktor",
    "text": "9.2 Multiple Regressionen mit nominalem Prädiktor\nGrundsätzlich können wir nominale Prädiktoren ganz normal dem Regressionsmodell hinzufügen, wie wir es auch bei den metrischen Prädiktoren gemacht haben. Schauen wir uns das einmal an, indem wir die Regression aus dem letzten Kapitel um das Geschlecht als UV erweitern:\n\n# Berechnet eine Regression mit dem Interesse an lokalen Geschehnissen als AV und dem Alter, dem Interesse an Geschehnissen in Deutschland und dem Geschlecht als UVs\nlmInteresseLokal &lt;- lm(interesseLokalRec ~ alter + interesseDERec + geschlecht, data = df_lokal)\n\n# Zeigt die Ergebnisse der Regression an\nsummary(lmInteresseLokal)\n\n\nCall:\nlm(formula = interesseLokalRec ~ alter + interesseDERec + geschlecht, \n    data = df_lokal)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89442 -0.07994  0.03707  0.10846  0.50426 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)        0.4019646  0.0209543  19.183 &lt; 0.0000000000000002 ***\nalter              0.0016333  0.0002896   5.639         0.0000000198 ***\ninteresseDERec     0.3880690  0.0210399  18.444 &lt; 0.0000000000000002 ***\ngeschlechtweiblich 0.0300721  0.0086718   3.468             0.000537 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1811 on 1791 degrees of freedom\n  (51 Beobachtungen als fehlend gelöscht)\nMultiple R-squared:  0.199, Adjusted R-squared:  0.1977 \nF-statistic: 148.4 on 3 and 1791 DF,  p-value: &lt; 0.00000000000000022\n\n\nWie Sie sehen, haben wir nun eine neue Zeile in der Tabelle über die Effekte, die mit geschlechtweiblich betitelt ist. Die Zeile gibt an, inwiefern sich die abhängige Variable (also das Interesse an lokalen Geschehnissen) ändert, wenn eine befragte Person nicht männlich, sondern weiblich ist. D.h., der positive Effekt in der Zeile geschlechtweiblich bedeutet, dass Frauen ein höheres Interesse an lokalen Geschehnissen haben als Männer.\nAußerdem ändert sich beim Einschluss von kategorialen Prädiktoren die Interpretation der Regressionskontante (Intercept). Im letzten Kapitel haben wir gelernt, dass dieser Wert dem durchschnittlichen Wert der abhängigen Variable entspricht, wenn alle unabhängigen Variablen den Wert 0 annehmen. Das ist grundsätzlich auch weiterhin der Fall, allerdings müssen wir nun beachten, dass dieser Wert nur noch dem Durchschnitt der Männer in der Stichprobe entspricht. Man sagt auch, dass es sich bei der Ausprägung männlich um die Referenzkategorie der Variable geschlecht handelt. Das erkennen Sie daran, dass oben in der Tabelle für die Effektstärke die andere Ausprägung (also “weiblich”) aufgeführt sind.\nWerfen wir als nächstes einen Blick auf die standardisierten Effektstärken:\n\n# Zeigt den standardisierten Koeffizienten beta für die  Prädiktoren an\nstandardize_parameters(lmInteresseLokal)\n\n# Standardization method: refit\n\nParameter             | Std. Coef. |         95% CI\n---------------------------------------------------\n(Intercept)           |      -0.08 | [-0.14, -0.02]\nalter                 |       0.12 | [ 0.08,  0.17]\ninteresseDERec        |       0.40 | [ 0.36,  0.44]\ngeschlecht [weiblich] |       0.15 | [ 0.06,  0.23]\n\n\nWir sehen einerseits, dass sich die Effektstärken des Alters und des Interesses an Geschehnissen in Deutschland im Vergleich zum Modell aus dem letzten Kapitel leicht geändert haben. Andererseits sehen wir nun auch eine Zeile für den Effekt der Variable geschlecht. Im Video im letzten Kapitel hatten wir festgehalten, dass standardisierte Effekte nur für metrische Prädiktoren sinnvoll angegeben werden können, da sie angeben, um wie viele Standardabweichungen sich die abhängige Variable ändert, wenn sich die entsprechende unabhängige Variable um eine Standardabweichung erhöht. Was bedeutet dieser Effekt also nun? Für nominale Variablen gibt die Funktion weiterhin aus, wie sich die abhängige Variable verändert, wenn sich die nominale Variable um eine Einheit erhöht bzw. verändert (also z.B. weiblich statt männlich). Der Unterschied zur Tabelle der Koeffizienten aus der summary()-Funktion besteht darin, dass die Veränderung hier jetzt nicht mehr einer Veränderung in den Rohdaten entspricht, sondern in Standardabweichungen ausgedrückt wird. Oder anders gesagt: verglichen mit Männern haben Frauen ein um 0,15 Standardabweichungen höheres Interesse an lokalen Geschehnissen.\nBeachten Sie, dass Sie beim Berichten von diesen Effekten ein bisschen Vorsicht walten lassen müssen. Wenn Sie ansonsten nur nicht-standardisierte Effekte berichten, haben Sie zwar kein Problem mit nominalen Prädiktoren, aber möglicherweise sind Ihre Ergebnisse dann etwas schwerer verständlich. Wenn Sie aber alle Effekte von metrischen Variablen in standardisierter Form ausdrücken, sollten Sie Ihren Leserinnen und Lesern unmissverständlich klar machen, dass sich die Standardisierung bei den Effekten von nominalen Prädiktoren nur auf die abhängige Variable bezieht.\n\n\n\n\n\n\nOrdinale Prädiktoren\n\n\n\nGrundsätzlich können Sie ordinale Prädiktoren genauso behandeln wie nominale Prädiktoren. Sollte die ordinale Variable aber viele Ausprägungen haben (z.B. über 10 verschiedene Einkommensgruppen), wird die Interpretation der Ergebnisse schnell sehr komplex. In solchen Fällen kann es sinnvoll sein, entweder die Anzahl der Ausprägungen zu reduzieren (z.B. zu geringen, mittleren und hohen Einkommen) oder die Variable als quasi-metrisch zu behandeln. Seien Sie hierbei aber sehr vorsichtig bei der Interpretation!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regressionen II</span>"
    ]
  },
  {
    "objectID": "Regressionen II.html#interaktionseffekte",
    "href": "Regressionen II.html#interaktionseffekte",
    "title": "9  Regressionen II",
    "section": "9.3 Interaktionseffekte",
    "text": "9.3 Interaktionseffekte\n\n9.3.1 Was sind Interaktionseffekte?\nOben im Kapitel haben wir bereits erfahren, dass Interaktionseffekte im Prinzip nichts anderes bedeuten, als dass der Effekt einer Variable von einer anderen abhängig ist. Stellen Sie sich z.B. vor, dass Sie herausfinden möchten, ob das Schauen eines Films mit Altersfreigabe FSK 16 einen Effekt auf die Stimmung von unter 16-Jährigen am nächsten Tag hat. Um den elterlichen Einfluss zu kontrollieren, erheben Sie auch, ob die Eltern im Anschluss an den Film mit ihren Kindern über das Gesehene gesprochen haben. Ein klassischer Interaktionseffekt würde dann vorliegen, wenn Sie zwar einen negativen Effekt des Films auf die Stimmung finden, aber nur in den Fällen, in denen kein Anschlussgespräch stattgefunden hat, wohingegen Kinder, die mit ihren Eltern über den Film gesprochen haben, entweder eine unveränderte oder sogar bessere Stimmung am nächsten Tag aufweisen.\nMathematisch funktionieren Interaktionseffekte so, dass das Produkt der beiden Variablen in das Modell aufgenommen wird.\n\n\n9.3.2 Interaktionseffekte berechnen\nUm einen Interaktionseffekt von zwei Variablen zu berechnen, müssen wir nur die Gleichung in unserem Aufruf der lm()-Funktion etwas anpassen. Statt zwei Prädiktoren mit einem + zu verbinden, können wir sie mit einem * verbinden. Das Resultat sehen Sie hier:\n\n# Berechnet eine Regression mit dem Interesse an lokalen Geschehnissen als AV und dem Alter, dem Interesse an Geschehnissen in Deutschland und dem Geschlecht als UVs. Dabei wird ein Interaktionseffekt zwischen Alter und Geschlecht angenommen.\nlmInteraktion &lt;- lm(interesseLokalRec ~ alter*geschlecht + interesseDERec, data = df_lokal)\n\n# Zeigt die Ergebnisse der Regression an\nsummary(lmInteraktion)\n\n\nCall:\nlm(formula = interesseLokalRec ~ alter * geschlecht + interesseDERec, \n    data = df_lokal)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89015 -0.08269  0.03840  0.10593  0.49950 \n\nCoefficients:\n                           Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)               0.3654555  0.0263466  13.871 &lt; 0.0000000000000002 ***\nalter                     0.0023459  0.0004257   5.510         0.0000000411 ***\ngeschlechtweiblich        0.0948118  0.0296693   3.196              0.00142 ** \ninteresseDERec            0.3874885  0.0210168  18.437 &lt; 0.0000000000000002 ***\nalter:geschlechtweiblich -0.0012933  0.0005669  -2.281              0.02264 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1809 on 1790 degrees of freedom\n  (51 Beobachtungen als fehlend gelöscht)\nMultiple R-squared:  0.2014,    Adjusted R-squared:  0.1996 \nF-statistic: 112.8 on 4 and 1790 DF,  p-value: &lt; 0.00000000000000022\n\n\nIn der Tabelle aus der summary()-Funktion sehen wir jetzt, dass die Effekte des Alters und des Geschlechts (weiblich vs. männlich) weiterhin positiv sind. Diese beiden Effekte werden auch als Haupteffekte der beiden Variablen bezeichnet. Wichtig ist, dass diese Effekte in Anwesenheit einer signifikanten Interpretation mit äußerster Vorsicht interpretiert werden sollten! Schließlich wissen Sie dann schon, dass die Effekte von der jeweils anderen Variable abhängig sind.\nDie neue Zeile ganz unten (alter:geschlechtweiblich) zeigt den Interaktionseffekt an, der negativ und signifikant ist. Das bedeutet, Frauen haben zwar insgesamt ein höheres Interesse an lokalen Geschehnissen als Männer und ältere Menschen haben ein höheres Interesse als jüngere Menschen, aber je älter die Frauen werden, desto geringer ist ihr Interesse im Vergleich zu älteren Männern.\nZugegebenermaßen ist diese Interpretation nicht ganz offensichtlich. Bei der Einordnung hilft, sich ins Gedächtnis zu rufen, dass die Effekte immer aussagen, wie sich die abhängige Variable verändert, wenn wir die Werte der unabhängigen Variablen einzeln ändern. Also konkret: der positive Effekt des Alters sagt aus, dass ältere Menschen ein überdurchschnittliches Interesse an lokalen Geschehnissen haben, wenn alle anderen Variablen konstant gehalten werden. Gleiches gilt für Frauen vs. Männer. Der Interaktionseffekt sagt nun aus, wie sich das Interesse verändert, wenn eine Person weiblich statt männlich ist und älter statt jünger. Wir ändern also beide Variablen gleichzeitig und halten nur noch das Interesse an Geschehnissen in Deutschland konstant.\nDer Vollständigkeit halber lassen wir uns auch noch die standardisierten Effektstärken anzeigen:\n\n# Zeigt den standardisierten Koeffizienten beta für die Prädiktoren an\nstandardize_parameters(lmInteraktion)\n\n# Standardization method: refit\n\nParameter                     | Std. Coef. |         95% CI\n-----------------------------------------------------------\n(Intercept)                   |      -0.09 | [-0.15, -0.03]\nalter                         |       0.18 | [ 0.11,  0.24]\ngeschlecht [weiblich]         |       0.15 | [ 0.07,  0.24]\ninteresseDERec                |       0.40 | [ 0.36,  0.44]\nalter × geschlecht [weiblich] |      -0.10 | [-0.18, -0.01]\n\n\n\n\n9.3.3 Interaktionseffekte visualisieren\nEine einfache Möglichkeit, Interaktionseffekte zu verstehen ist, sie zu visualisieren. Dazu nutzen wir die Funktion interact_plot() aus dem Paket interactions, das wir zunächst noch installieren müssen.\nDie Funktion erstellt basierend auf ggplot() eine Grafik, nimmt uns dabei aber viel manuelle Arbeit ab. Der Nachteil ist, dass wir etwas weniger Möglichkeiten haben, die Grafik individuell anzupassen.\nGrundsätzlich reicht es, der Funktion einige wenige Argumente zu übergeben:\n\nDas Modell, das wir zuvor mit lm() geschätzt haben.\nDen Prädiktor, den wir auf der x-Achse darstellen wollen. Das Argument heißt pred. Hier nehmen wir das Alter.\nDen Moderator, den wir im Argument modx angeben. Hier also das Geschlecht.\n\nAußerdem geben wir einige weitere Argumente an, durch die das Resultat noch etwas besser wird:\n\nDurch interval = TRUE können wir Konfidenzintervalle einzeichnen, wodurch die mit der Schätzung verbundene Unsicherheit visualisiert wird.\nMit x.label und y.label können wir die Achsen manuell beschriften.\nMit legend.main können wir den Titel der Legende anpassen.\n\n\n# Versucht das Paket \"interactions\" zu laden. Falls es nicht installiert ist, wird es erst installiert und dann geladen\nif(!require(interactions)){\n  install.packages(\"interactions\")\n  library(interactions)\n}\n\nLade nötiges Paket: interactions\n\n# Erstellt das Objekt \"plotInteraktion\", in dem die Interaktion visualisiert wird\nplotInteraktion &lt;- interact_plot(lmInteraktion, pred = alter, modx = geschlecht, interval = TRUE, x.label = \"Alter\", y.label = \"Interesse an lokalen Geschehnissen\", legend.main = \"Geschlecht\")\n\n# Zeigt die Grafik an\nplotInteraktion\n\n\n\n\n\n\n\n\nDie Grafik zeigt nun relativ eindeutig, dass jüngere Frauen ein deutlich höheres Interesse haben als jüngere Männer. Dafür ist der Effekt für Männer dann stärker, sprich, die Gerade ist deutlich steiler. Daraus resultiert dann, dass ältere Männer ein etwas höheres Interesse an lokalen Geschehnissen haben, wobei sich die beiden Gruppen im hohen Alter nur noch geringfügig unterscheiden.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regressionen II</span>"
    ]
  },
  {
    "objectID": "Regressionen II.html#voraussetzungen",
    "href": "Regressionen II.html#voraussetzungen",
    "title": "9  Regressionen II",
    "section": "9.4 Voraussetzungen",
    "text": "9.4 Voraussetzungen\nIn diesem Teil wird es zwischendurch ziemlich komplex, daher vorab das für Sie Wichtigste: Die meisten Voraussetzungen von Regressionen können wir in den meisten Fällen ignorieren! Lesen Sie die ersten beiden aufmerksam durch, auch Nummer 3 ist noch gut zu wissen. Alles danach sollten Sie auch lesen, aber machen Sie sich keine Sorgen, wenn Sie dabei nicht mehr ganz mitkommen. Fragen dürfen Sie natürlich trotzdem immer!\n\n9.4.1 Welche Voraussetzungen haben Regressionen?\nSchauen wir uns abschließend an, welche Voraussetzungen die Regressionsanalyse hat. Vorweg: Dieser Teil kann etwas abschreckend sein, aber die gute Nachricht ist, dass es in vielen Fällen reicht, die Voraussetzungen im Hinterkopf zu haben. Und noch ein Disclaimer: Dieser Teil basiert in weiten Teilen auf Kapitel 11.1 auf einem Lehrbuch von Gelman, Hill und Vehtari, das online verfügbar ist.\nDie zwei wichtigsten Voraussetzungen haben relativ wenig mit der eigentlichen Statistik zu tun:\n\nDie Daten müssen valide sein. Der Begriff ist Ihnen wahrscheinlich schon aus dem Studium bekannt. Wir verstehen darunter die Frage, ob die Daten (bzw. die Datenerhebung) wirklich das Messen, was sie messen sollen. Im Kontext von Regressionen (und den anderen Verfahren, die wir noch kennenlernen werden!) bedeutet es aber auch, dass unser Modell als Ganzes eine gute Repräsentation des zu untersuchenden Sachverhalts sein sollte. Ganz konkret sollten z.B. alle relevanten Prädiktoren berücksichtigt werden. Das ist in der Praxis gar nicht immer so einfach, allein schon deshalb, weil oftmals vor der Datenerhebung gar nicht klar ist, welche Konstrukte relevant sein könnten. Aber es ist immer gut, diese Frage im Hinterkopf zu haben und ggf. bei der Interpretation der Ergebnisse zu berücksichtigen.\nImmer, wenn wir eine Regression berechnen, versuchen wir basierend auf Daten einer Stichprobe Schlüsse über eine Grundgesamtheit zu ziehen. Damit diese Schlüsse zulässig sind, müssen unsere Daten repräsentativ sein. Wichtig ist, dass für die Regression nicht die Stichprobe selbst ein repräsentatives Abbild der Grundgesamtheit sein muss, sondern vielmehr die gemessene Verteilung der abhängigen Variable, die in Abhängigkeit der aller unabhänggen Variablen der Grundgesamtheit entsprechen muss. Wenn wir beispielsweise die Größe von Befragten durch ihr Geschlecht und ihre Ernährungsgewohnheiten während der Kindheit erklären wollen, wäre es völlig in Ordnung, wenn Frauen oder vegan ernährte Kinder überrepräsentiert wären. Allerdings hätten wir ein Problem, wenn überdurchschnittlich viele große Menschen in unserer Stichprobe enthalten wären. Ein Problem, das wir in diesem Kontext immer mal wieder haben ist, dass es schwer bis unmöglich sein kann, die Grundgesamtheit überhaupt zu bestimmen. Das ist z.B. immer dann der Fall, wenn Sie Ihre Stichprobe über persönliche Kontakte und/oder soziale Medien rekrutieren. Allen Ergebnissen, die Sie berechnen, liegt dann die Annahme zugrunde, dass die gemessenen Daten auf die Grundgesamtheit, an der Sie interessiert sind, übertragbar sind.\n\nDaneben gibt es einige Voraussetzungen, die auch im strengeren Sinne statistischer Natur sind.\n\nRegressionsmodelle müssen linear sein. Darüber haben wir bereits im Kontext von Korrelationen gesprochen. Sollten Sie z.B. einmal einen solchen Zusammenhang sehen, können Sie relativ leicht erkennen, dass es sich nicht um einen linearen Zusammenhang handelt. In diesem Fall würden wir eher von einem quadratischen Zusammenhang sprechen. In solchen Fällen haben Sie zwei Möglichkeiten: Entweder Sie transformieren eine der Variablen (üblicherweise die unabhängige Variable) oder Sie wählen ein anderes Verfahren (das Sie in dieser Veranstaltung allerdings nicht lernen).\n\n\n\n\n\n\n\n\n\n\n\nDie Effekte in Regressionsmodellen müssen additiv sein. Das heißt, wir gehen davon aus, dass die abhängige Variable am besten dadurch erklärt werden kann, dass wir die unabhängigen Variablen addieren, also nach dem Schema: y = x + z. Das ist eine durchaus starke Annahme. Genauso gut wäre es schließlich denkbar, dass die abhängige Variable das Ergebnis des Produkts von zwei Variablen ist, also: y = x*z. In der Realität kann das schwer zu erkennen sein, aber zum Glück können wir es relativ leicht berücksichtigen. Schließlich haben wir oben festgestellt, dass eine Interaktion nichts anderes ist, als das Produkt von zwei Prädiktoren.\nDie Schätzfehler (oder Residuen) müssen unabhängig voneinander sein. Das bedeutet, dass die Abweichung eines gemessenen Wertes vom in der Regression geschätzten Wert in einem Fall (also z.B. einem ausgefüllten Befragungsbogen) keinen Einfluss auf die Abweichung in einem anderen Fall (also einem anderen ausgefüllten Fragebogen) haben darf. In den allermeisten Fällen können Sie davon ausgehen, dass diese Voraussetzung erfüllt ist. In anderen Fällen ist es dagegen sehr offensichtlich, dass dies nicht der Fall ist, z.B. wenn Sie dieselben Personen immer wieder befragen und davon ausgehen müssen, dass die Antworten aus der ersten Befragungswelle und die Antworten aus der zweiten Welle nicht unabhängig voneinander sind. In anderen Fällen entsteht eine Abhängigkeit durch das Erhebungssetting. Wenn Sie beispielsweise mehrere Schulklassen untersuchen, ist es gut möglich, dass die Daten aus den jeweiligen Klassen nicht unabhängig voneinander sind, etwa weil die zuständige Lehrkraft einen Einfluss ausübt. In solchen Fällen sollten Sie keine Regression anwenden.\nDie Schätzfehler sollten eine gleichmäßige Streuung aufweisen. Das bedeutet, dass die durchschnittliche Abweichung der gemessenen Werte von den im Modell geschätzten Werten unabhängig davon sein sollte, welchen Wert ein Prädiktor hat. Im Bild unten sehen Sie, wie das aussehen könnte: Je kleiner die Werte von x, desto näher liegen sie, im Durchschnitt, an der Linie. Die höheren Werte sind dagegen deutlich weiter um die Linie herum gestreut. Man spricht in so einem Fall auch von Heteroskedastizität. Die gute Nachricht ist, dass Sie sich nicht wirklich um Heteroskedastizität sorgen müssen, sofern Sie nur daran interessiert sind, gemessene Daten zu erklären und nicht das Ziel haben, Werte zu prognostizieren bzw. vorherzusagen.\n\n\n\n\n\n\n\n\n\n\n\nDie Schätzfehler sollten normalverteilt sein. In der Ausgabe der summary()-Funktion haben wir im letzten Kapitel kurz über die Zusammenfassung der Residuen gesprochen. Wie Sie dort sehen, werden Minimal- und Maximalwerte sowie Quartile und der Median dort abgebildet. Was dort leider nicht steht, sind Mittelwert und Standardabweichung der Schätzfehler, die wir aber händisch berechnen könnten (aber zum Glück nur sehr selten müssen!). Was wir aber grundsätzlich aus der Angabe lernen ist, dass diese Schätzfehler irgendwie verteilt sind. Und diese Verteilung sollte normal sein, also durch eine Glockenkurve beschrieben werden können. Diese Voraussetzung wird in manchen Lehrbüchern und Online-Ressourcen fälschlicherweise angegeben als Normalverteilung der abhängigen Variable. Das ist nicht der Fall! Die Regression stellt keinerlei Anforderungen daran, wie diese Variable verteilt sein muss. Und es wird noch besser: Wie im Fall der Heteroskedastizität ist diese Voraussetzung nicht wirklich relevant, wenn Sie nur erhobene Daten erklären wollen und keine Vorhersagen basierend auf Ihrem Modell treffen wollen!\nDie unabhängigen Variablen sollten nicht (zu stark) korrelieren. Man spricht hier auch von einer möglichst geringen Kollinearität oder auch Multikollinearität. Liegt diese vor, also korrelieren die Prädiktoren stark miteinander, ist die Schätzung der Effektstärken mit mehr Unsicherheit verbunden, d.h., die Standardfehler und darauf basierend die Konfidenzintervalle werden größer. Das ergibt auch irgendwie Sinn, wenn man mal darüber nachdenkt: Eine Korrelation bedeutet am Ende nichts anderes, als dass zwei Variablen dieselben Informationen enthalten. Je mehr Sie über die eine wissen, desto mehr wissen Sie bei einer starken Korrelation auch über die andere. Wenn Sie nun hergehen wollen und basierend auf zwei stark korrelierenden Variablen eine abhängige Variable erklären wollen, gibt es schlicht keine Möglichkeit, mit Sicherheit zu sagen, welche der beiden Variablen für einen etwaigen Effekt verantwortlich ist. Die Schätzung enthält also viel Unsicherheit. Das muss aber nicht unbedingt ein Problem sein, sondern ist ein völlig legitimes Forschungsergebnis! Im Spezialfall von Interaktionseffekten würden wir sogar mit einer hohen Kollinearität rechnen und brauchen uns darum keine Sorgen zu machen. Nur in sehr seltenen Fällen kann es vorkommen, dass eine Regression aufgrund sehr starker Multikollinearität nicht geschätzt werden kann. In solchen Fällen müssten Sie dann eine der Variablen aus dem Modell ausschließen.\n\n\n\n9.4.2 Überprüfen der Voraussetzungen\nWie oben angedeutet, reicht es in den allermeisten Fällen, die Voraussetzungen im Hinterkopf zu haben. Falls Sie doch einmal in die Situation geraten, einzelne Voraussetzungen überprüfen zu wollen, können Sie dafür das performance-Paket nutzen.\n\n# Versucht das Paket \"performance\" zu laden. Falls es nicht installiert ist, wird es erst installiert und dann geladen\nif(!require(performance)){\n  install.packages(\"performance\")\n  library(performance)\n}\n\nLade nötiges Paket: performance\n\n\nWarning: Paket 'performance' wurde unter R Version 4.4.3 erstellt\n\n\nDieses Paket enthält eine Vielzahl von Funktionen, mit denen Sie Regressionsmodelle überprüfen können. Das Schema ist dabei immer gleich: Sie müssen der entsprechenden Funktion lediglich das berechnete Modell übergeben. Oftmals bietet es sich an, dabei visuell vorzugehen und die Funktionen innerhalb von plot() aufzurufen. Hier einige Beispiele. Fangen wir mit der Heteroskedastizität an:\n\n# Überprüft, ob Heteroskedastizität vorliegt und stellt das Ergebnis visuell dar\n\nplot(check_heteroscedasticity(lmInteraktion))\n\n\n\n\n\n\n\n\nHier sehen wir die Streuung der Schätzfehler. Freundlicherweise sagt uns die Funktion, wie das Ergebnis aussehen sollte. So erkennen wir gleich, dass wir die Voraussetzung nicht erfüllen. Aber da wir keine Vorhersagen treffen wollen, ist das nicht so schlimm.\nWeiter geht es mit der Normalverteilung der Residuen:\n\n# Überprüft, ob die Residuen normalverteilt sind und stellt das Ergebnis dar\n\nplot(check_normality(lmInteraktion))\n\nFor confidence bands, please install `qqplotr`.\n\n\n\n\n\n\n\n\n\nZum selben Resultat gelangen wir hier. Die Funktion sagt uns, dass die Punkte der Linie folgen sollten, was allerdings nicht der Fall ist. Aber da wir keine Vorhersagen treffen wollen, ist auch das kein Problem.\nWerfen wir abschließend einen Blick auf die (Multi-)Kollinearität:\n\n# Überprüft, ob Kollinearität vorliegt und stellt das Ergebnis visuell dar\nplot(check_collinearity(lmInteraktion))\n\nModel has interaction terms. VIFs might be inflated.\n  Try to center the variables used for the interaction, or check\n  multicollinearity among predictors of a model without interaction terms.\n\n\n\n\n\n\n\n\n\nHier wird der sogenannte Variance Inflation Faktor, kurz VIF, abgebildet. Er gibt an, wie stark die Korrelation zwischen den Prädiktoren ist. Werte über 10 gelten als problematisch (aber siehe oben!). Das ist hier für das Alter und die Interaktion aus Alter und Geschlecht der Fall. Nur: Es ist ja vollkommen logisch, dass die beiden Variablen stark korrelieren, denn die Interaktion ist nichts anderes als Alter*Geschlecht (wobei männlich = 0 und weiblich = 1)! Also alles in bester Ordnung.\nSie können auch die check_model()-Funktion verwenden, in der die drei Tests oben plus einige weitere durchgeführt und dargestellt werden. Das Ergebnis wird aber schnell unübersichtlich, daher machen wir es hier nicht.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regressionen II</span>"
    ]
  },
  {
    "objectID": "t-Test.html",
    "href": "t-Test.html",
    "title": "10  t-Test",
    "section": "",
    "text": "10.1 Vorbereitung\nAuch für den t-Test benötigen wir das ‘tidyverse’ und das Paket ‘effectsize’. Außerdem lesen wir den Datensatz ein und ändern die Optionen so, dass kleine Zahlen in einem uns gewohnten Format angezeigt werden.\n# Lädt das tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Lädt effectsize\nlibrary(effectsize)\n\nWarning: Paket 'effectsize' wurde unter R Version 4.4.3 erstellt\n\n# Liest die Daten ein\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Stellt ein, dass sehr kleine Zahlen normal dargestellt werden\noptions(scipen = 999)\nAußerdem werden wir in diesem Kapitel mit den Spalten A501_01 bis A501_04 arbeiten. In den entsprechenden Fragen sollten die Befragten angeben, wie stark ihre Bindung zu ihrem Wohnort ist (z.B. “Ich fühle mich als Teil meines Wohnorts”), wobei der Wert 1 für geringe Zustimmung zu den Aussagen steht und der Wert 5 für hohe Zustimmung.\nDa diese vier Variablen alle dasselbe Konstrukt abfragen, berechnen wir zunächst pro Person einen Mittelwert aus den vier Spalten. Dazu nutzen wir zunächst die Funktion rowwise() aus dem dplyr-Paket. Damit sagen wir R, dass die nachfolgenden Zeilen jeweils für jede einzelne Zeile im Datensatz ausgeführt werden sollen. Anschließend nutzen wir mutate() und darin mean(), um den Mittelwert zu berechnen. Wichtig ist, dass wir abschließend die ungroup()-Funktion nutzen, da R ansonsten versucht, auch den nachfolgenden Code, der sich auf den Datensatz bezieht, versucht pro Zeile durchzuführen.\n# berechnet pro Person einen Mittelwert der Spalten A501_01 bis A501_04\ndf_lokal &lt;- df_lokal |&gt;\n  rowwise() |&gt;\n  mutate(lokaleBindung = mean(c(A501_01, A501_02, A501_03, A501_04), na.rm = TRUE)) |&gt;\n  ungroup()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>t-Test</span>"
    ]
  },
  {
    "objectID": "t-Test.html#t-test-durchführen",
    "href": "t-Test.html#t-test-durchführen",
    "title": "10  t-Test",
    "section": "10.2 t-Test durchführen",
    "text": "10.2 t-Test durchführen\nUm einen t-Test durchzuführen benötigen wir die t.test()-Funktion. Damit können wir alle drei Varianten des t-Tests rechnen, die wir im Video kennengelernt haben.\n\n10.2.1 Einstichproben t-Test\nWie im Video besprochen berechnen wir einen t-Test für eine Stichprobe immer dann, wenn wir wissen möchten, ob sich der von uns gemessene Mittelwert von einem a priori definierten Wert unterscheidet. Wenn wir z.B. wüssten, dass die Menschen in Thüringen und Rheinland-Pfalz sich im Durchschnitt nicht sehr an ihren Wohnort gebunden fühlen (also z.B. der Wert 2 dem Mittelwert der Grundgesamtheit entspricht), könnten wir mit dieser Variante des t-Tests prüfen, ob das auch auf unsere Stichprobe zutrifft.\nUm diesen Test durchzuführen, übergeben wir der t.test()-Funktion zunächst die Daten. Hier also die Spalte lokaleBindung. Wichtig ist, dass wir in diesem Beispiel keine Pipe (|&gt;) nutzen und anders als bei der Regression sagen wir der t.test()-Funktion auch nicht, welchen Datensatz wir benutzen. Wir müssen also mit Hilfe des Dollarzeichens erst den Datensatz und dann die Spalte angeben, so wie wir es in Kapitel 2 kennengelernt haben. Außerdem nutzen wir das Argument mu. Damit können wir den Wert angeben, gegen den wir unseren Stichprobenmittelwert testen wollen.\n\n# berechnet einen einstichproben t-Test\ntTest1sample &lt;- t.test(df_lokal$lokaleBindung, mu = 2)\n\n# zeigt die Ergebnisse an\ntTest1sample\n\n\n    One Sample t-test\n\ndata:  df_lokal$lokaleBindung\nt = 83.155, df = 1840, p-value &lt; 0.00000000000000022\nalternative hypothesis: true mean is not equal to 2\n95 percent confidence interval:\n 3.889405 3.980683\nsample estimates:\nmean of x \n 3.935044 \n\n\nSchauen wir uns das Ergebnis an. Oben werden uns der t-Wert, also die Teststatistik, die Freiheitsgrade (df) und der p-Wert des Tests angezeigt. Darunter erinnert uns R daran, welche Hypothese wir getestet haben. Darunter befindet sich wiederum ein 95%-Konfidenzintervall. Die Darstellung ist hier etwas unglücklich, denn erst danach, also ganz unten, steht der Stichprobenmittelwert, auf den sich das Konfidenzintervall bezieht.\nWir halten fest: Wir haben die Alternativhypothese getestet, dass sich der Stichprobenmittelwert vom Wert 2 unterscheidet. Das Ergebnis lautet: t(1840) = 83,155; p &lt; 0,001. Das bedeutet, dass wir die Nullhypothese (Stichprobenmittelwert = 2) verwerfen müssen und die Alternativhypothese annehmen können.\n\n\n10.2.2 t-Test für gepaarte Stichproben\nDer t-Test für gepaarte Stichproben funktioniert im Grunde sehr ähnlich. Statt das mu-Argument zu nutzen, übergeben wir diesmal aber zwei Spalten aus einem Datensatz. Dazu nutzen wir das Argument paired = TRUE. Da der Datensatz, mit dem wir arbeiten eine einfache Querschnittsbefragung darstellt, in dem es keine Paare gibt, können wir leider kein Beispiel rechnen. Wie der Code aussehen würde, sehen Sie aber unten:\n\n#### BEISPIELCODE, DER NICHT AUSGEFÜHRT WERDEN KANN! ####\n# tTestPaare &lt;- t.test(df$Spalte1, df$Spalte2, paired = TRUE)\n\n\n\n10.2.3 t-Test für unabhängige Stichproben\nDer t-Test für unabhängige Stichproben sieht der Regression sehr ähnlich. Das heißt, wir geben auch hier eine “Formel” ein, bei der die abhängige Variable links von einer Tilde (~) steht und die unabhängige Variable rechts davon. Für das Beispiel nehmen wir wieder die lokale Bindung (df_lokal$lokaleBindung) als AV und dazu das Bundesland, aus dem die Befragten stammen, (df_lokal$Bula) als UV.\n\n# berechnet einen t-Test für unabhängige Stichproben\ntTestIndSample &lt;- t.test(df_lokal$lokaleBindung ~ df_lokal$Bula)\n\n# zeigt das Ergebnis an\ntTestIndSample\n\n\n    Welch Two Sample t-test\n\ndata:  df_lokal$lokaleBindung by df_lokal$Bula\nt = -3.2835, df = 1767.9, p-value = 0.001045\nalternative hypothesis: true difference in means between group RLP and group TH is not equal to 0\n95 percent confidence interval:\n -0.2439230 -0.0614935\nsample estimates:\nmean in group RLP  mean in group TH \n         3.868022          4.020730 \n\n\nSchauen wir uns auch hier die Ergebnisse an. Oben stehen wieder die Teststatistik t sowie die dazugehörigen Freiheitsgrade und der p-Wert. Hier lautet unser Ergebnis also: t(1767,9) = -3,28; p = 0,001.\nWeiter unten sehen wir dann wieder ein Konfidenzintervall. Es bezieht sich hier auf die Differenz zwischen den beiden Gruppenmittelwerten. Diese stehen dann direkt darunter. Wie wir sehen, fühlen sich die Menschen aus Rheinland-Pfalz etwas weniger lokal zugehörig (M = 3.87) als die Menschen aus Thüringen (M = 4,02).\nFür diesen Test berechnen wir nun Cohens d. Für die beiden Tests oben geht das grundsätzlich aber auch. Wir nutzen dafür die cohens_d()-Funktion aus dem Paket effectsize, der wir das Objekt übergeben, in dem wir das Ergebnis des Tests gespeichert haben (hier also tTestIndSample). Außerdem übergeben wir der Funktion das Argument pooled_sd = FALSE. Darauf werden wir gleich noch mal kurz eingehen.\n\ncohens_d(tTestIndSample, pooled_sd = FALSE)\n\nCohen's d |         95% CI\n--------------------------\n-0.15     | [-0.25, -0.06]\n\n- Estimated using un-pooled SD.\n\n\nCohens d wird hier mit dem Wert -0,15 ausgegeben. D.h., der Mittelwert der Menschen aus Rheinland-Pfalz liegt um 0,15 Standardabweichungen der AV unter dem Wert der Befragten aus Thüringen.\nUm das Ergebnis vollständig verschriftlichen zu können, benötigen wir noch die Mittelwerte und Standardabweichungen der beiden Gruppen. Das machen wir wie gewohnt mit Hilfe von summarise(). Damit wir die Werte für jede Gruppe erhalten, nutzen wir vorher noch group_by() und geben dort die Spalte Bula an.\n\n# berechnet Mittelwerte und Standardabweichungen für die beiden Gruppen\nMWsSDs &lt;- df_lokal |&gt;\n  group_by(Bula) |&gt;\n  summarise(MW = round(mean(lokaleBindung, na.rm = TRUE),2),\n            SD = round(sd(lokaleBindung, na.rm = TRUE),2))\n\n# zeigt die Werte an\nMWsSDs\n\n# A tibble: 2 × 3\n  Bula     MW    SD\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 RLP    3.87  1.02\n2 TH     4.02  0.97\n\n\nNun können wir das Ergebnis wie folgt aufschreiben: &gt; Durch einen t-Test wurde geprüft, ob sich die Bindung an den Wohnort von Menschen in Rheinland-Pfalz (M = 3,87; SD = 1,02) von der Bindung der Befragten in Thüringen (M = 4,02; SD = 0,97) unterscheidet. Die beiden Gruppen unterscheiden sich signifikant voneinander (t(1767,9) = -3,28; p = 0,001). Die Thüringer fühlen sich signifikant stärker an ihren Ort gebunden, aber dieser Effekt ist schwach (Cohens d = -0,15).\n\n\n\n\n\n\nVarianzhomogenität beim t-Test\n\n\n\nAm Ende des letzten Kapitels haben wir die Voraussetzungen von Regressionen kennengelernt. Dort hieß es unter anderem, dass die Streuung der Schätzfehler gleichmäßig sein muss. Ist das nicht der Fall, sprechen wir von Heteroskedastizität. Ist die Voraussetzung dagegen erfüllt, von Homoskedastizität. Im Video in diesem Kapitel haben Sie dann erfahren, dass der t-Test eigentlich nur eine spezielle Form der Regression ist und daher dieselben Voraussetzungen teilt. Im Kontext von t-Tests sprechen wir aber in der Regel von Varianzhomogenität (statt Homo- oder Heteroskedastizität).\nDer t-Test wird in R standardmäßig in einer Variante durchgeführt, die keine gleichen Varianzen voraussetzt. Das ist einerseits sinnvoll, da es immer gut ist, einen Test zu haben, der nominell weniger Voraussetzungen hat. Andererseits liegt hierin auch der Grund, dass wir in unserem Aufruf von cohens_d() das Argument pooled_sd = FALSE angeben mussten. Denn die cohens_d()-Funktion geht davon aus, dass die Version des t-Tests gerechnet wurde, die von Varianzhomogenität ausgeht.\nAlternativ könnten Sie auch beim Aufruf der t.test()-Funktion das Argument var.equal = TRUE angeben und könnten im Gegenzug pooled_sd = FALSE beim Aufruf von cohens_d() weglassen.\nIn dieser Variante können wir übrigens schön sehen, dass der t-Test eigentlich eine Regression ist. Wenn Sie die Ergebnisse des Codes unten aufmerksam vergleichen, werden Sie feststellen, dass die t- und p-Werte des t-Tests und des entsprechenden Koeffizienten in der Regression quasi identisch sind. Gleiches gilt für die berechnete Effektstärke von standardize_parameters() und cohens_d().\n\n# berechnet einen t-Test mit angenommener Varianzhomogenität\ntTest &lt;- t.test(df_lokal$lokaleBindung ~ df_lokal$Bula, var.equal = TRUE)\n\n# zeigt das Ergebnis an\ntTest\n\n\n    Two Sample t-test\n\ndata:  df_lokal$lokaleBindung by df_lokal$Bula\nt = -3.2651, df = 1839, p-value = 0.001114\nalternative hypothesis: true difference in means between group RLP and group TH is not equal to 0\n95 percent confidence interval:\n -0.24443519 -0.06098132\nsample estimates:\nmean in group RLP  mean in group TH \n         3.868022          4.020730 \n\n# berechnet dasselbe Modell als Regression\nregression &lt;- lm(lokaleBindung ~ Bula, data = df_lokal)\n\n# zeigt die Ergebnisse an\nsummary(regression)\n\n\nCall:\nlm(formula = lokaleBindung ~ Bula, data = df_lokal)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0207 -0.6180  0.1320  0.9793  1.1320 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.86802    0.03098 124.838 &lt; 0.0000000000000002 ***\nBulaTH       0.15271    0.04677   3.265              0.00111 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9958 on 1839 degrees of freedom\n  (5 Beobachtungen als fehlend gelöscht)\nMultiple R-squared:  0.005764,  Adjusted R-squared:  0.005223 \nF-statistic: 10.66 on 1 and 1839 DF,  p-value: 0.001114\n\n\n\n# berechnet Cohens d und zeigt es an\ncohens_d(tTest)\n\nCohen's d |         95% CI\n--------------------------\n-0.15     | [-0.25, -0.06]\n\n- Estimated using pooled SD.\n\n# berechnet standardisierte Effektstärken für die Regression und zeigt sie an\nstandardize_parameters(regression)\n\n# Standardization method: refit\n\nParameter   | Std. Coef. |         95% CI\n-----------------------------------------\n(Intercept) |      -0.07 | [-0.13, -0.01]\nBula [TH]   |       0.15 | [ 0.06,  0.24]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>t-Test</span>"
    ]
  },
  {
    "objectID": "t-Test.html#ergebnis-visualisieren",
    "href": "t-Test.html#ergebnis-visualisieren",
    "title": "10  t-Test",
    "section": "10.3 Ergebnis visualisieren",
    "text": "10.3 Ergebnis visualisieren\nIm bisherigen Lauf der Veranstaltung haben wir schon einige Möglichkeiten kennengelernt, um Mittelwerte bzw. die dazugehörige Verteilung zu visualisieren. Für das Beispiel des t-Tests für unabhängige Stichproben, den wir oben berechnet haben, lernen Sie nun eine weitere Visualisierung kennen. Wir starten zunächst mit einem Histogramm, das die beiden Gruppen berücksichtigt. Wir starten mit einer sehr einfachen Variante und verfeinern sie nach und nach. Das Endresultat steht dann ganz unten.\nIm ersten Schritt filtern wir fehlende Werte in der Spalte lokaleBindung aus und übergeben unseren Datensatz an ggplot(). Dort legen wir fest, dass die Spalte lokaleBindung auf der x-Achse dargestellt werden soll. Beim Histogramm wird auf der y-Achse automatisch die Häufigkeit dargestellt. Um die Gruppen abzubilden nutzen wir zusätzlich das Argument fill = Bula. Mit geom_histogram() erstellen wir dann das Histogramm. Dabei geben wir bins = 17 an. Unter bins verstehen wir, in wie viele Bereiche die Daten eingeteilt werden sollen. D.h., bei einem Histogramm werden nicht wie bei einem Balkendiagramm alle Werte einzeln abgebildet, sondern kleine Gruppen gebildet. Hier nehmen wir den Wert 17, weil er a) die tatsächlich beobachteten Werte gut abbildet und b) weil das Resultat besser aussieht als der Standardwert 30 oder andere Werte. Abschließend machen wir den Plot mit theme_minimal() direkt noch etwas schöner.\n\n# erstellt ein Histogramm der Spalte \"lokaleBindung\" nach Gruppe (RLP vs. TH)\nhistBindung &lt;- df_lokal |&gt;\n  filter(!is.na(lokaleBindung)) |&gt;\n  ggplot(aes(x = lokaleBindung, fill = Bula))+\n  geom_histogram(bins = 17)+\n  theme_minimal()\n\n# zeigt das Histogramm an\nhistBindung\n\n\n\n\n\n\n\n\nDas Resultat sieht nicht schrecklich aus, aber kann noch deutlich verbessert werden. Dass die Farben nicht sonderlich schön sind, ist das eine, aber vor allem handelt es sich hierbei um ein gestapeltes Histogramm. D.h., die Anzahl, die auf der y-Achse dargestellt wird, ist die Gesamtanzahl der entsprechenden Werte in beiden Gruppen zusammen.\nDas können wir beheben, indem wir geom_histogram() das Argument position = \"identity\" übergeben. So werden die Balken des Histogramms übereinandergelegt. Außerdem passen wir die Farben an. Dafür nutzen wir wie schon in anderen Beispielen die scale_fill_manual()-Funktion und übergeben ihr im Argument values zwei Farben (also eine pro Gruppe).\n\n# erstellt ein Histogramm der Spalte \"lokaleBindung\" nach Gruppe (RLP vs. TH)\nhistBindung &lt;- df_lokal |&gt;\n  filter(!is.na(lokaleBindung)) |&gt;\n  ggplot(aes(x = lokaleBindung, fill = Bula))+\n  geom_histogram(bins = 17, position = \"identity\")+\n  theme_minimal()+\n  scale_fill_manual(values=c(\"red\", \"orange\"))\n\n# zeigt das Histogramm an\nhistBindung\n\n\n\n\n\n\n\n\nDas sieht schon besser aus! Da die Balken nun hintereinander liegen und die Thüringer im Vordergrund dargestellt werden, wird der Balken für Rheinland-Pfalz an einigen Stellen verdeckt. Um das zu beheben, können wir geom_histogram() das Argument alpha übergeben. Damit machen wir die Balken etwas transparent. Der Wert muss immer zwischen 0 und 1 liegen. Im Beispiel unten wurde 0,6 gewählt. Aber das ist eine Frage der Präferenz!\nEin etwas weniger offensichtliches Problem gibt es allerdings noch. Und zwar, dass die beiden Gruppen unterschiedlich groß sind. Oben im Test haben wir erfahren, dass die Thüringer einen etwas höheren Mittelwert haben als die Menschen aus Rheinland-Pfalz. Allerdings sehen wir in diesem Diagramm, dass in fast alle Wertebereichen mehr Menschen aus Rheinland-Pfalz als aus Thüringen fallen. In anderen Worten: die unterschiedlichen Fallzahlen machen es nahezu unmöglich, den höheren Mittelwert der Thüringer optisch zu erahnen. Wir können es beheben, indem wir die Darstellung auf der y-Achse von absoluten Häufigkeiten zur sogenannten Wahrscheinlichkeitsdichte ändern. Das machen wir, indem wir geom_histogram() die aes()-Funktion übergeben und dort y = after_stat(density) angeben. Dadurch werden die beiden Gruppen vergleichbar. Stark vereinfacht können wir sagen, dass die Wahrscheinlichkeitsdichte angibt, wie wahrscheinlich es ist, dass ein zufälliger Wert aus unserer Stichprobe in einem bestimmten Wertebereich liegt.\nDa wir nun die Bedeutung der Achse geändert haben, sollten wir auch die Beschriftung der Achsen anpassen. Dazu nutzen wir wieder die labs()-Funktion.\n\n# erstellt ein Histogramm der Spalte \"lokaleBindung\" nach Gruppe (RLP vs. TH)\nhistBindung &lt;- df_lokal |&gt;\n  filter(!is.na(lokaleBindung)) |&gt;\n  ggplot(aes(x = lokaleBindung, fill = Bula))+\n  geom_histogram(bins = 17, position = \"identity\", alpha = 0.6, aes(y = after_stat(density)))+\n  theme_minimal()+\n  scale_fill_manual(values=c(\"red\", \"orange\"))+\n  labs(x = \"Bindung an den Wohnort\", y = \"Wahrscheinlichkeitsdichte\")\n\n# zeigt das Histogramm an\nhistBindung\n\n\n\n\n\n\n\n\nEine Besonderheit der Wahrscheinlichkeitsdichte ist, dass wir sie auch kontinuierlich darstellen können. Im Prinzip wird dazu auf Basis unserer Beobachtungen eine Funktion geschätzt, die die Verteilung einer kontinuierlichen, metrischen Variable abbildet. Dazu können wir geom_histogram() durch geom_density() ersetzen. Wir nutzen wieder das Argument alpha, um die Darstellung leicht transparent zu machen.\n\n# erstellt einen Densityplot der Spalte \"lokaleBindung\" nach Gruppe (RLP vs. TH)\ndensityBindung &lt;- df_lokal |&gt;\n  filter(!is.na(lokaleBindung)) |&gt;\n  ggplot(aes(x = lokaleBindung, fill = Bula))+\n  geom_density(alpha = .6)+\n  theme_minimal()+\n  scale_fill_manual(values=c(\"red\", \"orange\"))+\n  labs(x = \"Bindung an den Wohnort\", y = \"Wahrscheinlichkeitsdichte\")\n\n# zeigt den Densityplot an\ndensityBindung",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>t-Test</span>"
    ]
  },
  {
    "objectID": "ANOVA.html",
    "href": "ANOVA.html",
    "title": "11  Varianzanalyse",
    "section": "",
    "text": "11.1 Vorbereitung\nAuch für die Varianzanalyse benötigen wir das tidyverse und das Paket effectsize. Außerdem lesen wir den Datensatz ein und ändern die Optionen so, dass kleine Zahlen in einem uns gewohnten Format angezeigt werden.\n# Lädt das tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Lädt effectsize\nlibrary(effectsize)\n\nWarning: Paket 'effectsize' wurde unter R Version 4.4.3 erstellt\n\n# Liest die Daten ein\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Stellt ein, dass sehr kleine Zahlen normal dargestellt werden\noptions(scipen = 999)\nZusätzlich benötigen wir zwei neue Pakete namens car und emmeans, die wir installieren und laden müssen.\n# prüft, ob die Pakete \"car\" und \"emmeans\" installiert sind; falls nein, werden sie installiert und geladen\n\nif(!require(car)){\n  install.packages(\"car\")\n  library(car)\n}\n\nLade nötiges Paket: car\n\n\nLade nötiges Paket: carData\n\n\n\nAttache Paket: 'car'\n\n\nDas folgende Objekt ist maskiert 'package:dplyr':\n\n    recode\n\n\nDas folgende Objekt ist maskiert 'package:purrr':\n\n    some\n\nif(!require(emmeans)){\n  install.packages(\"emmeans\")\n  library(emmeans)\n}\n\nLade nötiges Paket: emmeans\n\n\nWelcome to emmeans.\nCaution: You lose important information if you filter this package's results.\nSee '? untidy'\nWie im letzten Kapitel werden wir mit den Spalten A501_01 bis A501_04 arbeiten, die die Bindung der Befragten an den Wohnort angeben. Wir bilden also wieder einen Mittelwertindex.\n# berechnet pro Person einen Mittelwert der Spalten A501_01 bis A501_04\ndf_lokal &lt;- df_lokal |&gt;\n  rowwise() |&gt;\n  mutate(lokaleBindung = mean(c(A501_01, A501_02, A501_03, A501_04), na.rm = TRUE)) |&gt;\n  ungroup()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "ANOVA.html#t-test-durchführen",
    "href": "ANOVA.html#t-test-durchführen",
    "title": "11  Varianzanalyse",
    "section": "11.2 t-Test durchführen",
    "text": "11.2 t-Test durchführen\nUm einen t-Test durchzuführen benötigen wir die t.test()-Funktion. Damit können wir alle drei Varianten des t-Tests rechnen, die wir im Video kennengelernt haben.\n\n11.2.1 Einstichproben t-Test\nWie im Video besprochen berechnen wir einen t-Test für eine Stichprobe immer dann, wenn wir wissen möchten, ob sich der von uns gemessene Mittelwert von einem a priori definierten Wert unterscheidet. Wenn wir z.B. wüssten, dass die Menschen in Thüringen und Rheinland-Pfalz sich im Durchschnitt nicht sehr an ihren Wohnort gebunden fühlen (also z.B. der Wert 2 dem Mittelwert der Grundgesamtheit entspricht), könnten wir mit dieser Variante des t-Tests prüfe, ob das auch auf unsere Stichprobe zutrifft.\nUm diesen Test durchzuführen, übergeben wir der t.test()-Funktion zunächst die Daten. Hier also die Spalte engagement. Wichtig ist, dass wir in diesem Beispiel keine Pipe (|&gt;) nutzen und anders als bei der Regression sagen wir der t.test()-Funktion auch nicht, welchen Datensatz wir benutzen. Wir müssen also mit Hilfe des Dollarzeiens erst den Datensatz und dann die Spalte angeben, so wie wir es in Kapitel 2 kennengelernt haben. Außerdem nutzen wir das Argument mu. Damit können wir den Wert angeben, gegen den wir unseren Stichprobenmittelwert testen wollen.\n\n# berechnet einen einstichproben t-Test\ntTest1sample &lt;- t.test(df_lokal$lokaleBindung, mu = 2)\n\n# zeigt die Ergebnisse an\ntTest1sample\n\n\n    One Sample t-test\n\ndata:  df_lokal$lokaleBindung\nt = 83.155, df = 1840, p-value &lt; 0.00000000000000022\nalternative hypothesis: true mean is not equal to 2\n95 percent confidence interval:\n 3.889405 3.980683\nsample estimates:\nmean of x \n 3.935044 \n\n\nSchauen wir uns das Ergebnis an. Oben werden uns der t-Wert, also die Teststatistik, die Freiheitsgrade (df) und der p-Wert des Tests angezeigt. Darunter erinnert uns R daran, welche Hypothese wir getestet haben. Darunter befindet sich wiederum ein 95%-Konfidenzintervall. Die Darstellung ist hier etwas unglücklich denn erst danach, also ganz unten, steht der Stichprobenmittelwert auf den sich das Konfidenzintervall bezieht.\nWir halten fest: Wir haben die Alternativhypothese getestet, dass sich der Stichprobenmittelwert vom Wert 2 unterscheidet. Das Ergebnis lautet: t(1840) = 83,155; p &lt; 0,001. Das bedeutet, dass wir die Nullhypothese (Stichprobenmittelwert = 2) verwerfen müssen und die Alternativhypothese annehmen können.\n\n\n11.2.2 t-Test für gepaarte Stichproben\nDer t-Test für gepaarte Stichproben funktioniert im Grunde sehr ähnlich. Statt das mu-Argument zu nutzen, übergeben wir diesmal aber zwei Spalten aus einem Datensatz. Dazu nutzen wir das Argument paired = TRUE. Da der Datensatz mit dem wir arbeiten eine einfache Querschnittsbefragung darstellt, in dem es keine Paare gibt, können wir leider kein Beispiel rechnen. Wie der Code aussehen würde, sehen Sie aber unten:\n\n#### BEISPIELCODE, DER NICHT AUSGEFÜHRT WERDEN KANN! ####\n# tTestPaare &lt;- t.test(df$Spalte1, df$Spalte2, paired = TRUE)\n\n\n\n11.2.3 t-Test für unabhängige Stichproben\nDer t-Test für unabhängige Stichproben sieht der Regression sehr ähnlich. Das heißt, wir geben auch hier eine “Formel” ein, bei der die abhängige Variable links von einer Tilde (~) steht und die unabhängige Variable rechts davon. Für das Beispiel nehmen wir wieder die lokale Bindung (df_lokal$lokaleBindung) als AV und dazu das Bundesland aus dem die Befragten stammen (df_lokal$Bula) als UV.\n\n# berechnet einen t-Test für unabhängige Stichproben\ntTestIndSample &lt;- t.test(df_lokal$lokaleBindung ~ df_lokal$Bula)\n\n# zeigt das Ergebnis an\ntTestIndSample\n\n\n    Welch Two Sample t-test\n\ndata:  df_lokal$lokaleBindung by df_lokal$Bula\nt = -3.2835, df = 1767.9, p-value = 0.001045\nalternative hypothesis: true difference in means between group RLP and group TH is not equal to 0\n95 percent confidence interval:\n -0.2439230 -0.0614935\nsample estimates:\nmean in group RLP  mean in group TH \n         3.868022          4.020730 \n\n\nSchauen wir uns auch hier die Ergebnisse an. Oben stehen wieder die Teststatistik t sowie die dazugehörigen Freiheitsgrad und der p-Wert. Hier lautet unser Ergebnis also: t(1767,9) = -3,28; p = 0,001.\nWeiter unten sehen wir dann wieder ein Konfidenzintervall. Es bezieht sich hier auf die Differenz zwischen den beiden Gruppenmittelwerten. Diese stehen dann direkt darunter. Wie wir sehen, fühlen sich die Menschen aus Rheinland-Pfalz etwas weniger lokal zugehörig (M = 3.87) als die Menschen aus Thüringen (M = 4,02).\nFür diesen Test berechnen wir nun Cohens d. Für die beiden Tests oben geht das grundsätzlich aber auch. Wir nutzen dafür ie cohens_d()-Funktion aus dem Paket effectsize, der wir das Objekt übergeben, in dem wir das Ergebnis des Tests gespeichert haben (hier also tTestIndSample). Außerdem übergeben wir der Funktion das Argument pooled_sd = FALSE. Darauf werden wir gleich noch mal kurz eingehen.\n\ncohens_d(tTestIndSample, pooled_sd = FALSE)\n\nCohen's d |         95% CI\n--------------------------\n-0.15     | [-0.25, -0.06]\n\n- Estimated using un-pooled SD.\n\n\nCohens d wird hier mit dem Wert -0,15 ausgegeben.D.h., der Mittelwert der Menschen aus Rheinland-Pfalz liegt um 0,15 Standardabweichungen der AV unter dem Wert der Befragten aus Thüringen.\nUm das Ergebnis vollständig verschriftlichen zu können, benötigen wir noch die Mittelwerte und Standardabweichungen der beiden Gruppen. Das machen wir wie gewohnt mit Hilfe von summarise(). Damit die Werte für jede Gruppe erhalten, nutzen wir vorher noch group_by() und geben dort die Spalte Bula an.\n\n# berechnet Mittelwerte und Standardabweichungen für die beiden Gruppen\nMWsSDs &lt;- df_lokal |&gt;\n  group_by(Bula) |&gt;\n  summarise(MW = round(mean(lokaleBindung, na.rm = TRUE),2),\n            SD = round(sd(lokaleBindung, na.rm = TRUE),2))\n\n# zeigt die Werte an\nMWsSDs\n\n# A tibble: 2 × 3\n  Bula     MW    SD\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 RLP    3.87  1.02\n2 TH     4.02  0.97\n\n\nNun können wir das Ergebnis wie folgt aufschreiben: &gt; Durch einen t-Test wurde geprüft, ob sich die Bindung an den Wohnort von Menschen in Rheinland-Pfalz (M = 3,87; SD = 1,02) von der Bindung der Befragten in Thüringen (M = 4,02; SD = 0,97). Die beiden Gruppen unterscheiden sich signifikant voneinander (t(1767,9) = -3,28; p = 0,001). Die Thüringer fühlen sich signifikant stärker an ihren Ort gebunden, aber dieser Effekt ist schwach (Cohens d = -0,15).\n\n\n\n\n\n\nVarianzhomogenität beim t-Test\n\n\n\nAm Ende des letzten Kapitels haben wir die Voraussetzungen von Regressionen kennengelernt. Dort hieß es unter anderem, dass die Streuung der Schätzfehler gleichmäßig sein muss. Ist das nicht der Fall, sprechen wir von Heteroskedastizität. Ist die Voraussetzung dagegen erfüllt, von Homoskedastizität. Im Video in diesem Kapitel haben Sie dann erfahren, dass der t-Test eigentlich nur eine spezielle Form der Regression ist und daher dieselben Voraussetzungen teilt. Im Kontext von t-Tests sprechen wir aber in der Regel von Varianzhomogenität (statt Homo- oder Heteroskedastizität).\nDer t-Test wird in R standardmäßig in einer Variante durchgeführt, die keine gleichen Varianzen voraussetzt. Das ist einerseits sinnvoll, da es immer gut ist, einen Test zu haben, der nominell weniger Voraussetzungen hat. Andererseits liegt hierin auch der Grund, dass wir in unserem Aufruf von cohens_d() das Argument pooled_sd = FALSE angeben mussten. Denn die cohes_d()-Funktion geht davon aus, dass die Version des t-Tests gerechnet wurde, die von Varianzhomogenität ausgeht.\nAlternativ könnten Sie auch beim Aufruf der t.test()-Funktion das Argument var.equal = TRUE angeben und könnten im Gegenzug pooled_sd = FALSE beim Aufruf von cohens_d() weglassen.\nIn dieser Variante können wir übrigens schön sehen, dass der t-Test eigentlich eine Regression ist. Wenn Sie die Ergebnisse des Codes unten aufmerksam vergleichen, werden Sie feststellen, dass die t- und p-Werte des t-Tests und des entsprechenden Koeffizienten in der Regression quasi identisch sind. Gleiches gilt für die berechnete Effektstärke von standardize_parameters() und cohens_d().\n\n# berechnet einen t-Test mit angenommener Varianzhomogenität\ntTest &lt;- t.test(df_lokal$lokaleBindung ~ df_lokal$Bula, var.equal = TRUE)\n\n# zeigt das Ergebnis an\ntTest\n\n\n    Two Sample t-test\n\ndata:  df_lokal$lokaleBindung by df_lokal$Bula\nt = -3.2651, df = 1839, p-value = 0.001114\nalternative hypothesis: true difference in means between group RLP and group TH is not equal to 0\n95 percent confidence interval:\n -0.24443519 -0.06098132\nsample estimates:\nmean in group RLP  mean in group TH \n         3.868022          4.020730 \n\n# berechnet dasselbe Modell als Regression\nregression &lt;- lm(lokaleBindung ~ Bula, data = df_lokal)\n\n# zeigt die Ergebnisse an\nsummary(regression)\n\n\nCall:\nlm(formula = lokaleBindung ~ Bula, data = df_lokal)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0207 -0.6180  0.1320  0.9793  1.1320 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.86802    0.03098 124.838 &lt; 0.0000000000000002 ***\nBulaTH       0.15271    0.04677   3.265              0.00111 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9958 on 1839 degrees of freedom\n  (5 Beobachtungen als fehlend gelöscht)\nMultiple R-squared:  0.005764,  Adjusted R-squared:  0.005223 \nF-statistic: 10.66 on 1 and 1839 DF,  p-value: 0.001114\n\n\n\n# berechnet Cohens d und zeigt es an\ncohens_d(tTest)\n\nCohen's d |         95% CI\n--------------------------\n-0.15     | [-0.25, -0.06]\n\n- Estimated using pooled SD.\n\n# berechnet standardisierte Effektstärken für die Regression und zeigt sie an\nstandardize_parameters(regression)\n\n# Standardization method: refit\n\nParameter   | Std. Coef. |         95% CI\n-----------------------------------------\n(Intercept) |      -0.07 | [-0.13, -0.01]\nBula [TH]   |       0.15 | [ 0.06,  0.24]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "ANOVA.html#ergebnis-visualisieren",
    "href": "ANOVA.html#ergebnis-visualisieren",
    "title": "11  Varianzanalyse",
    "section": "11.3 Ergebnis visualisieren",
    "text": "11.3 Ergebnis visualisieren\nIm Kontext von ANOVAs wird oft eine Visualisierung verwendet, die wir ganz am Ende von Kapitel 4 schon mal kennengelernt haben: Dort haben wir die Mittelwerte und Standardabweichungen von Gruppenmittelwerten geplottet. Hier machen wir im Prinzip das Gleiche, nur dass wir nicht die Standardabweichungen, sondern die 95%-Konfidenzintervalle der Mittelwerte visualisieren. Außerdem müssen wir anders als in Kapitel 4 berücksichtigen, dass wir nun zwei Gruppierungsvariablen haben.\nZuerst erstellen wir ein Objekt, in dem wir die Informationen aus der emmeans()-Funktion speichern. Da diese Funktion ein etwas komplexeres Format zurückgibt, nutzen wir dazu die Funktion as.data.frame().\n\n# wandelt die Ausgabe von emmeans() in einen Datensatz um und speichtert diesen in einem Objekt namens MWsCIs\nMWsCIs &lt;- as.data.frame(emmeans(model, specs = c(\"Bula\", \"aboLokal\")))\n\nDamit können wir nun einen Plot erstellen. Verglichen mit dem Plot aus Kapitel 4 ergänzen wir hier nur das Argument color innerhalb der aes()-Funktion, um das Lokalzeitungsabo darzustellen.\n\n# Plottet die Gruppenmittelwerte und Konfidenzintervalle\nplotANOVA &lt;- MWsCIs |&gt;\n  ggplot(aes(x = Bula, y = emmean, color = aboLokal))+\n  geom_point(size = 2)+\n  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), size = 1)+\n  labs(x = \"Bundesland\", y = \"lokale Bindung\", color = \"Lokalzeitung\")+\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplotANOVA\n\n\n\n\n\n\n\n\nIm Grunde sieht das schon ganz gut aus. Allerdings ist es etwas ungünstig, dass sich die Balken für die beiden Gruppen aus Thüringen überschneiden. Das können wir ganz einfach beheben, indem wir sowohl geom_point() als auch geom_errorbar() das Argument position mit dem Wert position_dodge(1) übergeben. Außerdem sind hier noch die Farben etwas angepasst:\n\n# Plottet die Gruppenmittelwerte und Konfidenzintervalle\nplotANOVA &lt;- MWsCIs |&gt;\n  ggplot(aes(x = Bula, y = emmean, color = aboLokal))+\n  geom_point(size = 2, position = position_dodge(1))+\n  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), size = 1, position = position_dodge(1))+\n  labs(x = \"Bundesland\", y = \"lokale Bindung\", color = \"Lokalzeitung\")+\n  theme_minimal()+\n  scale_color_manual(values = c(\"orange\", \"darkgreen\"))\n\nplotANOVA",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "ANOVA.html#varianzanalyse-durchführen",
    "href": "ANOVA.html#varianzanalyse-durchführen",
    "title": "11  Varianzanalyse",
    "section": "11.2 Varianzanalyse durchführen",
    "text": "11.2 Varianzanalyse durchführen\nIm folgenden Teil werden wir eine Varianzanalyse berechnen, bei der wir die neue Variable lokaleBindung als AV verwenden und das Bundesland, in dem die Befragten wohnen, sowie die Abfrage, ob die Befragten eine Lokalzeitung aboniert haben als UVs (Spalten Bula und A204). Letztere codieren wir zu einem beschrifteten Faktor um, wodurch die Interpretation später etwas leichter wird.\n\n#codiert die Spalte A204 zu einem beschrifteten Faktor um\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(aboLokal = factor(A204, labels = c(\"Abo\", \"kein Abo\")))\n\n\n11.2.1 ANOVA\nUm eine ANOVA zu berechnen, gehen wir in 3 Schritten vor:\n\nWir nutzen die lm()-Funktion, um ein lineares Modell (also quasi eine Regression) zu schätzen. Dabei ist es wichtig, dass wir das contrasts-Argument nutzen, und für jede kategoriale Variable (Faktoren, in der ANOVA-Sprache) den Wert \"contr.sum\" angeben. Das bedeutet nur, dass die entsprechenden Variablen unter der Haube so umcodiert werden, dass die Ausprägungen entweder den Wert -1 oder 1 annehmen. Dadurch kann der Interzept nun als Mittelwert der Gesamtstichprobe interpretiert werden.\nWir nutzen die Anova()-Funktion aus dem car-Paket, um einen F-Test für jeden Prädiktor durchzuführen.\nSofern mindestens ein Prädiktor signifikant war, können wir Post-hoc-Tests durchführen, um zu erfahren, welche Gruppen sich voneinander unterscheiden.\n\nFür den ersten Schritt nutzen wir die oben berechnete Variable lokaleBindung als AV. Die Spalten Bula und aboLokal verbinden wir mit dem *-Operator, sodass wir neben den Haupteffekten auch den Interaktionseffekt berechnen. Ganz konkret bedeutet das, dass wir hier 4 Gruppen miteinander vergleichen: (1) Thüringer mit Lokalzeitungsabo, (2) Thüringer ohne Abo, (3) Rheinland-Pfälzer mit Abo und (4) Rheinland-Pfälzer ohne Abo.\n\n# berechnet eine Regression mit der lokalen Bindung als AV und Bundesland und Lokalzeitungsabo als UVs. Achten Sie auf das contrasts-Argument!\nmodel &lt;- lm(lokaleBindung ~ Bula*aboLokal, data = df_lokal, \n             contrasts=list(Bula=\"contr.sum\", aboLokal=\"contr.sum\"))\n\nAusnahmsweise ignorieren wir die Ausgabe dieser Berechnung. Stattdessen nutzen wir direkt die Anova()-Funktion, die für jeden Prädiktor einen F-Wert berechnet. Wichtig ist hier, dass wir das Argument type = 3 verwenden. Auf die Details gehen wir an dieser Stelle nicht ein, aber im Prinzip geben wir R damit vor, in welcher Reihenfolge die Werte berechnet werden sollen. Tatsächlich kann das bei der ANOVA einen Unterschied machen!\n\n# berechnet F-Werte für jeden Prädiktor\nmodelAnova &lt;- Anova(model, type = 3)\n\n# zeigt die Ergebnisse an\nmodelAnova\n\nAnova Table (Type III tests)\n\nResponse: lokaleBindung\n               Sum Sq   Df    F value                Pr(&gt;F)    \n(Intercept)   23546.0    1 24406.4143 &lt; 0.00000000000000022 ***\nBula              8.7    1     9.0139              0.002716 ** \naboLokal         36.0    1    37.2712        0.000000001253 ***\nBula:aboLokal     5.8    1     5.9887              0.014492 *  \nResiduals      1760.7 1825                                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBevor wir die Ergebnisse verschriftlichen können, müssen wir noch die Effektstärken berechnen. Dazu nutzen wir die eta_squared()-Funktion aus dem effectsize-Paket. Dieser können wir unsere oben berechnete ANOVA übergeben. Außerdem geben wir das Argument alternative = \"two\" an. Damit legen wir fest, dass wir keine a priori Annahmen darüber hatten, welche Gruppe(n) einen höheren Mittelwert hatte(n).\n\n# berechnet das partielle eta² pro Prädiktor\neta_squared(modelAnova, alternative = \"two\")\n\nType 3 ANOVAs only give sensible and informative results when covariates\n  are mean-centered and factors are coded with orthogonal contrasts (such\n  as those produced by `contr.sum`, `contr.poly`, or `contr.helmert`, but\n  *not* by the default `contr.treatment`).\n\n\n# Effect Size for ANOVA (Type III)\n\nParameter     | Eta2 (partial) |       95% CI\n---------------------------------------------\nBula          |       4.91e-03 | [0.00, 0.01]\naboLokal      |           0.02 | [0.01, 0.03]\nBula:aboLokal |       3.27e-03 | [0.00, 0.01]\n\n\nBevor wir die Ergebnisse verschriftlichen, lohnt ein genauerer Blick auf die Ausgabe. Ganz oben sehen wir eine Warnung, dass die Effekte nur dann sinnvoll interpretiert werden können, wenn die sogenannten Kontraste angepasst wurden. Das haben wir oben in unserem Aufruf der lm()-Funktion erledigt! Außerdem weist R uns darauf hin, dass etwaige Kovariaten um den Mittelwert zentriert sein müssen (mean-centered).\n\n\n\n\n\n\nMittelwertzentrierung\n\n\n\nEine Variable zu zentrieren bedeutet, ihren Mittelwert auf 0 zu setzen. Das geht, indem der Mittelwert von jeder einzelnen Beobachtung abgezogen wird. Wenn wir z. B. einen Vektor haben, der die Zahlen von 0 bis 100 enthält, hat dieser Vektor den Mittelwert 50:\n\n# erstellt einen Vektor aus den Zahlen von 1 bis 100 und zeigt den Mittelwert an\nzahlen &lt;- 0:100\nmean(zahlen)\n\n[1] 50\n\n\nWir können diesen Vektor zentrieren, indem wir von jedem Element den Mittelwert abziehen. Der neue Mittelwert ist dann 0:\n\n# händische Zentrierung des Vektors\nzahlenZentriert &lt;- zahlen-50\nmean(zahlenZentriert)\n\n[1] 0\n\n\nGlücklicherweise hat R eine Funktion, die diesen Prozess für uns übernimmt. Sie heißt scale(). Die Funktion nimmt immer einen Vektor (entweder so wie hier oder z. B. eine Spalte aus einem Datensatz) als erstes Argument. Zwei weitere Argumente sind center und scale, die beide automatisch als TRUE angenommen werden, sofern wir nichts anderes angeben. center bedeutet, dass der Mittelwert subtrahiert wird. scale bedeutet, dass die Beobachtungen zusätzlich durch die Standardabweichung aller Beobachtungen geteilt werden. Das Ergebnis ist dann nicht mehr eine zentrierte Variable, sondern eine normalisierte, die den Mittelwert 0 und die Standardabweichung 1 hat. Wenn Sie eine ANCOVA berechnen, sollten Sie für metrische Variablen vorher unbedingt diese Funktion anwenden!\n\n# nutzt die Funktion scale(), um den Vektor zu normalisieren\nzahlenScale &lt;- scale(zahlen)\nmean(zahlenScale)\n\n[1] 0\n\nsd(zahlenScale)\n\n[1] 1\n\n\n\n\nMit dieser Warnung im Hinterkopf können wir die Ergebnisse interpretieren. Lassen Sie sich von der etwas komischen Darstellung der Effektstärke nicht abschrecken!\nFür das Bundesland wird ein Effekt von `4.91e-03` angegeben. Übersetzt in “normale” Zahlen bedeutet das 0,00491. Wir schreiben also einfach nur 3 Nullen vor die 4,91. Wenn Sie mal nicht sicher sind können Sie solche Zahlen auch unten in die Konsole eingeben und mit Enter ausführen, dann zeigt R Ihnen die Zahl im gewohnten Format an. Ein partielles eta² von 0,00491 bedeutet, dass die Variable ca. 0,05 Prozent der Varianz erklärt. Ob die Leute ein Lokalzeitungsabo haben erklärt dagegen etwa 2 Prozent (partielles eta² = 0,02) und die Interaktion aus diesen beiden Variablen erklärt ca. 0,03 Prozent (partielles eta² = 0,003). Wir haben also drei (sehr) schwache Effekte identifiziert, die aber laut unserer ANOVA-Tabelle oben alle signifikant sind.\nMit Hilfe der beiden Ausgaben können wir die Ergebnisse nach dem folgenden Muster aufschreiben: Pro Prädiktor werden der F-Wert, die dazugehörigen Freiheitsgrade (Spalte Df) und der p-Wert angegeben. Zusätzliche Informationen über das Gesamtmodell können wir der Tabelle oben entnehmen.\n\nUm zu prüfen, ob ein Lokalzeitungsabo und das Bundesland, in dem die Befragten wohnen, sowie der Interaktionseffekt dieser beiden Variablen einen Effekt auf die lokale Bindung der Befragten haben, wurde eine ANOVA berechnet. Das Bundesland hatte einen signifikanten, aber sehr schwachen Effekt (F(1, 1825) = 9,01; p = 0,003; partielles eta² = 0,005). Der Effekt des Lokalzeitungsabos war ebenfalls signifikant und schwach (F(1, 1825) = 37,27; p &lt; 0,001; partielles eta² = 0,02). Auch der Interaktionseffekt der beiden Variablen war signifikant, aber sehr schwach (F(1, 1825) = 5,99; p = 0,014; partielles eta² = 0,003).\n\nBevor die Analyse abgeschlossen ist, müssen wir noch die sogenannten Post-hoc-Tests durchführen, bei denen wir die jeweiligen Gruppenmittelwerte miteinander vergleichen. Dazu nutzen wir zwei Funktionen: emmeans() berechnet die Mittelwerte und die dazugehörigen Standardfehler für alle Gruppen. Dazu übergeben wir der Funktion das oben geschätzte Modell (aus der lm()-Funktion) und mit dem Argument specs die Faktoren aus unserer ANOVA Das Ergebnis dieser Funktion geben wir an pairs() weiter. Diese Funktion berechnet Mittelwertvergleiche (also t-Tests) für alle möglichen Gruppenvergleiche. Dabei werden die p-Werte korrigiert (siehe Video).\n\n# berechnet Post-Hoc-Tests für die Anova\nemmeans(model, specs = c(\"Bula\", \"aboLokal\")) |&gt;\n  pairs()\n\n contrast                   estimate     SE   df t.ratio p.value\n RLP Abo - TH Abo            -0.0284 0.0853 1825  -0.332  0.9873\n RLP Abo - RLP kein Abo       0.4370 0.0626 1825   6.982  &lt;.0001\n RLP Abo - TH kein Abo        0.1585 0.0626 1825   2.534  0.0552\n TH Abo - RLP kein Abo        0.4654 0.0808 1825   5.759  &lt;.0001\n TH Abo - TH kein Abo         0.1869 0.0808 1825   2.314  0.0953\n RLP kein Abo - TH kein Abo  -0.2785 0.0562 1825  -4.953  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nHier sehen wir (in der letzten Spalte), dass wir drei signifikante Gruppenvergleiche haben: Menschen aus Rheinland-Pfalz mit Abo haben eine höhere lokale Bindung als Menschen aus Rheinland-Pfalz ohne Abo. Das gleiche gilt nicht für Menschen aus Thüringen! Außerdem haben Menschen aus Thüringen mit Abo eine höhere Bindung als Menschen aus RLP ohne Abo. Letztere haben außerdem eine geringere Bindung als Menschen aus Thüringen ohne Abo. Das ist zugegebenermaßen etwas komplex. Um die Ergebnisse etwas besser nachvollziehen zu können, lohnt ein Blick auf die deskriptiven Werte. Diese können Sie sich anzeigen lassen, indem Sie nur die emmeans()-Funktion aufrufen, ohne das Ergebnis an pairs() weiterzugeben. Eine solche Tabelle sollten Sie auch im Ergebnisteil einer empirischen Arbeit mit angeben!\n\n# berechnet die Gruppenmittelwerte aus der ANOVA und zeigt diese an\nemmeans(model, specs = c(\"Bula\", \"aboLokal\")) \n\n Bula aboLokal emmean     SE   df lower.CL upper.CL\n RLP  Abo        4.13 0.0483 1825     4.04     4.23\n TH   Abo        4.16 0.0703 1825     4.02     4.30\n RLP  kein Abo   3.70 0.0398 1825     3.62     3.77\n TH   kein Abo   3.97 0.0397 1825     3.90     4.05\n\nConfidence level used: 0.95 \n\n\nHier sehen wir nun sehr deutlich, dass Befragte aus Thüringen mit und ohne Abo jeweils eine etwas höhere lokale Bindung hatten als die jeweiligen Gruppen aus Rheinland-Pfalz. Außerdem haben in jedem Bundesland die Befragten mit Abo eine höhere Bindung als Befragte ohne Abo. Den signifikanten Interaktionseffekt erkennen Sie daran, dass der Unterschied zwischen Thüringen und Rheinland-Pfalz nur für Befragte ohne Abo relativ klar zu sehen ist, nicht aber für Befragte mit Abo!\nBevor wir diese Ergebnisse visualisieren, schreiben wir sie noch kurz auf:\n\nPost-hoc-Tests zeigten, dass drei Gruppenvergleiche signifikant waren: Befragte aus Rheinland-Pfalz mit Abo einer Lokalzeitung hatten eine höhere lokale Bindung als Befragte aus Thüringen ohne Abo (t(1825) = 6,98; p &lt; 0,001). Außerdem hatten Befragte aus TH eine höhere Bindung als Befragte aus Rheinland-Pfalz ohne Abo (t(1825) = 5,75; p &lt; 0,001). Diese Gruppe hatte wiederum eine geringere Bindung als Befragte aus Thüringen ohne Abo (t(1825) = -4,95; p &lt; 0,001)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "Reliabilität.html",
    "href": "Reliabilität.html",
    "title": "12  Reliabilität",
    "section": "",
    "text": "12.1 Reliabilität von Skalen\nIn den vergangenen Kapiteln und Übungssitzungen haben wir mehrfach sogenannte Mittelwertindizes berechnet. Dabei haben wir pro Person einen Mittelwert aus den Antworten auf mehrere Fragen gebildet. Das machen wir immer dann, wenn wir Dinge wie Einstellungen oder Persönlichkeitsmerkmale und -eigenschaften nicht mit einem Item (also einer Frage) messen, sondern mit einer Fragebatterie - einer sogenannten Skala. Mittelwertindizes sind zwar die geläufigste Art, solche Daten zu verdichten, aber manchmal werden Sie in der Literatur auch andere Formen der Indexbildung sehen, z. B. Summenindizes, bei denen die einzelnen Antworten einfach aufsummiert werden. Beides ist aber nur dann zulässig, wenn die Skala das Konstrukt, das sie messen soll, auch zuverlässig misst. Oder anders gesagt: Indizes dürfen wir nur dann bilden, wenn die Skala reliabel ist.1",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reliabilität</span>"
    ]
  },
  {
    "objectID": "Reliabilität.html#reliabilität-von-skalen",
    "href": "Reliabilität.html#reliabilität-von-skalen",
    "title": "12  Reliabilität",
    "section": "",
    "text": "12.1.1 Cronbachs Alpha\nDie Reliabilität einer Skala berechnen wir mit Cronbachs Alpha. Im Prinzip handelt es sich dabei um ein Maß, das angibt, wie stark die einzelnen Items einer Skala korrelieren. Entsprechend wird Cronbachs Alpha auch manchmal als Maß der internen Konsistenz einer Skala bezeichnet. Wie auch der Korrelationskoeffizient kann Cronbachs Alpha maximal den Wert 1 annehmen. Negative Werte sind theoretisch auch möglich, deuten aber in erster Linie darauf hin, dass eine wichtige Voraussetzung nicht erfüllt wurde (siehe unten). Werte ab 0,7 gelten als akzeptabel bzw. gut (ab 0,8) oder sehr gut (ab 0,9). Werte unter 0,7 sind eher schlecht, sodass kein Index aus allen Items gebildet werden sollte. Glücklicherweise berechnet R (siehe nächster Abschnitt) nicht nur, wie hoch Cronbachs Alpha tatsächlich ist, sondern auch, wie hoch es wäre, falls einzelne Items ausgelassen werden würden. Dadurch können schlechte Reliabilitätswerte in der Praxis häufig verbessert werden, indem nur ein Teil einer ursprünglich abgefragten Skala für die Analyse verwendet wird. Dabei ist es wichtig, alles genau zu dokumentieren!\nUm Cronbachs Alpha sinnvoll berechnen zu können, sollten zwei Bedingungen erfüllt sein: Erstens sollte das Konstrukt eindimensional sein. Das bedeutet, dass alle Items versuchen, denselben Aspekt eines Konstrukts zu messen. Bei mehrdimensionalen Konstrukten muss Cronbachs Alpha für jede Dimension berechnet werden. Zweitens sollten alle Items identisch gepolt sein. Damit ist gemeint, dass hohe und niedrige Werte für jedes Item dieselbe Bedeutung haben (z. B. 5 = hohe Zustimmung = positive Einstellung). Schauen Sie sich z.B. mal die beiden untenstehenden Items aus einer Skala zur Messung des Kognitionsbedürfnisses an, also der Tendenz, ausführlich über Dinge nachzudenken:\n\nIn erster Linie denke ich, weil ich muss.\n\n\nDie Aufgabe, neue Lösungen für Probleme zu finden, macht mir wirklich Spaß.\n\nDie beiden Items sind unterschiedlich gepolt: Eine hohe Zustimmung beim ersten Item spricht für ein geringes Kognitionsbedürfnis, wohingegen eine hohe Zustimmung beim zweiten Item auf ein hohes Kognitionsbedürfnis schließen lässt.\nBevor wir also Cronbachs Alpha berechnen bzw. bevor wir einen Index bilden, müssen wir prüfen, ob alle Items identisch gepolt sind.\n\n\n12.1.2 Cronbachs Alpha berechnen\nCronbachs Alpha wird mit der alpha()-Funktion aus dem psych-Paket berechnet, das wir im untenstehenden Code laden bzw. erst installieren. Zusätzlich laden wir das tidyverse.\n\n# versucht das Paket psych zu laden und installiert und lädt es, falls es nicht installiert ist\nif(!require(psych)){\n  install.packages(\"psych\")\n  library(psych)\n}\n\nLade nötiges Paket: psych\n\n# lädt das tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWie Sie vielleicht bemerkt haben, ist die Ausgabe, die erscheint, wenn das tidyverse geladen wird, etwas länger als sonst. Das liegt daran, dass das tidyverse und das psych-Paket einige Funktionsnamen teilen. Tatsächlich ist für uns ein relevanter Hinweis dabei: ggplot2::alpha() masks psych::alpha(). Ganz konkret bedeutet das, dass R unter dem Funktionsnamen alpha() aktuell eine Funktion aus dem ggplot2-Paket erwartet. Wie wir trotzdem die Funktion aus dem psych-Paket ausführen können, sehen wir gleich weiter unten.\nVorher laden wir noch den Datensatz mit Ihren Antworten aus der ersten Sitzung. Zur Erinnerung, dort haben Sie Ihre Zustimmung zu fünf Aussagen angegeben (1 = “stimme überhaupt nicht zu”, 7 = “stimme voll und ganz zu”). Die Items lauteten wie folgt:\n\nIch fühle mich sicher im Umgang mit Zahlen.\nIn Mathe war ich immer schlecht.\nWenn ich etwas berechnen soll, versuche ich, mich davor zu drücken.\nMir macht es Spaß, knifflige Aufgaben zu lösen.\nIch hoffe einfach nur, dass dieser Kurs schnell vorbei ist.\n\n\n# lädt den Datensatz mit Antworten aus Sitzung 1\ndf_sitzung1 &lt;- read.csv(\"Daten/datenSitzung1.csv\")\n\nFür die ersten 5 Items können wir nun Cronbachs Alpha berechnen. Vorher prüfen wir die Voraussetzungen: Über die Eindimensionalität kann man bei dieser Abfrage sicher streiten. Es handelt sich um eine Ad-hocSkala, die messen soll, wie groß Ihre Berührungsängste vor Statistik waren. Ad hoc bedeutet in diesem Kontext nur, dass ich mir einige Items ausgedacht habe, ohne in einem aufwändigen, iterativen Prozess die Items zu formulieren. Für unsere Übungszwecke reicht das aus; für eine wissenschaftliche Publikation eher nicht. Ein Blick auf die einzelnen Items verrät darüber hinaus, dass sie nicht alle gleich gepolt sind. Konkret unterscheiden sich die Items mit den Nummern 1 und 4 von den anderen drei. Eine hohe Zustimmung hier bedeutet eher, dass Sie keine großen Sorgen vor Statistik (und R) haben. Bei den anderen Items stehen hohe Werte dagegen für größere Berührungsängste. Das können wir bei der Berechnung von Cronbachs Alpha direkt berücksichtigen.\nWie oben beschrieben nutzen wir dafür die alpha()-Funktion aus dem psych-Paket. Durch das Argument check.keys = TRUE legen wir fest, dass R prüfen soll, ob alle Items gleich gepolt sind. Bei Bedarf werden die Items dann automatisch angepasst. Wichtig ist außerdem, dass wir die Funktion nicht wie sonst aufrufen, indem wir nur alpha() schreiben, sondern den Namen des Pakets gefolgt von :: davor schreiben. Dadurch sagen wir R, welche alpha()-Funktion wir aufrufen wollen. Hier schreiben wir also psych::alpha(). Da unser Datensatz noch weitere Spalten enthält müssen wir außerdem erst die fünf Items auswählen. Dazu nutzen wir die select()-Funktion aus dem tidyverse bzw. darin dplyr. Innerhalb von select() nutzen wir contains(). Diese Funktion erlaubt es uns, Spalten basierend auf ihrem Namen bzw. einem Teil des Namens auszuwählen. Indem wir select(contains(\"item\")) aufrufen, wählen wir alle Spalten aus, deren Name die Zeichenfolge “item” enthält.\n\n# berechnet Cronbachs Alpha für die 5 Items \ncronbachsAlpha &lt;- df_sitzung1 |&gt;\n  select(contains(\"item\")) |&gt;\n  psych::alpha(check.keys = TRUE)\n\nWarning in psych::alpha(select(df_sitzung1, contains(\"item\")), check.keys = TRUE): Some items were negatively correlated with the first principal component and were automatically reversed.\n This is indicated by a negative sign for the variable name.\n\n# zeigt das Ergebnis an\ncronbachsAlpha\n\n\nReliability analysis   \nCall: psych::alpha(x = select(df_sitzung1, contains(\"item\")), check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.73      0.74     0.8      0.37 2.9 0.087    4 1.1     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.52  0.73  0.87\nDuhachek  0.56  0.73  0.90\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nitem1-      0.63      0.64    0.71      0.30 1.7    0.126 0.058  0.29\nitem2       0.74      0.74    0.76      0.42 2.9    0.086 0.047  0.34\nitem3       0.55      0.57    0.59      0.25 1.3    0.151 0.039  0.23\nitem4-      0.72      0.73    0.73      0.40 2.7    0.095 0.055  0.40\nitem5       0.76      0.77    0.83      0.46 3.4    0.084 0.070  0.52\n\n Item statistics \n        n raw.r std.r r.cor r.drop mean  sd\nitem1- 25  0.80  0.81  0.77   0.67  4.4 1.4\nitem2  25  0.62  0.61  0.51   0.36  3.9 1.8\nitem3  25  0.90  0.91  0.94   0.83  4.4 1.6\nitem4- 25  0.64  0.65  0.58   0.41  4.4 1.7\nitem5  25  0.54  0.54  0.32   0.29  3.2 1.6\n\nNon missing response frequency for each item\n         1    2    3    4    5    6    7 miss\nitem1 0.12 0.08 0.20 0.28 0.28 0.04 0.00    0\nitem2 0.12 0.08 0.28 0.12 0.16 0.20 0.04    0\nitem3 0.00 0.12 0.20 0.20 0.24 0.12 0.12    0\nitem4 0.16 0.12 0.16 0.20 0.24 0.12 0.00    0\nitem5 0.16 0.20 0.28 0.20 0.04 0.08 0.04    0\n\n\nSchauen wir uns nun die Ausgabe an. Ganz oben zeigt R uns an, wie wir die Funktion aufgerufen haben. Hier sieht alles so aus, wie wir es gewollt haben.\nIn der ersten kleinen Tabelle darunter interessiert uns vor allem der Wert unter raw_alpha, also hier 0,73. Wir liegen also knapp über dem akzeptablen Niveau von 0,7. Darunter zeigt uns R zwei verschiedene Varianten von Konfidenzintervallen an. Diese werden üblicherweise nicht angegeben, aber falls Sie mal dazu aufgefordert werden, können Sie es hier einsehen. Danach sehen wir eine Tabelle mit der Überschrift Reliability if an item is dropped:. Hier wird uns angezeigt, wie sich Cronbachs Alpha verändern würde, wenn wir einzelne Items aus der Analyse ausschließen. In der ersten Spalte stehen die Namen der Items. Am - neben item1 und item4 erkennen Sie, dass diese beiden Items umcodiert wurden. In der Spalte raw_alpha sehen wir, dass wir die Reliabilität durch einen Ausschluss von Item 2 bzw Item 5 minimal verbessern könnten (die Werte sind größer als 0,73). Allerdings wären diese Verbesserungen so marginal, dass es hier nicht unbedingt ratsam wäre. Tatsächlich gibt es keine festen Richtlinien oder Best Practices, an denen wir uns beim Ausschluss von Variablen orientieren könnten. Wäre eine deutliche Verbesserung möglich, könnten Sie aber darüber nachdenken. Die unteren beiden Tabellen können Sie weitestgehend ignorieren. Die vorletzte Tabelle enthält Informationen zu den Items, wie Mittelwert und Standardabweichung. Die letzte Tabelle zeigt dagegen an, wie sich die gültigen Antworten auf die möglichen Antworten verteilen.\nJetzt wissen wir also, dass wir einen Mittelwertindex bilden dürfen. Das können Sie so machen, wie in den letzten Kapiteln und Übungssitzungen. Dabei müssten Sie nur darauf achten, die Items 1 und 4 vorher umzucodieren. Eine ganz einfache Möglichkeit mit mutate() sieht so aus:\n\n# codiert Items 1 und 4 um\ndf_sitzung1 &lt;- df_sitzung1 |&gt;\n  mutate(item1rec = item1*-1+8,\n         item4rec = item4*-1+8)\n\nKonkret wurden hier die Ursprungswerte erst in eine negative Zahl umgewandelt. Das +8 sorgt dann dafür, dass alle Werte wieder positiv sind und von 1 bis 7 reichen. Wenn eine Person z.B. vorher den Wert 7 hatte, wurde dieser erst zu -7 und durch das +8 dann zu 1.\nDer Vollständigkeit halber können wir nun den Index wie gewohnt berechnen:\n\n# berechnet einen Mittelwertindex für die 5 Items\ndf_sitzung1 &lt;- df_sitzung1 |&gt;\n  rowwise() |&gt;\n  mutate(index = mean(c(item1rec, item2, item3, item4rec, item5))) |&gt;\n  ungroup()\n\nAlternativ können wir das Ergebnis der alpha()-Funktion verwenden. Wenn Sie einmal rechts in Ihr Environment schauen, sehen Sie, dass das Objekt cronbachsAlpha eine Liste ist. Diese enthält das Objekt scores, in dem ein Mittelwertindex gespeichert ist. Den Index können wir damit auch wie folgt in unseren Datensatz übertragen:\n\n# übertragt die von alpha() berechneten Scores in den Datensatz\ndf_sitzung1$indexAlternativ &lt;- cronbachsAlpha$scores",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reliabilität</span>"
    ]
  },
  {
    "objectID": "Reliabilität.html#reliabilität-in-der-inhaltsanalyse",
    "href": "Reliabilität.html#reliabilität-in-der-inhaltsanalyse",
    "title": "12  Reliabilität",
    "section": "12.2 Reliabilität in der Inhaltsanalyse",
    "text": "12.2 Reliabilität in der Inhaltsanalyse\nIm bisherigen Kursverlauf haben wir uns mit Daten aus Befragungen befasst. Alles was Sie dabei gelernt haben, lässt sich grundsätzlich auch auf andere Daten anwenden, z.B. solche aus Experimentaldesigns oder auch aus Inhaltsanalysen.\nWie Sie vielleicht noch aus der Methodenvorlesung wissen, entstehen die Daten in der Inhaltsanalyse dadurch, dass Sie als Forschende selber Hand anlegen und Medieninhalte codieren. Zuvor entwerfen Sie Kategorien, die mit Fragen in der Befragung vergleichbar sind, und dazugehörige Ausprägungen. Die Medieninhalte aus Ihrer Stichprobe ordnen Sie dann jeweils in eine Ausprägung pro Kategorie ein. Idealerweise ist es bei Inhaltsanalysen egal, wer die Codierung durchführt. Das Ergebnis sollte immer dasselbe sein. Die Reliabilität bezieht sich hier also strenggenommen auf das Codebuch. Gleichzeitig ist das Codieren eben eine händische Arbeit, bei der Fehler passieren können. Und bei schlechten Codebüchern kann es passieren, dass unterschiedliche Codierinnen und Codierer das Codebuch anders interpretieren und entsprechend zu unterschiedlichen Ergebnissen kommen. Und wenn nur eine Person codiert, ist es möglich, dass die Codierungen von Tagesform und Gemüt der Person abhängig sind.\nEntsprechend sollte für jede Kategorie einer Inhaltsanalyse die Reliabilität berechnet werden. Dazu muss ein Teil der Stichprobe entweder von mehreren Codiererinnen und Codierern bearbeitet werden (Intercoder-Reliabilität) oder von einer Person mehrfach (Intracoder-Reliabilität).\nUm Ihnen das Verfahren zu zeigen nutzen wir das tidycomm-Paket und den darin enthaltenen Datensatz fbposts. Dieser enthält die Daten von 45 politischen Facebook Posts (Spalte post_id), die von sechs Codiererinnen und Codierern codiert wurden (Spalte coder_id). Für jeden Post wurde codiert, um was für eine Art es sich handelt (type, z.B. ein Foto oder Video), die Anzahl der enthaltenen Bilder (n_pictures), sowie einige Eigenschaften, die populistischen Inhalten zugeschrieben werden. Konkret, ob Eliten attackiert wurden (pop_elite), ob die Menschen/das Volk erwähnt wurden (pop_people) und ‘othering’ betrieben wird, also bestimmte Gruppen als ‘die anderen’ ausgegrenzt wurden (pop_othering).\n\n# versucht das Paket tidycomm zu laden und installiert und lädt es, falls es nicht installiert ist\n\nif(!require(tidycomm)){\n  install.packages(\"tidycomm\")\n  library(tidycomm)\n}\n\nLade nötiges Paket: tidycomm\n\n\n\nAttache Paket: 'tidycomm'\n\n\nDas folgende Objekt ist maskiert 'package:psych':\n\n    describe\n\n# lädt den im Paket enthaltenen Datensatz fbposts\ndf_fb  &lt;- fbposts\n\nUm die Reliabilität zu berechnen können wir nun die test_icr()-Funktion verwenden. Dieser Funktion müssen wir immer mindestens drei Argumente übergeben:\n\nDie Daten. Das können wir gewohnt mit der Pipe (|&gt;) machen.\nEine Spalte, die den Fall identifiziert (hier post_id).\nEine Spalte, die angibt, wer codiert hat (hier coder_id).\n\nZusätzlich können Sie angeben, für welche Spalten wir die Reliabilität berechnen wollen. Tun wir das nicht, werden die Werte für jede Spalte berechnet. Der R-Code sieht dann so aus:\n\n# berechnet die Intercoder-Reliabilität für den Datensatz df_fb\n\ndf_fb |&gt;\n  test_icr(post_id, coder_id)\n\n# A tibble: 5 × 8\n  Variable     n_Units n_Coders n_Categories Level   Agreement Holstis_CR\n* &lt;chr&gt;          &lt;int&gt;    &lt;int&gt;        &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 type              45        6            4 nominal     1          1    \n2 n_pictures        45        6            7 nominal     0.822      0.930\n3 pop_elite         45        6            6 nominal     0.733      0.861\n4 pop_people        45        6            2 nominal     0.778      0.916\n5 pop_othering      45        6            4 nominal     0.867      0.945\n# ℹ 1 more variable: Krippendorffs_Alpha &lt;dbl&gt;\n\n\nDie Tabelle enthält neben einigen Informationen über die Variablen (ihren Namen, die Fallzahl, Anzahl der Codiererinnen und Codierern, Anzahl der Ausprägungen und das Datenniveau) drei Reliabilitätswerte:\n\nDie Spalte Agreement gibt an, wie hoch die (prozentuale) Übereinstimmung aller Codiererinnen und Codierer ist. Wenn sich alle immer einig sind, hat das Agreement den Wert 1 und entsprechend den Wert 0, wenn in keinem Fall Einigkeit herrscht. Wichtig ist, dass die Übereinstimmung pro Fall nur dann als solche gewertet wird, wenn sich wirklich alle Codiererinnen und Codierer einig sind. Weicht auch nur eine Person ab, gilt der Fall nicht als übereinstimmend codiert.\nDie Spalte Holstis_CR gibt an, wie hoch die Übereinstimmung nach Holsti ist. Hierbei handelt es sich um die durchschnittliche prozentuale Übereinstimmung aller Paare von Codiererinnen und Codierern. Wenn bspw. drei Personen einen Fall codiert haben und sich zwei davon einig sind, die dritte Person aber abweicht, ist das Agreement wie oben beschrieben 0, wohingegen der Koeffizient nach Holsti 0,33 ist, weil sich eins von drei möglichen Paaren aller Codiererinnen und Codierer einig ist. In der Regel ist dieser Wert also etwas höher als die einfache Übereinstimmung.\nIn der Spalte Krippendorffs_Alpha steht der Wert für, Sie ahnen es, Krippendorffs Alpha. Hierbei handelt es sich um einen Reliabilitätskoeffizienten, der berücksichtigt, dass eine gewisse Übereinstimmung allein durch Zufall zu erwarten wäre. Außerdem berücksichtigt der Wert das Datenniveau der Kategorien. Krippendorffs Alpha ist so etwas wie der Goldstandard unter den Reliabilitätskoeffizienten in der Kommunikationswissenschaft und sollte in der Regel bevorzugt angegeben werden. Werte ab 0,8 gelten dabei als akzeptabel. Wird dieser Wert (im Rahmen eines Pretests) nicht erreicht, sollte das Codebuch angepasst werden. In der Tabelle oben sehen wir, dass die drei Kategorien mit Populismusbezug einen sehr niedrigen Alpha-Wert aufweisen. Entsprechend können die Daten nur schwer interpretiert werden. Außerdem lohnt sich ein genauer Blick auf die Spalte zur Variable n_pictures. Dort steht als Datenniveau nominal, obwohl es sich ja um eine metrische, verhältnisskalierte Variable handelt (die Anzahl der Bilder in einem Post hat einen natürlichen Nullpunkt und die Abstände zwischen den möglichen Ausprägungen sind identisch). Indem wir der test_icr()-Funktion das Argument levels übergeben, können wir das Datenniveau manuell vorgeben, sodass R nicht raten muss. Das sieht dann so aus:\n\n\n# berechnet die Intercoder-Reliabilität für den Datensatz df_fb\n\ndf_fb |&gt;\n  test_icr(post_id, coder_id, levels = c(n_pictures = \"ratio\"))\n\n# A tibble: 5 × 8\n  Variable     n_Units n_Coders n_Categories Level   Agreement Holstis_CR\n* &lt;chr&gt;          &lt;int&gt;    &lt;int&gt;        &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 type              45        6            4 nominal     1          1    \n2 n_pictures        45        6            7 ratio       0.822      0.930\n3 pop_elite         45        6            6 nominal     0.733      0.861\n4 pop_people        45        6            2 nominal     0.778      0.916\n5 pop_othering      45        6            4 nominal     0.867      0.945\n# ℹ 1 more variable: Krippendorffs_Alpha &lt;dbl&gt;\n\n\nTatsächlich hat sich der Wert für die Variable n_pictures nun geändert bzw. er ist etwas besser geworden. Wie Sie sehen, sollte das korrekte Datenniveau also unbedingt spezifiziert werden!",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reliabilität</span>"
    ]
  },
  {
    "objectID": "Reliabilität.html#footnotes",
    "href": "Reliabilität.html#footnotes",
    "title": "12  Reliabilität",
    "section": "",
    "text": "Und valide, aber da sich Validität nicht bzw. nur sehr schwer quantifizieren lässt, gehen wir darauf nicht ein.↩︎",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reliabilität</span>"
    ]
  },
  {
    "objectID": "Datenaufbereitung.html",
    "href": "Datenaufbereitung.html",
    "title": "13  Datenaufbereitung",
    "section": "",
    "text": "13.1 Daten exportieren\nBevor Sie die Daten aufbereiten können, müssen Sie die Daten herunterladen. Nachdem Sie sich bei soscisurvey eingelogggt haben, müssen Sie zunächst über die Menüleiste links zu Erhobene Daten navigieren.\nIn der Navigation öffnen sich nun neue Unterpunkte. Sie können z.B. doe Rücklaufstatistik einssehen oder Fälle für den Export selektieren. Beim Export am Ende des Projektes sollten Sie darauf achten, dass Sie nur die Daten der eigentlichen Stichprobe exportieren und nicht die des Pretests.\nAnschließend können Sie über die Navigationsleiste zu Daten herunterladen wechseln. Damit Sie die Daten in R wie gewohnt einlesen und verarbeiten können, müssen Sie einige Dinge beachten:\nMit diesen Einstellungen sollte das Einlesen der Daten in R wie gewohnt funktionieren.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datenaufbereitung</span>"
    ]
  },
  {
    "objectID": "Datenaufbereitung.html#daten-exportieren",
    "href": "Datenaufbereitung.html#daten-exportieren",
    "title": "13  Datenaufbereitung",
    "section": "",
    "text": "Wählen Sie ganz oben das Dateiformat CSV aus.\nEntfernen Sie den Haken bei Unicode-Unterstützung für nicht-westeuropäische Sprachen. Falls Sie im Rahmen Ihrer Studie Textdaten erheben, die nicht-westeuropäische Zeichen enthalten, sollten Sie diesen Haken bestehen lassen. Sie müssen dann beim Einlesen der Daten innerhalb von read.csv() das Argument fileEncoding = \"UTF-16LE\" ergänzen.\nUnter Antworten der Antworten sollte Numerische Codes angegeben werden. Das bedeutet, dass alle Antworten in Form von Zahlen gespeichert werden. Ausgenommen daovn sind nur offene Texteingaben. Nominale und ggf. ordinale Daten können Sie später in R mit factor() beschriften.\nDirekt darunter bei Fehlende Werte können Sie Fehlende Werte entfernen auswählen. An den entsprechenden Stellen im Datensatz steht dann ein NA.\nAls Dezimalzeichen sollte der Punkt ausgewöhlt werden (siehe Kapitel 2).\nUnter Variablen-Trennzeichen können Sie das Komma auswählen.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datenaufbereitung</span>"
    ]
  },
  {
    "objectID": "Datenaufbereitung.html#fälle-mit-zu-vielen-fehlenden-werten-aussortieren",
    "href": "Datenaufbereitung.html#fälle-mit-zu-vielen-fehlenden-werten-aussortieren",
    "title": "13  Datenaufbereitung",
    "section": "13.2 Fälle mit zu vielen fehlenden Werten aussortieren",
    "text": "13.2 Fälle mit zu vielen fehlenden Werten aussortieren\nZur Veranschaulichung der Datenaufbereitung werden wir in diesem Kapitel keine echten Daten verwenden. Der Code, den Sie im Lauf des Kapitels hier sehen, wird also nicht funktionieren. Sie können ihn aber mit Ihren eigenen Daten ergänzen und dann ausführen.\nWir beginnen wie immer damit, dass wir das tidyverse laden und Daten einlesen.\n# lädt das tidyverse}\nlibrary(tidyverse)  \n\n# liest einen Datensatz ein \ndf &lt;- read.csv(\"Daten/einDatensatz.csv\")\nOft ist es sinnvoll oder sogar notwendig, Fälle aus dem Datensatz auszuschließen, die entweder zu viele fehlende Werte ausweisen, oder die zumindest felende Werte in zentralen Konstrukten (z.B. der AV) haben. In diesen Situationen können Sie die filter()- und is.na()-Funktionen verwenden, die Sie bereits kennengelernt haben.\nIm einfachsten Fall haben Sie eine AV (ggf. in Form eines Index) und wollen alle Fälle ausschließen, die hier keinen Wert haben:\n# filtert alle Fälle aus, die einen fehlenden Wert bei der AV haben\ndf &lt;- df |&gt;\n  filter(!is.na())\nIn anderen Situationen kann es sinnvoll sein, Fälle auszuschließen, die insgesamt zu viele fehlende Werte aufweisen, z.B. 50 Prozent aller Antworten. Hierzu müssen Sie erst die Gesamtanzahl an fehlenden Werten berechnen. Dazu nutzen wir mutate(). In diesem mutate()-Aufruf berechnen wir die neue Spalte summeNAs. Dazu nutzen wir die rowSums()-Funktion. Jetzt wird es etwas komplexer: innerhalb von rowSums() rufen wir is.na() auf, denn wir wollen ja wissen, wie viele fehlende Werte es gibt. Damit R weiß, auf welche Spalten wir uns dabei beziehen, rufen wir innerhalb von is.na() die Funktion across() auf, der wir dann einen Vector übergeben, der die Spalten enthält. Diesen Vektor können wir entweder mit c() erstellen, oder in dem wir die erste und letzte Spalte mit einen Doppelpunkt verbinden (siehe Kapitel 2). Mit filter() können wir dann alle Fälle ausschließen, die die von uns gewünschte Mindestzahl an gültigen Antworten nicht erreichen.\n# berechnet die Anzahl der fehlenden Werte pro Person und schließt alle Fälle aus, die weniger als eine zuvor spezifizierte Anzhal gültiger Antworten haben\ndf &lt;- df |&gt;\n  mutate(summeNAs = rowSums(is.na(across(ersteSpalte:letzteSpalte)))) |&gt;\n  filter(summeNAs &lt;= zielwert)\n\n\n\n\n\n\nWarning\n\n\n\nBeim Ausschluss von Fällen basierend auf der Anzahl der fehlenden Werte handelt es sich um ein Vorgehen, dass die Datenqualität erhöhen soll. Dabei ist die Entscheidung, ab wann Fälle zu viele fehlende Werte haben meistens relativ wahrlos. Ganz grundsätzlich sollten Sie sich schon vor der Datenerhebung Gedanken darüber machen, ab wann ein Fall ausgeschlossen werden soll! Tun Sie das nicht, entsteht leicht der Eindurck, dass Sie solange Daten aussortiert haben, bis Ihnen die Ergebnisse gefallen!",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datenaufbereitung</span>"
    ]
  },
  {
    "objectID": "Datenaufbereitung.html#bearbeitungszeiten-überprüfen",
    "href": "Datenaufbereitung.html#bearbeitungszeiten-überprüfen",
    "title": "13  Datenaufbereitung",
    "section": "13.3 Bearbeitungszeiten überprüfen",
    "text": "13.3 Bearbeitungszeiten überprüfen\nBei den Bearbeitungszeten verhält es sich ähnlich wie bei fehlenden Werten. Ganz grundsätzlich kann es sinnvoll sein, Teilnehmerinnen und Teilnehmer auszuschließen, weil diese zu schnell (oder zu langsam) geantwortet haben. Allerdings sollte diese Entscheidung bereits vor der Datenerhebung getroffen werden!\nIn der Regel sollte diese Entscheidung davon abhngig sein, wie lang Ihre Befragung ist. Wenn Sie z.B. eine etwas längere Befragung durchführen, die im Pre-Test regelmäßig 10 Minuten gedauert hat, können sehr kurze Bearbeitungszeiten (z.B. weniger als 2 Minuten) darauf hinweisen, dass ein Fragebogen automatisiert ausgefüllt wurde. Wenn Sie ein Experiment oder eine Befragung durchführen, die einen Stimulus enthalten, können Sie sich auch daran gut orientieren. Z.B. sollten Sie helhörig werden, wenn eine Person nur 30 Sekunden für eine Seite im Fragebogen benötigt, die ein Video enthält, das 90 Sekunden lang ist.\nDie Daten, die Sie bei soscisurvey herunterladen, enthalten pro Seite im Fragebogen eine Spalte, in der die Bearbeitungsdauer angegeben wird (z.B. TIME001 für die erste oder TIME004 für die vierte Seite). Der darin gespeicherte Wert entspricht der Bearbeitungszeit der Seite in Sekunden. Außerdem gibt es eine Spalte namens TIME_SUM, in der die Gesamtbearbeitungszeit angegeben wird. Allerdings ist hier Vorsicht geboten: Die Werte sind bereits korrigiert und angepasst. Konkret ersetzt soscisurvey sehr lange Bearbeitungszeiten durch den Median aller anderen Bearbeitungszeiten für diese Seite, da davon ausgegangen wird, dass lange Zeiten eine Pause oder Unterbrechung der Befragung darstellen.\nSofern Se wissen, ab wann eine Bearbeitungszeit für Ihre Studie zu schnell oder zu langsam ist, können Sie die entsprechenden Fälle mit filter() aussortieren.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datenaufbereitung</span>"
    ]
  },
  {
    "objectID": "Datenaufbereitung.html#offene-antworten-überprüfen",
    "href": "Datenaufbereitung.html#offene-antworten-überprüfen",
    "title": "13  Datenaufbereitung",
    "section": "13.4 Offene Antworten überprüfen",
    "text": "13.4 Offene Antworten überprüfen\nGanz grundsätzlich sind offene Antworten schwer auszuwerten und sollten im Rahmen von standardisierten Befragungen nur in ausgewählten Situationen verwedent werden. Denn strenggenommen müssten Sie für jede Frage mit offener Antwort eine Inhaltsanalyse durchführen, um die Daten auszuwerten. Allerdings gibt es immer mal wieder Fälle, in denen offene Fragen oder Antwortenoptionen sinnvoll sein können, z.B. die Sonntagsfrage mit der Antwortoptionen “sonstige und zwar ____”.\nDie Aufbereitung solcher Abfragen ist sehr individuell. Ein Beispiel haben Sie in Kapitel 3 kennengelernt, als wir das Geburtsjahr in eine Angabe des Alters umgewandelt haben. Es gibt allerdings einige Funktionen, die für offene Fragen sinnvoll sein können:\ntable(), um einen Überblick über die Häufigkeit der einzelnen Antworten zu erhalten.\nunique(), um alle Antworten zu sehen, die mindestens einmal angegeben wurden.\nBeide Funktionen sind insbesondere dann sinnvoll, wenn Sie erstmal einen Überblick darüber bekommen wollen, was Ihre Befragten eigentlich in das offene Feld geschrieben haben.\nEine hilfreiche Funktion zum umcodieren von offenen Antworten ist case_when(). Damit können wir Werte immer dann verändern, wenn eine bestimmte Bedingung zutrifft. Diese Funktion wird innerhalb von mutate() aufgerufen. Konkret geben wir die Bedingung an (z.B. in Form eines Vergleichs) und schreiben dann hinter eine Tile (~), welchen Wert die neue Spalte (die wir mit mutate() anlegen) annehmen soll, falls die Bedingung zutrifft. Durch ein Komma können wir mehrere Bedingungen nacheinander prüfen und mit dem Argument .default können wir angeben, welcher Wert standardmäßig vergeben werden soll. Hierbei kann es sich um einen festen Wert handeln, aber auch eine Variable, die in einer anderen Spalte gespeichert ist.\n# nutzt case_when(), um eine fiktive Sonntagsfrage umzucodieren\ndf &lt;- df |&gt;\n  mutate(sonntagNeu = case_when(\n    sonntagAusgang == \"Tierschuttpartei\" ~ \"Tierschutzpartei\",\n    sonntagAusgang == \"SDP\" ~ \"SPD\",\n    .default = sonntagAusgang\n  ))",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datenaufbereitung</span>"
    ]
  },
  {
    "objectID": "Datenaufbereitung.html#umbenennen-beschriften-und-umcodieren",
    "href": "Datenaufbereitung.html#umbenennen-beschriften-und-umcodieren",
    "title": "13  Datenaufbereitung",
    "section": "13.5 Umbenennen, Beschriften und Umcodieren",
    "text": "13.5 Umbenennen, Beschriften und Umcodieren\nDie letzten Aspekte, die zu jeder Datenaufbereitung gehöhren, kennen Sie mittlerweise zur Genüge.\nMit rename() können Sie Spalten umbennen. Denken Sie daran, dass Sie mehrere Spalten auf einen Schlag umbenennen können, indem Sie die Kombinationen aus neuen und alten Namen mit einem Komma trennen. Beachten Sie auch, dass ein rename()-Befehl nur einmal ausgeführt werden kann, nachdem der Datensatz geladen wurde. Im Anschluss erkennt R die alten Namen nicht mehr und wird einen Fehler ausgeben. Im Zweifel lesen Sie den Datensatz einfach noch mal ein!\nMit factor() können Sie nominale und ordinale Daten beschriften. Achten Sie darauf, dass Sie wirklich nur solche Daten beschriften! Eine Frage die mit \"stimme überhaupt nicht zu\", \"stimme eher nicht zu\", \"teils-teils\", \"stimme eher zu\" oder \"stimme voll und ganz zu\" beantwortet wurde und die Sie als metrische AV verwenden wollen, sollte nicht beschriftet werden!\nDas Umcodieren von Variablen ist in zwei Fällen sinnvoll:\n\nWenn Sie eine Skala verwendet haben, bei die Items unterschiedlich gepolt sind. Also z.B. wenn hohe Werte in einigen Items ausdrücken, dass jemand eine bestimmte Einstellung sehr stark vertritt, während für andere Items niedrige Werte genau das angeben.\nWenn Sie möchten, dass hohe oder niedrige Werte eine bestimmte Bedeutung haben (z.B. starke Zustimmung = hoch).\n\nIn beiden Fällen können Sie mutate() nutzen, um eine recodierte Version des Items zu berechnen. Die Formel dazu haben Sie in Kapitel 12 schon mal kennengelernt. Ganz grundsätzlich können Sie bei metrischen Items nach folgendem Schema vorgehen: mutate(itemRecodiert = item*-1+x), wobei x der Anzahl der Antwortoptionen +1 entspricht (also z.B. 5+1 bei einer 5er-Skala oder 7+1 bei einer 7er-Skala).\nNatürlich gibt es auch viele Fälle, in denen es nötig sein kann, nicht metrische Variablen umzucodieren. Was ganz konkret zu tun ist, hängt jeweils von Ihren Daten und wie Sie diese auswerten wollen ab. Falls Sie mal vor einem Problem stehen, dessen Lösung Sie in diesem Kurs nicht kennengelernt haben, versuchen Sie sich ruhig mal an Google oder ChatGPT (bzw. dem Chatbot Ihrer Wahl). Für Google gilt: versuchen Sie Ihr Problem möglichst kurz und bündig zusammenzufassen, also z.B.: “Werte zu NAs umcodieren R”. Falls Sie einen Chatbot nutzen, versuchen Sie dagegen möglichst konkret zu werden. Beschreiben Sie Ihren Datensatz bzw. die Variable, die Sie umcodieren möchten und geben Sie vor allem sehr genau an, was am Ende herauskommen soll!",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datenaufbereitung</span>"
    ]
  },
  {
    "objectID": "Datenaufbereitung.html#umbenennen-beschriften-umcodieren-und-berechnen",
    "href": "Datenaufbereitung.html#umbenennen-beschriften-umcodieren-und-berechnen",
    "title": "13  Datenaufbereitung",
    "section": "13.5 Umbenennen, Beschriften, Umcodieren und Berechnen",
    "text": "13.5 Umbenennen, Beschriften, Umcodieren und Berechnen\nDie letzten Aspekte, die zu jeder Datenaufbereitung gehöhren, kennen Sie mittlerweise zur Genüge.\n\n13.5.1 Umbenennen\nMit rename() können Sie Spalten umbennen. Denken Sie daran, dass Sie mehrere Spalten auf einen Schlag umbenennen können, indem Sie die Kombinationen aus neuen und alten Namen mit einem Komma trennen. Beachten Sie auch, dass ein rename()-Befehl nur einmal ausgeführt werden kann, nachdem der Datensatz geladen wurde. Im Anschluss erkennt R die alten Namen nicht mehr und wird einen Fehler ausgeben. Im Zweifel lesen Sie den Datensatz einfach noch mal ein!\n\n\n13.5.2 Beschriften\nMit factor() können Sie nominale und ordinale Daten beschriften. Achten Sie darauf, dass Sie wirklich nur solche Daten beschriften! Eine Frage die mit \"stimme überhaupt nicht zu\", \"stimme eher nicht zu\", \"teils-teils\", \"stimme eher zu\" oder \"stimme voll und ganz zu\" beantwortet wurde und die Sie als metrische AV verwenden wollen, sollte nicht beschriftet werden!\n\n\n13.5.3 Umcodieren\nDas Umcodieren von Variablen ist in zwei Fällen sinnvoll:\n\nWenn Sie eine Skala verwendet haben, bei die Items unterschiedlich gepolt sind. Also z.B. wenn hohe Werte in einigen Items ausdrücken, dass jemand eine bestimmte Einstellung sehr stark vertritt, während für andere Items niedrige Werte genau das angeben.\nWenn Sie möchten, dass hohe oder niedrige Werte eine bestimmte Bedeutung haben (z.B. starke Zustimmung = hoch).\n\nIn beiden Fällen können Sie mutate() nutzen, um eine recodierte Version des Items zu berechnen. Die Formel dazu haben Sie in Kapitel 12 schon mal kennengelernt. Ganz grundsätzlich können Sie bei metrischen Items nach folgendem Schema vorgehen: mutate(itemRecodiert = item*-1+x), wobei x der Anzahl der Antwortoptionen +1 entspricht (also z.B. 5+1 bei einer 5er-Skala oder 7+1 bei einer 7er-Skala).\nNatürlich gibt es auch viele Fälle, in denen es nötig sein kann, nicht metrische Variablen umzucodieren. Ein relativ häufiger Fall ist das Zusammenfassen von Ausprägungen. Das ist immer dann sinnvoll, wenn Sie sehr vielfältige Antwortoptionen vorgegeben haben, von denen dann viele nur sehr selten angekreuzt wurden. Auch hier können Sie die case_when()-Funktion verwenden, die Sie oben bereits kennengelernt haben. Das nachfolgende Beispiel zeigt, wie der Code aussehen würde, wenn Sie von insgesamt 10 Ausprägungen 4 zusammenfassen würden:\n# nutzt case_when(), um Antwortoptinen einer fiktive Abfrage zusammenzufassen\ndf &lt;- df |&gt;\n  mutate(variableZusammengefasst = case_when(\n    ausgangsVariable == \"Ausprägung 1\" ~ \"Ausprägung 2\",\n    ausgangsVariable == \"Ausprägung 3\" ~ \"Ausprägung 4\",\n    ausgangsVariable == \"Ausprägung 5\" ~ \"Ausprägung 6\",\n    ausgangsVariable == \"Ausprägung 7\" ~ \"Ausprägung 8\",\n    .default = ausgangsVariable\n  ))\nNatürlich geht das auch mit Variablen, die in Zahlenform vorliegen. Und wie konkret Sie die Werte zusammenfassen hängt immer davon ab, was Sie eigentlich erhoben haben und was Sie damit auswereten wollen.\n# nutzt case_when(), um Antwortoptinen einer fiktive Abfrage zusammenzufassen\ndf &lt;- df |&gt;\n  mutate(variableZusammengefasst = case_when(\n    ausgangsVariable == 1 ~ 2,\n    ausgangsVariable == 3 ~ 1,\n    ausgangsVariable == 5 ~ 1,\n    ausgangsVariable == 7 ~ 3,\n    .default = ausgangsVariable\n  ))\nWeiterhin gibt es unzählige weitere Möglichkeiten, Variablen umzucodieren, z.B. das Deklarieren von fehlenden Werten, wie wir es im Fall der Geschlechtsabfrage im Kurs kennengelernt haben. Was für eine bestimmte Umcodierung ganz konkret zu tun ist, hängt jeweils von Ihren Daten und wie Sie diese auswerten wollen ab. Falls Sie mal vor einem Problem stehen, dessen Lösung Sie in diesem Kurs nicht kennengelernt haben, versuchen Sie sich ruhig mal an Google oder ChatGPT (bzw. dem Chatbot Ihrer Wahl). Für Google gilt: versuchen Sie Ihr Problem möglichst kurz und bündig zusammenzufassen, also z.B.: “Werte zu NAs umcodieren R”. Falls Sie einen Chatbot nutzen, versuchen Sie dagegen möglichst konkret zu werden. Beschreiben Sie Ihren Datensatz bzw. die Variable, die Sie umcodieren möchten und geben Sie vor allem sehr genau an, was am Ende herauskommen soll!\n\n\n13.5.4 Variablen neu berechnen\nIm Lauf des Kurses haben wir viele Variablen neu berechnet. Ganz grundsätzlich sind wir dabei immer gleich vorgegangen: mutate(neueVariable = Formel/Funktion/Berechnung)\nEinen Sonderfall, den Sie kennengelernt haben ist die Bildung von sogenannten Indizes, insbesondere von Mittelwertindizes. Wir haben Sie wie folgt berechnet:\n# berechnet einen Mittelwertindex für die 5 Items\ndf &lt;- df |&gt;\n  rowwise() |&gt;\n  mutate(index = mean(c(item1, item2, item3, item4, item5))) |&gt;\n  ungroup()\nBeachten Sie, dass Sie vor der Berechnung eines solches Indizes immer Cronbachs Alpha berechnen sollten (siehe Kapitel 12).\nEine weitere Berechnung, die Sie gelgentlich brauchen werden, ist das Summieren von angekreuzten Antwortoptionen bei Fragen mit Mehrfachauswahl. Bevor Sie mit der Berechnung loslegen können, sollten Sie darauf achten, dass die Daten so codiert sind, dass der Wert 1 jeweils einer Auswahl einer Option entspricht. Wurde eine Option nicht ausgewählt, sollte der Wert 0 eingetragen sein. Da jede Option im Datensatz eine eigene Spalte erhält, können Sie die Anzahl der ausgewählten Optionen dann ganz einfach berechnen, indem Sie die Werte in den Spalten addieren. Wie oben, als wir die Anzahl fehlender Werte berechnet haben, nutzen wir dafür rowSums().\n# berechnet einen Summenindex für Mehrfachantworten\ndf &lt;- df |&gt;\n  mutate(summe = rowSums(across(c(option1, option2, option3, option4))))) |&gt;\nAnsonsten gilt hier dasselbe wie beim Umcodieren: Es gibt unzählige Möglichkeiten, neue Variablen zu berechnen. Was jeweils sinnvoll ist, hängt davon ab, was Sie genau vorhaben!",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datenaufbereitung</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#deskription-und-visualisierung-der-daten",
    "href": "Zusammenhänge zwischen ordinalen und metrischen Variablen.html#deskription-und-visualisierung-der-daten",
    "title": "7  Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen",
    "section": "7.2 Deskription und Visualisierung der Daten",
    "text": "7.2 Deskription und Visualisierung der Daten\nWir starten wie im letzten Kapitel damit, unsere R-Umgebung vorzubereiten, indem wir das tidyverse laden, den Datensatz einlesen und die Optionen so ändern, dass kleine Zahlen in einem uns gewohnten Format angezeigt werden.\n\n# Lädt das tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Liest die Daten ein\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Stellt ein, dass sehr kleine Zahlen normal dargestellt werden\noptions(scipen = 999)\n\nEine gute Möglichkeit, den Zusammenhang zwischen zwei metrischen Variablen grafisch darzustellen, ist durch ein sogenanntes Streudiagramm bzw. eine Punktewolke. Dabei wird eine Variable auf der x- und die andere auf der y-Achse dargestellt. Schauen wir uns das einmal am Beispel der Spalten A202_01 und A202_02 an, in denen das Interesse an Nachrichten über lokale bzw. nationale Geschehnisse abgefragt wurde.1 Zunächst benennen wir die beiden Spalten um. Anschließend rufen wir ggplot() auf, geben dort innerhalb von aes() an, welche Variable wir auf der x- bzw. y-Achse darstellen wollen. Dann müssen wir nur noch geom_point() ergänzen. Mit theme_minimal() und labs() verschönern wir unseren Plot gleich noch ein wenig.\n\n# Benennt die Spalten A202_01 und A202_02 um\ndf_lokal &lt;- df_lokal |&gt;  \n  rename(interesseLokal = A202_01,\n         interesseDE = A202_02)\n\n# Erstellt ein Streudiagramm der beiden Spalten\nstreudiagrammInt &lt;- df_lokal |&gt;\n  ggplot(aes(x = interesseLokal, y = interesseDE))+\n  geom_point()+\n  theme_minimal()+\n  labs(x = \"Interesse an lokalen Geschehnissen\", y = \"Interesse an nationalen Geschehnissen\")\n\nstreudiagrammInt\n\n\n\n\n\n\n\n\nDas sieht gar nicht schlecht aus. Wir sehen allerdings, dass relativ viele Befragte die Skala vollständig ausgereizt haben und den jeweiligen Maximalwert (101) angegeben haben. Daran können wir auch ein Verhalten von geom_point() erahnen, das tendenziell problematisch ist. Und zwar werden identische Datenpunkte einfach übereinander gelegt. Wenn also zwei Personen die exakt gleichen Antworten gegeben haben, zeigt geom_point() nur einen Punkt an.\nSchauen wir uns das mal am Beispiel der Spalten A208_01 und A208_03 an. In beiden Spalten wurden Aspekte der politischen Selbstwirksamkeit abgefragt, jeweils bezogen auf den eigenen Wohnort. Die konkreten Formulierungen lauteten:\n\nA208_01: Wichtige Fragen der Lokalpolitik kann ich gut verstehen und einschätzen.\nA208_03: Ich traue mir zu, mich an einem Gespräch über Fragen der Lokalpolitik aktiv zu beteiligen.\n\nSchauen wir uns einmal ein Streudiagramm dieser beiden Variablen an. Da die Items Teil einer größeren Abfrage waren und üblicherweise nicht einzeln ausgewertet werden würden, sind die Beschriftungen hier sehr pragmatisch gewählt.\n\n# Benennt die Spalten A208_01 und A208_03 um\ndf_lokal &lt;- df_lokal |&gt;  \n  rename(polSelbstw1 = A208_01,\n         polSelbstw3 = A208_03)\n\n# Erstellt ein Streudiagramm der beiden Spalten\n\nstreudiagrammSelbstw &lt;- df_lokal |&gt;\n  ggplot(aes(x = polSelbstw1, y = polSelbstw3))+\n  geom_point()+\n  theme_minimal()+\n  labs(x = \"lokale politische Selbstwirksamkeit Item 1\", y = \"lokale politische Selbstwirksamkeit Item 3\")\n\nstreudiagrammSelbstw\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHier wird das Problem an geom_point() deutlich. Die Grafik verrät uns herzlich wenig. Wir haben nach wie vor keinerlei Vorstellung davon, wie stark der Zusammenhang sein könnte. Wir erfahren lediglich, dass jede mögliche Wertekombination im Datensatz enthalten ist. Es gibt also z.B. Leute, die auf eines der Items mit 1 (= “stimme überhaupt nicht zu”) und auf das andere mit 5 (= “stimme sehr zu”) geantwortet haben.\nIn solchen Fällen ist es sinnvoll, die Alternative geom_jitter() zu nutzen. Jitter ist Englisch für zittern und genau das tut die Funktion: Sie verschiebt die einzelnen Punkte minimal nach oben, unten, rechts und links (lässt sie also zittern), sodass wir besser erkennen können, wie die Daten verteilt sind. Um das umzusetzen, müssen wir nur geom_point() durch geom_jitter() ersetzen.\n\n# Erstellt ein Streudiagramm der beiden Spalten mit geom_jitter()\n\nstreudiagrammSelbstw &lt;- df_lokal |&gt;\n  ggplot(aes(x = polSelbstw1, y = polSelbstw3))+\n  geom_jitter()+\n  theme_minimal()+\n  labs(x = \"lokale politische Selbstwirksamkeit Item 1\", y = \"lokale politische Selbstwirksamkeit Item 3\")\n\nstreudiagrammSelbstw\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWir sehen nun relativ eindeutig, dass es einen Zusammenhang zwischen den beiden Variablen zu geben scheint.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Korrelationen</span>"
    ]
  },
  {
    "objectID": "Objekte, Daten, Funktionen.html#r-beenden",
    "href": "Objekte, Daten, Funktionen.html#r-beenden",
    "title": "2  Objekte, Daten, Funktionen",
    "section": "2.4 R beenden",
    "text": "2.4 R beenden\nWenn Sie R bzw. RStudio beenden, fragt Sie das Programm, ob Sie den sogenannten Workspace speichern möchten. Dabei handelt es sich um alle Objekte (u.ä.), die Sie in R angelegt haben. In der Regel sollten Sie das ablehnen. Andernfalls wird ihr Environment (siehe letztes Kapitel) im Lauf der Zeit immer größer und nimmt zu viel Speicherplatz ein.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objekte, Daten, Funktionen</span>"
    ]
  },
  {
    "objectID": "Skalenniveaus und deskriptive Datenanalyse.html#deskriptive-datenanalyse",
    "href": "Skalenniveaus und deskriptive Datenanalyse.html#deskriptive-datenanalyse",
    "title": "4  Skalenniveaus und deskriptive Datenanalyse",
    "section": "4.2 Deskriptive Datenanalyse",
    "text": "4.2 Deskriptive Datenanalyse\nIn diesem Abschnitt werden wir uns anschauen, wie wir die verschiedenen Lagemaße aus dem Video in R berechnen können. Außerdem lernen Sie, wie Sie Daten in Abhängigkeit ihres Skalenniveaus visualisieren können.\n\n4.2.1 Modus\nR hat nach der Installation keine Funktion, die den Modus (oder die Modi) einer Verteilung ermittelt. Wir können aber das Paket DescTools installieren, das eine solche Funktion enthält.\n\n# Installiert das Paket \"DescTools\", falls es noch nicht installiert ist und lädt es anschließend. Andernfalls wird es nur geladen.\n\nif(!require(DescTools)){\n  install.packages(\"DescTools\")\n  library(DescTools)\n  }\n\nLade nötiges Paket: DescTools\n\n\nAnschließend laden wir wieder den Datensatz und wandeln die Geschlechtsabfrage um. Den Code kennen Sie schon aus dem letzten Kapitel. Da für dieses Kapitel ein neues Skript sinnvoll ist, führen wir auch den Code noch mal aus.\n\n# Lädt das tidyverse\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Einlesen der Daten \n\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Erstellt eine Spalte aus der Geschlechtsabfrage. Erst wird der Wert \"keine Angabe\" als fehlend deklariert, dann wird ein Faktor mit den übrigen drei Kategorien erstellt\n\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geschlechtMitNAs = na_if(A602, 4)) |&gt;\n  mutate(geschlechtMitNAs = factor(geschlechtMitNAs, labels = c(\"männlich\", \"weiblich\", \"divers\")))\n\nAnschließend können wir die Mode()-Funktion aus dem DescTools-Paket verwenden. Um damit den Modus zu berechnen, müssen wir die Funktion innerhalb der dplyr-Funktion summarise() aufrufen. Diese verdichtet Datensätze. Schauen wir uns einmal an, was passiert, wenn wir Mode() einfach entsprechend ausführen:\n\n#Versucht den Modus der Spalte geschlechtMitNAs zu berechnen\n\nmodus_geschlecht &lt;- df_lokal |&gt;\n  summarise(Modus = Mode(geschlechtMitNAs))\n\nmodus_geschlecht\n\n  Modus\n1    NA\n\n\nWie Sie sehen, gibt die Funktion ein NA zurück. Das ist Rs Art uns zu sagen, dass eine Berechnung nicht durchgeführt werden kann, weil die Daten fehlende Werte enthalten. Wir können das Problem beheben, indem wir der Mode()-Funktion das Argument na.rm = TRUE übergeben. na.rm steht für NA remove. Mit dem Wert TRUE sagen wir also, dass fehlende Werte vor der Berechnung entfernt werden sollen.\n\n#Berechnet den Modus der Spalte geschlechtMitNAs \n\nmodus_geschlecht &lt;- df_lokal |&gt;\n  summarise(Modus = Mode(geschlechtMitNAs, na.rm = TRUE))\n\nmodus_geschlecht\n\n     Modus\n1 weiblich\n\n\nNun sehen wir, dass mit 996 Fällen der Wert weiblich am häufigsten vorkommt.\nSchauen wir uns als nächstes an, wie Sie nominale Daten bzw. den Modus visualisieren können. Im letzten Kapitel wurde in Kürze das tidyverse vorgestellt und auf das Paket ggplot2 verwiesen, mit dem wir Plots erstellen können.\nIm Folgenden werden Schritt für Schritt ein einfaches Balkendiagramm erstellen und es nach und nach verbessern. Wir fangen damit an, dass wir das tidyverse laden. Wir erstellen dann ein neues Objekt für unseren Plot. Dazu geben wir unseren Datensatz mit der Pipe an die Funktion ggplot() weiter. Das ist die Hauptfunktion des Pakets. Üblicherweise wird innerhalb dieser Funktion die Funktion aes() (für aesthetics) aufgerufen, in der wir je nach Diagramm angeben, welche Spalten auf der x- und y-Achse dargestellt werden sollen. Für ein Balkendiagramm benötigen wir nur die x-Achse, geben also x = geschlechtMitNAs an. Damit sagen wir erstmal nur, welche Spalte wir darstellen wollen, aber noch nichts darüber, was für eine Darstellung es werden soll. Um die Art des Diagramms festzulegen, gibt es in ggplot2 unzählige Funktionen, die alle mit geom_ beginnen (für geometry). Hinter dem Unterstrich folgt dann der (englische) Name des Diagrammtyps. In unserem Fall eines Balkendiagramms, heißt die entsprechende Funktion geom_bar(). Anders als sonst, verbinden wir die Funktionen in ggplot2 nicht über die Pipe, sondern ein Pluszeichen. In der Summe sieht der Code dann so aus:\n\n# Erstellt ein einfaches Balkendiagramm der Geschlechtsabfrage\n\nplotGeschlecht &lt;- df_lokal |&gt;\n  ggplot(aes(x = geschlechtMitNAs))+\n  geom_bar()\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nDas ist - nun ja - sagen wir mal, es ist nicht sonderlich hübsch. Folgende Dinge fallen auf:\n\nEs gibt einen Balken für NAs, dabei enthalten diese ja per definitionem keine Informationen.\nDie Angabe absoluter Häufigkeiten ist etwas problematisch; besser wären relative Häufigkeiten.\nDie Werte der einzelnen Kategorien können wir aktuell nur schätzen.\nDer Hintergrund und die Farbe der Balken sind nicht sonderlich ansehnlich.\nDie Beschriftungen der Achsen (geschlechtMitNAs und count) sind nicht gerade selbsterklärend.\n\nDiese Liste können wir nun einfach abarbeiten. Manchmal ist es sinnvoll, die Daten noch ein bisschen aufzubereiten, bevor wir sie an ggplot() übergeben. Dafür deklarieren wir ein neues Objekt, das die Daten enthalten soll. Als Erstes können wir dann mit der filter()-Funktion, die Sie im letzten Kapitel kennengelernt haben, die NAs entfernen. Anschließend nutzen wir die group_by()-Funktion aus dplyr. Diese Funktion sorgt dafür, dass die nachfolgenden Funktionen nicht auf den gesamten Datensatz angewendet werden, sondern auf die jeweiligen Gruppen (männlich, weiblich, divers). Anschließend nutzen wir wieder die summarise()-Funktion. Dort können wir die Prozentwerte der einzelnen Gruppen mit der folgenden Formel berechnen: n()/nrow(df_lokal)*100. n() gibt die Anzahl der Fälle zurück. Durch das Aufrufen von group_by() sind es hier die Fälle pro Ausprägung von geschlechtMitNAs. Diesen Wert teilen wir durch nrow(df_lokal). Diese Funktion gibt die Anzahl der Zeilen im kompletten Datensatz zurück. Das Ergebnis multiplizieren wir mit 100, um einen Prozentwert zu erhalten. Als letztes nutzen wir mutate(), um die Prozentwerte auf zwei Nachkommastellen zu runden. Das Ergebnis sehen Sie unten:\n\n# Erstellt einen reduzierten Datensatz, der die relativen Häufigkeiten der Ausprägungen von geschlechtMitNAs enthält\n\ndf_plot_geschlecht &lt;- df_lokal |&gt;\n  filter(!is.na(geschlechtMitNAs)) |&gt;\n  group_by(geschlechtMitNAs) |&gt;\n  summarise(prozent = n()/nrow(df_lokal)*100) |&gt;\n  mutate(prozent = round(prozent, 2))\n\ndf_plot_geschlecht\n\n# A tibble: 3 × 2\n  geschlechtMitNAs prozent\n  &lt;fct&gt;              &lt;dbl&gt;\n1 männlich           44.0 \n2 weiblich           54.0 \n3 divers              0.38\n\n\nWenn wir diesen Datensatz an ggplot() übergeben und versuchen, ein Balkendiagramm zu erstellen, haben wir zwar keinen Balken für NA mehr, aber dafür ein neues Problem: Alle Balken sind gleich hoch.\n\n# Versucht, ein Balkendiagramm der Geschlechtsabfrage ohne NAs zu erstellen\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs))+\n  geom_bar()\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nUm dieses Problem zu beheben, können wir angeben, dass die y-Achse den Wert aus der Spalte prozent darstellen soll. Zusätzlich müssen wir beim Aufrufen von geom_bar() das Argument stat = \"identity\" angeben. Damit sagen wir der Funktion, dass sie nichts berechnen muss, sondern wir schon die finalen Werte als y-Wert angegeben haben.\n\n# Erstellt ein Balkendiagramm der relativen Häufigkeiten der Geschlechtsabfrage ohne NAs\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = prozent))+\n  geom_bar(stat = \"identity\")\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nAls nächstes wollen wir die Werte beschriften. Dazu fügen wir unserem Plot eine neue geom-Funktion hinzu, nämlich geom_text(). Dieser Funktion übergeben wir auch wieder aes(), um zu bestimmen, was angezeigt werden soll. Das Argument für Beschriftungen lautet label. Als Wert können wir die Spalte prozent angeben. Die Standardeinstellung ist, dass der Wert an seine Position auf der y-Achse geschrieben wird. Das ist etwas unpraktisch, da dort ja auch die Balken aufhören. Indem wir geom_text() das Argument vjust (für vertical justification) und den Wert -0,5 (in R reicht -.5) übergeben, können wir die Beschriftung leicht nach oben verschieben.\n\n# Erstellt ein beschriftetes Balkendiagramm der relativen Häufigkeiten der Geschlechtsabfrage ohne NAs\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = prozent))+\n  geom_bar(stat = \"identity\")+\n  geom_text((aes(label = prozent)), vjust = -.5)\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nAls nächstes passen wir die Farben etwas an. Gleich vorab: ggplot2 bietet nahezu unendlich viele Möglichkeiten, das Aussehen von Diagrammen anzupassen. Hier machen wir es uns relativ einfach und nutzen vorhandene Funktionen. So wie es viele geom_Funktionen gibt, gibt es auch einige theme_ Funktionen. Hier ergänzen wir theme_minimal() zu unserem Diagramm. Damit wird zwar der Hintergrund, nicht aber die Farbe der Balken angepasst. Das können wir tun, indem wir der geom_bar()-Funktion das Argument fill mit einer Farbe übergeben. Diese müssen auf Englisch angegeben werden. Hier verwenden wir ein helles Grau.\n\n# Erstellt ein beschriftetes Balkendiagramm der relativen Häufigkeiten der Geschlechtsabfrage ohne NAs\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = prozent))+\n  geom_bar(stat = \"identity\", fill = \"lightgrey\")+\n  geom_text((aes(label = prozent)), vjust = -.5)+\n  theme_minimal()\n\nplotGeschlecht\n\n\n\n\n\n\n\n\nAls Letztes ändern wir die Achsenbeschriftungen. Dazu fügen wir die labs()-Funktion hinzu. Mit den Argumenten x und y können wir die Beschriftung anpassen.\n\n# Erstellt ein beschriftetes Balkendiagramm der relativen Häufigkeiten der Geschlechtsabfrage ohne NAs\n\nplotGeschlecht &lt;- df_plot_geschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = prozent))+\n  geom_bar(stat = \"identity\", fill = \"lightgrey\")+\n  geom_text((aes(label = prozent)), vjust = -.5)+\n  theme_minimal()+\n  labs(x = \"Geschlecht\", y = \"relative Häufigkeit in Prozent\")\n\nplotGeschlecht\n\n\n\n\n\n\n\n\n\n\n4.2.2 Median\nAnders als beim Modus gibt es für den Median eine R-Funktion, die wir ohne Weiteres nutzen können: die median()-Funktion. Auch hier müssen wir darauf achten, na.rm = TRUE anzugeben, damit der Median berechnet werden kann. Im folgenden Beispiel berechnen wir den Median der Spalte A203_06. In der Spalte ist codiert, wie häufig die Befragten Lokalzeitungen lesen (von 1 = “nie” bis 6 = “mehrmals täglich”).\n\n# Benennt die Spalte A203_06 in lokalzeitung um\ndf_lokal &lt;- df_lokal |&gt;\n rename(lokalzeitung = A203_06) \n\n# Berechnet den Median der Lokalzeitungsnutzung\nmedian_lokalzeitung &lt;- df_lokal |&gt;\n  summarise(median = median(lokalzeitung, na.rm = TRUE))\n\nmedian_lokalzeitung\n\n  median\n1      4\n\n\nDer Median ist 4, was einer Nutzung mehrmals pro Woche entspricht. Es gibt viele verschiedene Möglichkeiten, den Median bzw. die Verteilung von ordinalen Daten zu visualisieren. Bei wenigen Ausprägungen, so wie im Fall der Nutzung von Lokalzeitungen, können wir ähnlich vorgehen wie beim Modus oben. Dazu müssen wir die Spalte in einen Faktor umwandeln, bevor wir sie an ggplot() übergeben. Anschließend gehen wir ähnlich vor wie oben, allerdings bleiben wir der Einfachheit halber bei absoluten Häufigkeiten. Eine relevante Ergänzung nehmen wir aber vor: Mit der geom_vline()-Funktion (für vertical line) und dem Argument xintercept = 4, können wir eine Linie hinzufügen, die den Median anzeigt.\n\n# Wandelt die Nutzung von Lokalzeitungen in einen Faktor um und plottet die Daten als Balkendiagramm\nbalkenPlot_lokalzeitung &lt;- df_lokal |&gt;\n  mutate(lokalzeitungFaktor = factor(lokalzeitung, labels = c(\"nie\", \"weniger als ein Mal im Monat\",\n                                                              \"mehrmals im Monat\", \"mehrmals in der Woche\",\n                                                              \"täglich\", \"mehrmals täglich\"))) |&gt;\n  filter(!is.na(lokalzeitungFaktor)) |&gt;\n  ggplot(aes(x = lokalzeitungFaktor))+\n  geom_bar(fill = \"lightgrey\")+\n  theme_minimal()+\n  labs(x = \"Nutzung von Lokalzeitungen\", y = \"Häufigkeit\")+\n  geom_vline(xintercept = 4, linetype = \"dashed\")\n\nbalkenPlot_lokalzeitung\n\n\n\n\n\n\n\n\nDas ist schon sehr nah an einer akzeptablen Darstellung, aber die Wertbeschriftungen sehen furchtbar aus! Hier müssen wir etwas Hand anlegen und Zeilenumbrüche einfügen. Die Beschriftungen können wir mit der Funktion scale_x_discrete() und darin mit dem labels-Argument anpassen. Um einen Zeilenumbruch hinzuzufügen, können wir an einer beliebigen Stelle in einem string \\n ergänzen. Diese Funktion können wir mit einem + unserem bisherigen Objekt balkenPlot_lokalzeitung hinzufügen:\n\n# Verändert die Beschriftungen der Balken\n\nbalkenPlot_lokalzeitung &lt;- balkenPlot_lokalzeitung+\n  scale_x_discrete(labels=c(\"nie\" = \"nie\", \n                            \"weniger als ein Mal im Monat\" = \"weniger als\\n ein Mal im Monat\",\n                            \"mehrmals im Monat\" = \"mehrmals\\n im Monat\", \n                            \"mehrmals in der Woche\" = \"mehrmals\\n in der Woche\", \n                            \"täglich\" = \"täglich\",\n                            \"mehrmals täglich\" = \"merhmals\\n täglich\"))\n\nbalkenPlot_lokalzeitung\n\n\n\n\n\n\n\n\nEine andere häufige Visualisierung von ordinalen Daten ist der sogenannte Boxplot, der mit der Funktion geom_boxplot() erstellt wird. Beachten Sie im Beispiel unten, dass wir die Spalte lokalzeitung im Aufruf von ggplot() bzw. darin aes() als y-Wert definieren. Wir könnten auch den x-Wert wählen, dann würde der Boxplot auf der Seite liegen.\n\nboxplot_lokalzeitung &lt;- df_lokal |&gt;\n  filter(!is.na(lokalzeitung)) |&gt;\n  ggplot(aes(y = lokalzeitung))+\n  geom_boxplot()+\n  theme_minimal()+\n  labs(y = \"Häufigkeit der Lokalzeitungsnutzung\")\n\nboxplot_lokalzeitung\n\n\n\n\n\n\n\n\nWas Sie hier sehen, ist erstmal nicht sonderlich hübsch, sollte aber dennoch kurz erklärt werden: Die dicke schwarze Linie beim Wert 4 zeigt den Median. Die eingekasteten Bereiche darüber und darunter zeigen das 75. bzw. das 25. Quartil. Oder einfach gesagt: 25 % der Befragten haben den Wert 2 oder weniger angegeben und weitere 25 % den Wert 5 oder mehr. Die Linien nach unten und oben gehen bis zum Minimum bzw. Maximum.\nFolgende Probleme hat die Darstellung:\n\nDer Boxplot ist sehr breit. Das sieht furchtbar aus!\nDie y-Achse ist nur spärlich beschriftet.\nDie x-Achse hat eine Beschriftung, die überhaupt nicht nachvollziehbar ist.\nDas Koordinatensystem hat senkrechte Linien, die keinen Sinn ergeben, da wir ja eigentlich gar nichts auf der x-Achse abbilden.\n\nFangen wir mit den ersten beiden Problemen an. Wir sehen auf der Grafik oben, dass auf der x-Achse der Bereich von x = -0,4 bis x = 0,4 (was auch immer diese Werte bedeuten mögen!) dargestellt ist und der Boxplot genau diesen Bereich einnimmt. Wir können die Funktion xlim() nutzen, um den dargestellten Bereich zu erweitern. Dazu müssen wir einfach nur zwei Werte angeben, z.B. -1 und 1.\nUm die y-Achse etwas schöner zu machen, können wir eine ähnliche Funktion nutzen, wie im Balkendiagramm oben: scale_y_continuous(). Mit dem Argument breaks können wir angeben, welche Werte beschriftet sein sollen. Hier können wir 1:6 angeben, um alle ganzen Zahlen zwischen 1 und 6 anzeigen zu lassen.\n\nboxplot_lokalzeitung &lt;- boxplot_lokalzeitung+  \n  xlim(-1,1)+\n  scale_y_continuous(breaks = 1:6)\n\n\nboxplot_lokalzeitung\n\n\n\n\n\n\n\n\nDas sieht schon etwas besser aus. Wir haben nun aber ein neues Problem: Auf der y-Achse werden nun waagerechte Linien zwischen den ganzen Zahlen angezeigt, dabei konnte die Variable diese Werte gar nicht annehmen.\nDieses Problem können wir gemeinsam mit den übrigen Punkte von oben in einem Rutsch erledigen, indem wir die theme()-Funktion nutzen. Diese Funktion kann zugegebenermaßen etwas abschreckend sein. Sie können damit die Darstellung aller einzelnen Elemente eines Plots anpassen oder - und das ist für uns hier aber auch generell häufig entscheidend - sie entfernen! Das Schema ist immer gleich: Sie geben ein Element an und schreiben hinter ein Gleichheitszeichen, wie es dargestellt werden soll. Geben Sie dort element_blank() an, wird das Element entfernt. Das nutzen wir hier, um die folgenden Elemente zu entfernen:\n\nDie Beschriftung der x-Achse –&gt; das Element heißt axis.text.x\nDie vertikalen Linien im Koordinatensystem –&gt; die Elemente heißen panel.grid.major.x und panel.grid.minor.x\nDie horizontalen Linien zwischen den ganzen Zahlen –&gt; das Element heißt panel.grid.minor.y\n\n\nboxplot_lokalzeitung &lt;- boxplot_lokalzeitung+\n  theme(axis.text.x = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank())\n\nboxplot_lokalzeitung\n\n\n\n\n\n\n\n\nDeutlich häufiger werden Sie Boxplots begegnen (oder selbst erstellen), wenn es um die Darstellung mehrerer Gruppen geht. Im folgenden Beispiel sehen Sie einen Boxplot für die Nutzung von Lokalzeitungen nach Geschlecht der Befragten. Im wesentlichen ist er sehr ähnlich wie der Plot oben, allerdings müssen wir diesmal keine Elemente entfernen. Vor allem aber müssen wir im Aufruf von ggplot() bzw. darin aes() angeben, dass das Geschlecht der Befragten auf der x-Achse dargestellt werden soll.\n\nboxplot_lokalzeitung_geschlecht &lt;- df_lokal |&gt;\n  filter(!is.na(lokalzeitung)) |&gt;\n  filter(!is.na(geschlechtMitNAs)) |&gt;\n  ggplot(aes(y = lokalzeitung, x = geschlechtMitNAs))+\n  geom_boxplot()+\n  theme_minimal()+\n  scale_y_continuous(breaks = 1:6)+\n  labs(y = \"Häufigkeit der Lokalzeitungsnutzung\", x = \"Geschlecht\")\n\nboxplot_lokalzeitung_geschlecht\n\n\n\n\n\n\n\n\nDas Ergebnis sieht schon sehr ordentlich aus. Der einsame Punkt in der Spalte divers steht für einen Ausreißer: Eine Person liest deutlich häufiger Lokalzeitungen als andere Menschen, die sich nicht-binär identifizieren. In erster Linie liegt das an der sehr geringen Fallzahl in der Gruppe (vgl. das Balkendiagramm von oben). Um das sichtbar zu machen, können wir die Rohdaten anzeigen lassen. Das geht grundsätzlich mit geom_point(), hat aber den Nachteil, dass dann alle Datenpunkte an derselben Stelle dargestellt werden:\n\nboxplot_lokalzeitung_geschlecht+\n  geom_point()\n\n\n\n\n\n\n\n\nDas hilft uns nicht wirklich weiter. Eine bessere Möglichkeit ist geom_jitter(). Damit werden die Rohdaten etwas gestreut geplottet. Durch das Argument alpha = .25 können wir die Punkte zusätzlich etwas transparent machen.\n\nboxplot_lokalzeitung_geschlecht &lt;- boxplot_lokalzeitung_geschlecht+\n  geom_jitter(alpha = .25)\n\nboxplot_lokalzeitung_geschlecht\n\n\n\n\n\n\n\n\n\n\n4.2.3 Mittelwert und Standardabweichung\nFür Mittelwert und Standardabweichung gibt es ebenfalls zwei R-Funktionen, die wir direkt nutzen können: mean() und sd(). Auch hier müssen wir darauf achten, dass wir na.rm = TRUE angeben.\nIm folgenden Beispiel berechnen wir zunächst das Alter der Befragten mit dem Code aus dem letzten Kapitel. Anschließend nutzen wir summarise(), um den Datensatz auf Mittelwert und Standardabweichung des Alters zu reduzieren. Innerhalb von summarise() berechnen wir mit mean() den Mittelwert und mit sd() die Standardabweichung. Beides runden wir mit round() auf zwei Nachkommastellen.\n\n# Berechnet das Alter der Befragten\ndf_lokal &lt;- df_lokal |&gt;\n  mutate(geburtsjahr = str_trim(A601_01)) |&gt;\n  mutate(geburtsjahr = str_sub(geburtsjahr, -4)) |&gt;\n  mutate(geburtsjahr = as.integer(geburtsjahr)) |&gt;\n  mutate(geburtsjahr = ifelse(geburtsjahr &lt; 1000, geburtsjahr+1000, geburtsjahr)) |&gt;\n  mutate(alter = 2022 - geburtsjahr)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `geburtsjahr = as.integer(geburtsjahr)`.\nCaused by warning:\n! NAs durch Umwandlung erzeugt\n\n# Berechnet Mittelwert und Standardabweichung des Alters\n\ndf_alter &lt;- df_lokal |&gt;\n  summarise(MWAlter = round(mean(alter, na.rm = TRUE), 2),\n            SDAlter = round(sd(alter, na.rm = TRUE), 2))\n\ndf_alter\n\n  MWAlter SDAlter\n1   49.66   15.31\n\n\nWir sehen, dass die Befragten im Mittel 49,66 also fast 50 Jahre alt waren, bei einer Standardabweichung von 15,31 Jahren. Im Video oben haben Sie erfahren, dass bei einer Normalverteilung ca. 68 % aller Werte innerhalb der Region Mittelwert - 1 Standardabweichung bis Mittelwert + 1 Standardabweichung liegen. Hier wäre das entsprechend der Bereich von 34,35 bis 64.97 Jahren, zumindest sofern die Variable normalverteilt ist. Um das zu prüfen und generell um metrische Daten zu visualisieren, können wir ein sogenanntes Histogramm zeichnen. Dazu nutzen wir geom_histogram(). Im Prinzip handelt es sich dabei um eine Art Balkendiagramm für metrische Daten, bei der einzelne Ausprägungen zusammengefasst werden.\n\nhistogramm_alter &lt;- df_lokal |&gt;\n  filter(!is.na(alter)) |&gt;\n  ggplot(aes(x = alter))+\n  geom_histogram(fill = \"lightgrey\", bins = 40)+\n  theme_minimal()+\n  labs(x = \"Alter der Befragten\", y = \"Häufigkeit\")\n  \n\nhistogramm_alter\n\n\n\n\n\n\n\n\nDem Histogramm können wir entnehmen, dass die Daten annähernd normalverteilt sind. Das linke Ende der Verteilung ist etwas steiler. Das ist zu erwarten, denn üblicherweise gibt es ein Mindestalter zur Teilnahme an Befragungen. Wir sehen auch, dass Menschen um die 60 Jahre und älter relativ stark vertreten sind. Gemessen an der Bevölkerung ist auch das nicht ungewöhnlich. Für die meisten statistischen Zweck könnten Sie bei so einer Verteilung aber davon ausgehen, dass Sie zumindest nah genug an einer Normalverteilung dran sind.\nWährend uns das Histogramm einen guten Überblick über die Verteilung als Ganze gibt, ist es manchmal sinnvoll, Mittelwert und Standardabweichung direkt darzustellen. Das gilt insbesondere dann, wenn Sie mehrere Gruppen vergleichen wollen. Im folgenden Beispiel stelle wir das Durchschnittsalter (also den Mittelwert) der Befragten nach Geschlecht dar. Dazu wollen wir die Streuung um den Mittelwert der jeweiligen Gruppen darstellen. Hier machen wir das, indem wir die Standardabweichung ebenfalls darstellen. In wissenschaftlichen Arbeiten werden Sie auch immer mal Darstellungen begegnen, in denen andere Maße genutzt werden: entweder der Standardfehler oder sogenannte Konfidenzintervalle. Die Begriffe werden wir noch kennenlernen, ignorieren sie aber in diesem Kapitel noch.\nFür unser Beispiel berechnen wir als erstes Mittelwert und Standardabweichung der jeweiligen Gruppen. Alles, was wir dafür brauchen, haben wir schon kennengelernt: Wir nutzen filter() um fehlende Werte auszuschließen, nutzen group_by(), um die Daten zu gruppieren und rufen dann mean() und sd() innerhalb von summarise() auf.\n\n# Berechnet Mittelwert und Standardabweichung nach Geschecht\ndf_lokal_alterUndGeschlecht &lt;- df_lokal |&gt;\n  filter(!is.na(geschlechtMitNAs)) |&gt;\n  filter(!is.na(alter)) |&gt;\n  group_by(geschlechtMitNAs) |&gt;\n  summarise(MWAlter = mean(alter),\n            SDAlter = sd(alter))\n\nUm die Gruppenmittelwerte mit jeweiliger Standardabweichung abzubilden, nutzen wir zwei neue geom_ Funktionen: geom_point() für die Mittelwerte und geom_errorbar() für die Standardabweichungen. Aber fangen wir oben an: zunächst legen wir in ggplot() und aes() fest, dass wir das Alter auf der y-Achse und das Geschlecht auf der x-Achse darstellen wollen. geom_point() sorgt dafür, dass die Gruppenmittelwerte jeweils durch einen Punkt dargestellt werden. geom_errorbar() zeichnet Balken um diese Punkte. Dazu müssen wir wieder in aes() angeben, wo diese Balken anfangen und aufhören sollen. Den Startpunkt legen wir mit ymin=MWAlter-SDAlter und den Endpunkt mit ymax=MWAlter+SDAlter fest.\n\n# Plottet die Gruppenmittelwerte und Standardabweichungen\nalter_nach_geschlecht &lt;- df_lokal_alterUndGeschlecht |&gt;\n  ggplot(aes(x = geschlechtMitNAs, y = MWAlter))+\n  geom_point()+\n  geom_errorbar(aes(ymin=MWAlter-SDAlter, ymax=MWAlter+SDAlter))+\n  labs(x = \"Geschlecht\", y = \"Alter\")+\n  theme_minimal()\n\nalter_nach_geschlecht\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrafiken speichern\n\n\n\nWenn Sie einen Plot speichern wollen, können Sie die ggsave()-Funktion verwenden. Folgende Argumente sollten Sie der Funktion übergeben:\n\nfilename = Den Namen, den die Datei tragen soll (in Anführungszeichen).\nplot = Den Namen des Objekts, das Sie speichern wollen.\nwidth = Die gewünschte Breite der Grafik.\nheight = Die gewünschte Höhe der Grafik.\nunits = Die Maßeinheit in der Breite und Höhe angegeben werden: “mm”, “cm”, “px” oder “in” für Millimeter Zentimeter, Pixel oder Inch.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Skalenniveaus und deskriptive Datenanalyse</span>"
    ]
  },
  {
    "objectID": "Zusammenhänge zwischen nominalen und ordinalen Variablen.html#deskription-und-visualisierung-der-daten",
    "href": "Zusammenhänge zwischen nominalen und ordinalen Variablen.html#deskription-und-visualisierung-der-daten",
    "title": "6  Zusammenhänge zwischen nominalen und ordinalen Variablen: Kreuztabellen",
    "section": "6.2 Deskription und Visualisierung der Daten",
    "text": "6.2 Deskription und Visualisierung der Daten\nBevor wir starten, müssen wir ein paar Vorkehrungen treffen. Konkret laden wir das tidyverse und installieren das Paket effectsize, das wir später bei der statistischen Analyse benötigen werden. Anschließend lesen wir die Daten ein. Außerdem nutzen wir die options()-Funktion. Damit können wir die Einstellungen von R ändern. Hier nutzen wir das Argument scipen mit dem Wert 999. Im Prinzip sorgen wir damit nur dafür, dass sehr kleine Zahlen so angezeigt werden, wie Sie es erwarten würden. Das werden wir später bei den statistischen Analysen benötigen.\n\n# Lädt das tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Lädt bzw. installiert und lädt das Paket \"effectsize\"\nif(!require(effectsize)){\n  install.packages(\"effectsize\")\n  library(effectsize)\n}\n\nLade nötiges Paket: effectsize\n\n# Liest die Daten ein\ndf_lokal &lt;- read.csv(\"Daten/lokalkommunikation.csv\")\n\n# Stellt ein, dass sehr kleine Zahlen normal dargestellt werden\noptions(scipen = 999)\n\nSchauen wir uns nun einmal eine Kreuztabelle an. Anschließend visualisieren wir die Daten. Wir nehmen dazu die Spalte Bula, die angibt, aus welchem Bundesland die Befragten stammen und die Spalte A502_01, die angibt, ob die Menschen Mitglied in einem Sportverein sind. Um eine einfache Kreuztabelle zu erstellen, können wir die table()-Funktion nutzen, der wir die beiden Spalten übergeben. Vorher wandeln wir A502_01 in einen Faktor um, sodass wir etwas besser damit arbeiten können.\n\n# Wandelt die Spalte A502_01 in einen Faktor mit den Stufen \"nein\" und \"ja\" um.\ndf_lokal &lt;- df_lokal |&gt;\n    mutate(Sportverein = factor(A502_01, labels = c(\"nein\", \"ja\")))\n\n# Erstellt eine einfache 2x2 Tabelle, die wir \"tabelleSportNachBundesland\" nennen\n# select() wählt die Spalten aus, die dann an table() weitergereicht werden\ntabelleSportNachBundesland &lt;- df_lokal |&gt;\n  select(Bula, Sportverein) |&gt;\n  table() \n\ntabelleSportNachBundesland\n\n     Sportverein\nBula  nein  ja\n  RLP  649 385\n  TH   636 172\n\n\nFür sich genommen sagt uns diese Tabelle noch nicht allzu viel. Wir sehen, dass Befragte aus beiden Bundesländern ähnlich oft angegeben haben, nicht Mitglied eines Sportvereins zu sein, allerdings gab in Rheinland-Pfalz ein paar mehr Menschen, die in Vereinen aktiv sind.\nIm nächsten Schritt visualisieren wir diese Daten. Wir gehen dabei ähnlich vor wie bei den Balkendiagrammen mit relativen Häufigkeiten in Kapitel 4. Das heißt, wir erstellen einen reduzierten Datensatz. Dazu nutzen wir erst filter() und rufen darin !is.na() auf. Dabei müssen wir darauf achten, dass wir die Funktion für beide Variablen aufrufen und mit einem & verbinden. Anschließend nutzen wir group_by() und dann summarise(). Das Ergebnis ist ein Datensatz mit vier Zeilen, von denen jede eine Zelle aus der Tabelle oben darstellt.\n\n# Erstellt einen reduzierten Datensatz, der die relativen Häufigkeiten der Ausprägungen enthält\ndf_plot_kreuztabelle &lt;- df_lokal |&gt;\n  filter(!is.na(Sportverein) & !is.na(Bula)) |&gt;\n  group_by(Bula, Sportverein) |&gt;\n  summarise(Anzahl = n())\n\n`summarise()` has grouped output by 'Bula'. You can override using the\n`.groups` argument.\n\ndf_plot_kreuztabelle\n\n# A tibble: 4 × 3\n# Groups:   Bula [2]\n  Bula  Sportverein Anzahl\n  &lt;chr&gt; &lt;fct&gt;        &lt;int&gt;\n1 RLP   nein           649\n2 RLP   ja             385\n3 TH    nein           636\n4 TH    ja             172\n\n\nDiesen Datensatz können wir jetzt für die Visualisierung nutzen. Auch hier gehen wir im Prinzip wie in Kapitel 4 vor. Allerdings mit einem wichtigen Unterschied: Um die vier Zellen aus der Tabelle abbilden zu können, reichen einfache, nebeneinander stehende Balken nicht mehr aus. Zwar könnten wir theoretisch vier Balken zeichnen, allerdings wäre das nicht sonderlich übersichtlich. Stattdessen bietet sich ein sogenanntes Stapeldiagramm an.\nDie gute Nachricht ist, dass ein Stapeldiagramm sehr ähnlich erstellt wird wie die Balkendiagramme, die Sie bereits kennen. Beim Aufruf von ggplot() und darin der aes()-Funktion übergeben wir eine Spalte (hier Bundesland) für die x-Achse und eine andere (Anzahl) für die y-Achse. Zusätzlich nutzen wir das fill-Argument und geben dort die Spalte Sportverein an. Damit sagen wir ggplot, dass diese Spalte genutzt werden soll, um die Balken einzufärben. Das heißt, die Ausprägungen “ja” und “nein” erhalten andere Farben. Diese können wir in scale_fill_manual festlegen. In diesem Beispiel geben wir außerdem in geom_bar position = \"fill\" an. Dadurch werden die Werte in relative Häufigkeiten umgewandelt und beide Balken sind gleich hoch.\n\nplot_kreuztabelle &lt;- df_plot_kreuztabelle |&gt;\n  ggplot(aes(x = Bula, y = Anzahl, fill = Sportverein))+\n    geom_bar(position = \"fill\", stat = \"identity\")+\n    theme_minimal()+\n    scale_fill_manual(values = c(\"#F5C000\", \"#0035F5\"))+\n    labs(x = \"Bundesland\", y = \"relative Häufigkeit\", fill = \"Mitglied\\nim Sportverein\")\n\nplot_kreuztabelle\n\n\n\n\n\n\n\n\nDie Visualisierung der Daten macht deutlich, dass in Thüringen prozentual weniger Befragte Mitglied in einem Sportverein sind als in Rheinland-Pfalz. Das legt nahe, dass die beiden Variablen zusammenhängen. Als nächstes prüfen wir mit einem Signifikanztest, ob dies tatsächlich der Fall ist.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zusammenhänge zwischen nominalen und ordinalen Variablen: Kreuztabellen</span>"
    ]
  }
]